[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Awash in Data",
    "section": "",
    "text": "Introduction\nThis book leads you through a few introductory lessons in data science. You can think of it as a self-guided textbook for teachers or students. If you’re a teacher, you might assign chapters, problems, or projects for students to read and do.\nIn this book, you will use CODAP, the Common Online Data Analysis Platform, to do your data analysis. CODAP is free and web-based, that is, it runs in your browser. You do not even need to sign in or make an account to use it."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Tip\n\n\n\nthis is a DIV\n\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "01-lesson-part.html",
    "href": "01-lesson-part.html",
    "title": "2  Lessons",
    "section": "",
    "text": "3 Lessons Overview\nThe first “Part” of this book—the next few chapters—contains lessons and assignments for a quick introduction to data science. I created these lessons for students in a high school, as part of a class called “Applied Mathematics,” which includes a number of topics including consumer-ish math. This introduction occupied a couple of weeks of class time, roughly four seventy-five minute blocks. Although the students varied widely in their level of preparation, they were generally polite, attentive, and well-intentioned.\nWe ran this most recently in the Spring of 2020, under quarantine, when the class was “virtual,” run over Zoom. The lessons here roughly parallel what we did in class that time, when students were isolated in their homes and groupwork was possible, but harder than “before.” The discussion suggestions in the lessons work fine, but not as well as in an in-person classroom. Therefore I think these lessons might also work, with sensible modifications, for self-study. For example, you will not be turning in your projects—unless you want to send them to me; I’d love to see them!\nLikewise, if you are a teacher with a class of your own, of course you should modify what you see here so that it makes sense for your students and your situation. For example, our school uses Google apps extensively, so students all know about setting permissions and submitting links to Google docs via email. In previous years, I had insisted that students make pdfs and email those in. What works for you depends on your situation.\nYou may want to modify assignments, add new ones, skip others. Go for it. I will try to describe, here and in the commentary that accompanies each element in this Part, my rationale for the order and content I chose.\nClass preparation: We (the teacher-of-record and I) set up a Google doc as a “landing page” for the class. It had material that students would need: links to class-specific instructions, links to the CODAP software and to this book, as well as links to various data sets. Before each class, we edited the doc so that today’s most important links were at the top, with an agenda of what we would do. That way, all past links and agendas were still in the document, at the bottom, in reverse chronological order.\nFor the first lesson, for example, we had a link to this book and to a blank CODAP document, with advice to bookmark the page. We also had a link to an “800 children and teens” document, which we easily got to in the first session.\nThose data links are also embedded in this book, so if your students use this book (for ours, this was just a resource), or you are doing self-study, there’s no need to make an extra set of links."
  },
  {
    "objectID": "01-lesson-part.html#the-getting-started-lesson",
    "href": "01-lesson-part.html#the-getting-started-lesson",
    "title": "Lessons",
    "section": "The “getting started” lesson",
    "text": "The “getting started” lesson\nHere is a link to the lesson chapter\nCritical links:\n\na blank CODAP document, https://codap.concord.org/app\nthis book https://codap.xyz/awash\n\nThis introduces CODAP basics, especially how to make graphs. This takes students only a few minutes, and, as the commentary suggests, could even be homework.\nIn an actual classroom, circulate and make sure that everybody has found the relevant page and can make graphs. Lead a brief discussion as suggested in the commentary."
  },
  {
    "objectID": "01-lesson-part.html#children-and-teens-part-one",
    "href": "01-lesson-part.html#children-and-teens-part-one",
    "title": "Lessons",
    "section": "800 children and teens, part one",
    "text": "800 children and teens, part one\nHere is a link to the lesson chapter\nAdditional critical link:\n\nage and height data: http://codap.concord.org/app#shared=31797\n\nThe dataset has information about 800 USA-ians, aged 5 to 19, from a national health survey (NHANES). The attributes (a.k.a. variables) include age, gender, height, weight, armspan, upper arm length, and pulse. We will eventually focus on age, gender, and height—but we don’t tell students that at first.\nEventually, we focus on how height changes with age—and how it also depends on gender. That presents a problem that often makes students feel “awash”: they have three attributes but only two axes on a graph. The obvious solutions—multiple graphs, color the points—don’t work out very well.\n\nFiltering. We show students a different approach, using a data move: filtering. We look at only a small part of the data, just the 10-year-olds (where the girls are taller!). Then it’s easy to compare the boys and the girls in a graph, now that age is not a factor.\nGetting to the filtering is the critical part of the lesson, and sets up the groupwork described in that chapter: each group does this filtering move for different ages, pooling the class data so students can enter and plot the mean ages for each gender at each age. For logistics, we created a shared Google sheet where students entered the means."
  },
  {
    "objectID": "01-lesson-part.html#a-first-assignment",
    "href": "01-lesson-part.html#a-first-assignment",
    "title": "Lessons",
    "section": "A First Assignment",
    "text": "A First Assignment\nHere is a link to the assignment\n\nNew link—Census data: https://codap.concord.org/app#shared=22176\n\nThis assignment introduces a new dataset from the US Census. It has income data for 1000 25-to-44-year-olds, along with gender (of course) but also race, Hispanic status, and educational attainment.\nThe assignment asks students to explore the data, and then to come up with a claim and a graph to address the claim. This should be simple—simple enough that they could conceivably do the data analysis using the live illustration embedded in the assignment.\nThis assignment is written so it’s entirely done in CODAP (no Google doc at this point), so it can be done and turned in in class (although some students might not finish). It could even be done orally, though we did it as an assignment to turn in.\nMany students will use this dataset to compare incomes, and that will often lead into social justice issues. We love that kind of topic, and students are often intrigued and motivated that they are exploring it with real data about real people."
  },
  {
    "objectID": "01-lesson-part.html#children-and-teens-part-two",
    "href": "01-lesson-part.html#children-and-teens-part-two",
    "title": "Lessons",
    "section": "800 children and teens, part two",
    "text": "800 children and teens, part two\nHere is a link to the lesson chapter\n\nGrouping and summarizing. This chapter introduces grouping cases in the dataset by reorganizing the table and summarizing those groups by calculating summary (or aggregate) values, in this case, mean heights. These are our second and third core data moves. Conceptually, it’s the most subtle—and powerful—part of this intro to data science. Your goal, as a teacher, is to get as many students as possible to understand this.\n\nThe session may begin with the students entering the mean heights by gender and age that they worked on in groups. They can then make the summary graph. If they made this graph already (e.g., in the previous session), remind them of it.\n\nRemind students that that process used the filtering data move.\n\nNow students will learn how to have the computer do all that. This is perfect for a demo. Get their close attention and go through it slowly, sometimes backing up as students have questions. There are two main parts: making the groups; and then making the aggregate calculation. The text in the chapter has all sorts of questions embedded in it; use them to guide how you show students this technique.\nYou will probably end the session debriefing the first assignment (with the Census data) and previewing the second one."
  },
  {
    "objectID": "01-lesson-part.html#a-second-assignment",
    "href": "01-lesson-part.html#a-second-assignment",
    "title": "Lessons",
    "section": "A Second Assignment",
    "text": "A Second Assignment\nHere is a link to the assignment\nThis is a more complex version of that first assignment. Notice these differences:\n\nLogistical issues:\n\nIt’s supposed to be a Google doc, not just a CODAP doc. (If you want a different format, go ahead!)\nStudents put a link to their CODAP doc in the Google doc.\nThey need to learn how to get a graph into the Google doc.\nIf possible, they should use that grouping move they just saw, and, in addition, calculate summary values for the groups.\n\nThey need to enhance their investigation and give it more nuance. We refer to this in the text as “dig deeper.”\nThey need to pay more attention to communication, for example in the size and choice of their graphics and the coherence of their narrative.\n\n\nStudents may be perplexed about what we mean by “dig deeper.” An early section in the chapter contains an example of the kind of thing they could do—exploring the obvious issue of income disparity between males and females—and then shows what “digging deeper” might look like.\n\nThe other difference is that we restrict the topic: We give them just two claims to choose from. Most students become invested in the Census data. It’s engaging, and quickly brings social issues into math class. But they need an alternative; ours is data from BART, the Bay Area Rapid Transit. There are links in the assignment to orient you and those students to the data.\n\nThe next class session\nIn our most recent class, some students “got” this assignment and were done in a flash. For others, the whole grouping-and-aggregation thing was alien and complex. That’s not surprising: it is conceptually the hardest part of this unit.\nSo you cannot expect students to have completed even this simple task by the beginning of the next session. We devoted pretty much the whole session to working on this task, giving students help individually and in small groups, and getting them to help each other.\nFor students who were moving faster, we presented the “small project” task (below) and let them get started. It might be better, however, to give them other useful things to do. In an in-person class, these students could be resources for their peers, or we could have given them other enriching problems to solve. There are some in this book (xxx) that were not available at that time."
  },
  {
    "objectID": "01-lesson-part.html#a-small-project",
    "href": "01-lesson-part.html#a-small-project",
    "title": "Lessons",
    "section": "A Small Project",
    "text": "A Small Project\nHere is a link to the assignment\nThis is the capstone project for this unit, and is really just a small extension beyond the second assignment students have just completed.\nThe key differences are:\n\nStudents pick their own topics.\nThey can use the more extensive, national Census data that includes different states and different years.\nFor students using BART data, they can play the embedded “secret meeting game.”\n\nSome consequences are:\n\nStudents might be more invested in the topics because they chose them themselves.\nThey will have ideas for other things they want to do with the data.\n\nSome will be too hard, and students may need to find alternatives.\nSome will be possible, but may not have been covered in class.\n\n\n\nLinking. As an example of this last possibility, some students, independently, wanted to connect the Census data to data that wasn’t in the data set, that is, to link our data to external data. That’s another data move we call joining, so we’ve added a chapter about it even though it’s not really part of a simple introduction to data science.\nHere is a link to that chapter"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "01.01-start.html#commentary-a.k.a-tldrteachers-and-other-oldsters",
    "href": "01.01-start.html#commentary-a.k.a-tldrteachers-and-other-oldsters",
    "title": "2  Getting Started with CODAP",
    "section": "2.1 Commentary (a.k.a TL;DR)^[Teachers and other oldsters,",
    "text": "2.1 Commentary (a.k.a TL;DR)^[Teachers and other oldsters,\njust in case this is new: TL;DR means “Too Long; Didn’t Read.”]\nWe will include sections like this from time to time. Here is some meta-commentary, that is, commentary about this commentary:\n\nYou can learn a lot in this book even if you skip these sections. But doing things—actually interacting with the data—is more important than reading.\nTeachers: we will have suggestions for how to present this material and why we think it’s important.\nStudents: this may tell you what the teacher cares about in case you want to focus on something.\n\nEnough meta. Onward!\n\nOverview\nThe file has data about things like LifeSpan, Speed, and Diet for 27 species of mammal. Doing the activity, you make graphs of these attributes. This dataset is problematic in a number of ways, and does not smell like data science. Nevertheless, it’s a good place to start.\n\nWhat we’re calling attributes you could just as well call variables or even column headings. Sometimes I will use variable when my audience is likely to understand what I mean, and when attribute might make them pause longer than I want. Why “attributes?” See this section from a chapter in the distant future.\n\nWhen you first make a graph, the points appear randomly (but they still represent the data; try clicking on one and see what happens in the table). To organize the points, that is, to make a meaningful graph, do what we always tell students to do when making graphs by hand: label your axis, that is, drag a label—the name of an attribute—from the column heading to an axis of the graph.\n\n\nThe “Starting CODAP” Lesson\nYou could conceivably assign this for homework so that students “hit the ground running.” But it also works well simply to give them the link in class and let them do the task together.\n\nEncourage talking to others.\nWalk around and make sure everybody is eventually making graphs; at the end everyone should have bivariate graphs—different attributes on the two axes. These could be scatter plots or parallel dot plots.\nIf students are having trouble, have other students help them before you do.\n\nThe underlying point is that students do not need you to explain things. They can figure out a lot by themselves, and in collaboration with other students.\nThey should be secure in making graphs in just a few minutes. Stop them from spending too much time with this data set, and maybe lead the class in a discussion. Some possible issues:\n\nDid anybody get lots of colored points in your graph? How did you do that? (“Plop” an attribute into the middle of a graph)\nThe graph of Speed and the graph of Diet are qualitatively different. How? Why?\nWhat happens when you click on a point in a graph? (The same case gets selected in the table—and in any other graphs)\nWhat happens when you hover over a point in a graph?\nWhat does one row of the table represent? (Importantly: a species, not the common response, an animal.)\nHow do you change the size of the points in a graph? (It’s in the “paintbrush” palette. But maybe don’t tell them; Let somebody discover it and tell the class.)\nWhy are there no birds in this dataset?\n\n\nThese discussions have multiple purposes. They help students listen to each other and feel part of a community. And they also let you highlight software features that students will need to know about. Ideally, students highlight these features to each other, so it’s not always your voice.\n\n\nThis is important because students will go in different directions. If we wrote instructions for everything students might need, we would forever be teaching features and never get to exploring; furthermore, many students would be bored. So we delay learning about some features until students need them or discover them by accident. The best transmission is often viral: Zak sees that Annabelle has a graph with colored points, so he asks her how she got them. Then, in the discussion, she gets to tell the class and we can step in with the cool enhancement: here, in the paintbrush palette, is where you can change the colors. Everybody will remember."
  },
  {
    "objectID": "01.02-teens.html#exploration",
    "href": "01.02-teens.html#exploration",
    "title": "3  800 Children and Teens, part one",
    "section": "3.1 Exploration",
    "text": "3.1 Exploration\n\nYour first task is to explore the data. Here are some questions you can address:\n\nWhat attributes do you expect to be related?\nCan you show that relationship in a graph?\nWhat other relationships can you show?\nTry making more than one graph, and then select points in one of them. What happens? How might that be useful?\nWhat do you think the units are for these values? (especially Weight, Height, and Pulse)\nWhat’s BMI? If you don’t know, look it up."
  },
  {
    "objectID": "01.02-teens.html#a-specific-question-who-is-taller",
    "href": "01.02-teens.html#a-specific-question-who-is-taller",
    "title": "3  800 Children and Teens, part one",
    "section": "3.2 A Specific Question: Who is Taller?",
    "text": "3.2 A Specific Question: Who is Taller?\nWho is taller, males or females?\nStereotypically, we probably agree that, in general, males are taller. But is that really true? Let’s use the data to find out.\nThe next illustration contains a CODAP document that graphs Height against Gender. That’s the obvious way to look at our question.\n\n\n\nIt looks as if the pile of males is a bit higher up in the graph, that is, they’re taller. But how much? Let’s find the mean.\n\nOh, and if you’re reading this book in a browser, that illustration is live. You don’t have to make a separate CODAP window for this bit.\n\n\nClick on the graph to select it.\nAt the right of the graph, click on the “ruler” icon. A panel opens up. We call these things “palettes.”\nClick the checkbox for mean. (Of course you can try other options as well.)\nHover over the mean lines that appear. You can see the values.\n\nYou should find that the average height of males is about 10 (cm) greater than the average height of females. So that shows that our preconception (males are taller) is correct.\nBut, but…\nIf you stop and think a bit, our graph is deeply bogus. It’s a bad analysis. Why?\nTry not to read ahead…\n\nIf you’re a student in a class, discuss with your group.\nIf you’re studying alone, think about this before scrolling down to see what we think."
  },
  {
    "objectID": "01.02-teens.html#making-the-question-more-specific",
    "href": "01.02-teens.html#making-the-question-more-specific",
    "title": "3  800 Children and Teens, part one",
    "section": "3.3 Making the Question More Specific",
    "text": "3.3 Making the Question More Specific\nThe problem is that we haven’t taken Age into account, and Age is much more important than Gender in determining height. The whole long tail of short people—for both males and females–is made up of little kids. If you’re not convinced, drop Age into the middle of the graph.\nGo ahead, we’ll wait.\n\nIn general (the graph says), the short people are younger. Make sure you can explain how the graph shows that. What is it about the colors that says short people are younger?\n\nStill, it’s a confusing graph. Let’s make it simpler.\nInstead of looking at everybody we have, ages 5–19, let’s just look at one age: 10-year-olds. First we’ll filter the graph so it shows only 10-year-olds. Then we’ll compare the heights of those boys and those girls.\nYou get a fresh, live document below. Follow these steps for the filtering:\n\nDrag Age to the horizontal axis so you have a graph of Height against Age.\nTake a moment to discuss (or reflect) on whether that graph makes sense. It tells a story. What is it?\nSelect all the 10-year-olds. Do this by dragging a rectangle around those points. If this is unfamiliar to you, you can probably figure it out by messing around. If that doesn’t work for you, get help!\nWith the graph selected, click on the “eyeball” palette on the right to bring up a menu.\nChoose Hide Unselected Cases. Aha! Now the graph has only 10-year-olds.\n\n\n\n\nNow figure out how to compare the heights of the boys and the girls, this time of only the ten-year-olds. Be sure to put the mean on the graph so you get their average heights. See if you can get this graph:\n\n\n\n\n\nHeights of ten-year-olds, split by gender.\n\n\nSome questions to answer; if you don’t know, don’t be afraid to ask others!\n\nHow did you compare the 10-year-old girls to the boys?\nAre there other ways to compare them in a graph? Sure there are!\nWhich way works better?\nThe heights of females overlap with heights of males. What does that mean?\nWhat are the mean heights of the 10-year-old girls and boys?1 How did you find them?\nFor the whole dataset, males are taller. For 10-year-olds, females are taller. How is that possible? Does it fit with your experience?"
  },
  {
    "objectID": "01.02-teens.html#groupwork-getting-all-the-means",
    "href": "01.02-teens.html#groupwork-getting-all-the-means",
    "title": "3  800 Children and Teens, part one",
    "section": "3.4 Groupwork! Getting all the means",
    "text": "3.4 Groupwork! Getting all the means\nIf you’re in a class, and there is enough time, your instructor will break you into groups.\nEach group will be responsible for a couple of ages. For each age, do what we just did for 10-year-olds: find the mean height for the girls and the boys at that age. Then enter your data on a class table, which may be on a whiteboard, or perhaps online in a shared table such as a Google Sheet.\nThen, when all the groups are done, enter your data into a fresh CODAP document. How do you do that?\n\nBegin with a fresh CODAP document.\nMake a new table (look in the Tables tool).\nCreate the relevant columns (what columns do you need?).\nEnter the data by typing the numbers in to the table cells.\n\nIf you have the data in a Sheet, you could, instead:\n\nBegin with a fresh CODAP document.\nExport the sheet as a .csv file. (in Google, it’s in the File menu. Choose Download and then Comma-separated Values.)\nDrop the file into your CODAP document.\n\nThen plot the means as a function of age. Make sure you can tell the males from the females!\n\nPlotting two things at once\nIf the mean heights for males and females are in different columns in your table, you might first plot the females on the vertical axis and age on the horizontal. But then, if you plot the males in the normal way, the female data will disappear. How do you get them both on the same graph?\nThe trick is this: as you are dragging the males in, wait. With the mouse down, pause and look: there is a gray outline of a plus sign at the top of the axis. Drop the attribute there instead of on the axis; it will add the data to the plot instead of replacing it."
  },
  {
    "objectID": "01.02-teens.html#commentary",
    "href": "01.02-teens.html#commentary",
    "title": "3  800 Children and Teens, part one",
    "section": "3.5 Commentary",
    "text": "3.5 Commentary\nThere are three main phases to this lesson.\n\nFirst, students mess around with the data, making graphs using any attributes they like, looking for relationships. Ideally, a few of them get to show and explain their graphs, and you run a discussion as decscribed in the commentary.\nIn the second phase, we focus on a specific issue: who is taller, females or males? We learn to show means on the graph. When the obvious analysis doesn’t work well, and we are still awash in data, we get even more specific and focus on 10-year-olds. We use a data move, filtering, to do this. Very important.\nIf possible, there’s a third phase where students—probably in groups— find the mean heights for boys and girls at each age, then plot those results.\n\nThe final graph tells a clear story about height and gender and age.\n\nWho is taller? More detail…\nIn the first, exploratory phase, somebody probably made a graph with height and age or height and gender. Explain that we will now focus on this issue.\nIn a class—even online—rather than having students read the instructions and do this alone or in pairs, I do this next part as a demo, and go through the process outlined in the text. I focus on height versus gender, with questions along the way (e.g., Why is height on the vertical axis? Because it’s the response? Right. But also, because height is vertical.)\nI make the graph and show how to put the means on the graph using the ruler palette. We see, by hovering, the different values. We ask, “Are we done? Males are taller than females?”\nAnd if no one says so, we tell them that actually, there is something deeply bogus (or hinky or whatever the current term is) about our graph. It’s a fine graph, but it’s unfair. Why? What’s missing?\nWait time. Wait time.\nSomeone will say “age.” Probe for what they mean. Plop Age onto the graph to see the color gradient. Yeah, the tails are all little kids.\nEarlier someone might have made the graph of Height against Age. You can refer back to it, and make that graph for the class. If they haven’t, you can do it, plopping Gender into the middle.\nWhat do you think about this graph? Is it clear what’s going on? Partly, but it’s hard work to read this graph. It’s still confusing. Let’s find a way to have this make more sense.\nSo we do the filtering move: we select the 10-year-olds and hide the rest, then put Gender on the axis and show the mean heights of each group, reflecting on the realization that, although in general the males are taller, the story is different for particular ages.\nThat’s about as much lecture/demo as we can tolerate, so we switch to a different mode.\n\n\nGroupwork! Getting the means in more detail…\nWhat if we did that procedure—looking at only the ten-year-olds, and recording the mean heights of the males and females— but for every age?\nIt would be really cool. So we do exactly that. Every group gets an age or two to be in charge of. Their task is to find the mean heights of males and females at those ages. Groups post the means in a table on the class whiteboard or in a Google Sheet.\n\n\n\n\n\n\nGroups post mean heights in a Google Sheet.\n\n\n\n\n\n\n\nValues from the sheet, plotted in CODAP.\n\n\n\n\n\n\nThe fact that it was groups gave students a welcome break, a chance to talk, and also reinforced that key skill of filtering. When we were in quarantine, we did this in randomized Zoom breakouts. We had pre-assigned the ages to group numbers.\n\n\nIn addition, this part of the activity gave students practice with computing means; introduced how to enter your own data into CODAP; and most importantly showed them where you can go with this.\n\nI think it’s important that the filtering and typing will be slow and a little laborious, for two reasons: first, of course, when you show them how the computer can do it in the next lesson, they will see how cool and time-saving it is. Second, slowing down highlights this idea of turning the means into values in a table that you can graph. It gives it time to sink in.\n\n\nThe key moment: filtering\n\nFiltering is our first data move. When we display all the data, the Heightvs.Age graph does make sense, and it tells a story, but it’s complicated. Looking at it, we’re pretty “awash in data.”\n\nFor example, we were trying to compare heights across genders, right? But the height-age graph doesn’t have gender (left-hand illustration). What if we drop Gender onto the middle of the graph? You’ll get something like the right-hand illustration (and you should do it yourself).\n\n\n\n\n\n\nHeight by age, no gender.\n\n\n\n\n\n\n\nSame, with Gender. This is confusing. We’re awash.\n\n\n\n\n\nYou can kind of see that males are taller, at least older males are taller, but it’s not really clear because so many of the points are stacked on top of each other. You can’t tell whether purples are hiding under oranges or what.\nBut when we filter, and look at only the 10-year-olds, everything is clearer. Looking at that graph, we are no longer “awash.” It’s all manageable.\n\n\n\n\n\nHeights of ten-year-olds, split by gender (again).\n\n\n\nThe dots are not hiding each other any more.\nThere are fewer dots altogether.\nIt’s a normal kind of graph, a kind we might be more used to reading.\n\nOr, more deeply, this graph reduces the dimensionality of the problem. Before the filtering, we really needed to show three attributes at once: Height, Age, and Gender. But our graph has only two dimensions.\nBy focusing only on the 10-year-olds, we eliminate the Age attribute: it’s now irrelevant because everybody we’re looking at is 10 years old. That means we can use that horizontal, Age axis for Gender instead.\nWe’re still interested in age—after all, it has a big influence on height— but we have decided to ignore it, temporarily, strategically. We used the filtering data move to help us be less awash, to help us see something familiar.\nThat familiarity can also be an inspiration for what to do next, for how to “dig deeper” into the data."
  },
  {
    "objectID": "01-lesson-part.html#overview",
    "href": "01-lesson-part.html#overview",
    "title": "Lessons",
    "section": "Overview",
    "text": "Overview\nThe first “Part” of this book—the next few chapters—contains lessons and assignments for a quick introduction to data science. I created these lessons for students in a high school, as part of a class called “Applied Mathematics,” which includes a number of topics including consumer-ish math. This introduction occupied a couple of weeks of class time, roughly four seventy-five minute blocks. Although the students varied widely in their level of preparation, they were generally polite, attentive, and well-intentioned.\nWe ran this most recently in the Spring of 2020, under quarantine, when the class was “virtual,” run over Zoom. The lessons here roughly parallel what we did in class that time, when students were isolated in their homes and groupwork was possible, but harder than “before.” The discussion suggestions in the lessons work fine, but not as well as in an in-person classroom. Therefore I think these lessons might also work, with sensible modifications, for self-study. For example, you will not be turning in your projects—unless you want to send them to me; I’d love to see them!\nLikewise, if you are a teacher with a class of your own, of course you should modify what you see here so that it makes sense for your students and your situation. For example, our school uses Google apps extensively, so students all know about setting permissions and submitting links to Google docs via email. In previous years, I had insisted that students make pdfs and email those in. What works for you depends on your situation.\nYou may want to modify assignments, add new ones, skip others. Go for it. I will try to describe, here and in the commentary that accompanies each element in this Part, my rationale for the order and content I chose.\nClass preparation: We (the teacher-of-record and I) set up a Google doc as a “landing page” for the class. It had material that students would need: links to class-specific instructions, links to the CODAP software and to this book, as well as links to various data sets. Before each class, we edited the doc so that today’s most important links were at the top, with an agenda of what we would do. That way, all past links and agendas were still in the document, at the bottom, in reverse chronological order.\nFor the first lesson, for example, we had a link to this book and to a blank CODAP document, with advice to bookmark the page. We also had a link to an “800 children and teens” document, which we easily got to in the first session.\nThose data links are also embedded in this book, so if your students use this book (for ours, this was just a resource), or you are doing self-study, there’s no need to make an extra set of links."
  },
  {
    "objectID": "01-lesson-part.html#data-move-filtering",
    "href": "01-lesson-part.html#data-move-filtering",
    "title": "Lessons",
    "section": "Data move: filtering",
    "text": "Data move: filtering\nWe show students a different approach, using a data move: filtering. We look at only a small part of the data, just the 10-year-olds (where the girls are taller!). Then it’s easy to compare the boys and the girls in a graph, now that age is not a factor.\nGetting to the filtering is the critical part of the lesson, and sets up the groupwork described in that chapter: each group does this filtering move for different ages, pooling the class data so students can enter and plot the mean ages for each gender at each age. For logistics, we created a shared Google sheet where students entered the means."
  },
  {
    "objectID": "01-lesson-part.html#data-moves-grouping-and-summarizing",
    "href": "01-lesson-part.html#data-moves-grouping-and-summarizing",
    "title": "Lessons",
    "section": "Data moves: grouping and summarizing",
    "text": "Data moves: grouping and summarizing\nThis chapter introduces grouping cases in the dataset by reorganizing the table and summarizing those groups by calculating summary (or aggregate) values, in this case, mean heights. These are our second and third core data moves. Conceptually, it’s the most subtle—and powerful—part of this intro to data science. Your goal, as a teacher, is to get as many students as possible to understand this."
  },
  {
    "objectID": "01-lesson-part.html#data-move-linking",
    "href": "01-lesson-part.html#data-move-linking",
    "title": "Lessons",
    "section": "Data move: linking",
    "text": "Data move: linking\nAs an example of this last possibility, some students, independently, wanted to connect the Census data to data that wasn’t in the data set, that is, to link our data to external data. That’s another data move we call joining, so we’ve added a chapter about it even though it’s not really part of a simple introduction to data science.\nHere is a link to that chapter"
  },
  {
    "objectID": "01.01-start.html#commentary-a.k.a-tldr",
    "href": "01.01-start.html#commentary-a.k.a-tldr",
    "title": "2  Getting Started with CODAP",
    "section": "2.1 Commentary (a.k.a TL;DR)1",
    "text": "2.1 Commentary (a.k.a TL;DR)1\nWe will include sections like this from time to time. Here is some meta-commentary, that is, commentary about this commentary:\n\nYou can learn a lot in this book even if you skip these sections. But doing things—actually interacting with the data—is more important than reading.\nTeachers: we will have suggestions for how to present this material and why we think it’s important.\nStudents: this may tell you what the teacher cares about in case you want to focus on something.\n\nEnough meta. Onward!\n\nOverview\nThe file has data about things like LifeSpan, Speed, and Diet for 27 species of mammal. Doing the activity, you make graphs of these attributes. This dataset is problematic in a number of ways, and does not smell like data science. Nevertheless, it’s a good place to start.\n\nWhat we’re calling attributes you could just as well call variables or even column headings. Sometimes I will use variable when my audience is likely to understand what I mean, and when attribute might make them pause longer than I want. Why “attributes?” See this section from a chapter in the distant future.\n\nWhen you first make a graph, the points appear randomly (but they still represent the data; try clicking on one and see what happens in the table). To organize the points, that is, to make a meaningful graph, do what we always tell students to do when making graphs by hand: label your axis, that is, drag a label—the name of an attribute—from the column heading to an axis of the graph.\n\n\nThe “Starting CODAP” Lesson\nYou could conceivably assign this for homework so that students “hit the ground running.” But it also works well simply to give them the link in class and let them do the task together.\n\nEncourage talking to others.\nWalk around and make sure everybody is eventually making graphs; at the end everyone should have bivariate graphs—different attributes on the two axes. These could be scatter plots or parallel dot plots.\nIf students are having trouble, have other students help them before you do.\n\nThe underlying point is that students do not need you to explain things. They can figure out a lot by themselves, and in collaboration with other students.\nThey should be secure in making graphs in just a few minutes. Stop them from spending too much time with this data set, and maybe lead the class in a discussion. Some possible issues:\n\nDid anybody get lots of colored points in your graph? How did you do that? (“Plop” an attribute into the middle of a graph)\nThe graph of Speed and the graph of Diet are qualitatively different. How? Why?\nWhat happens when you click on a point in a graph? (The same case gets selected in the table—and in any other graphs)\nWhat happens when you hover over a point in a graph?\nWhat does one row of the table represent? (Importantly: a species, not the common response, an animal.)\nHow do you change the size of the points in a graph? (It’s in the “paintbrush” palette. But maybe don’t tell them; Let somebody discover it and tell the class.)\nWhy are there no birds in this dataset?\n\n\nThese discussions have multiple purposes. They help students listen to each other and feel part of a community. And they also let you highlight software features that students will need to know about. Ideally, students highlight these features to each other, so it’s not always your voice.\n\n\nThis is important because students will go in different directions. If we wrote instructions for everything students might need, we would forever be teaching features and never get to exploring; furthermore, many students would be bored. So we delay learning about some features until students need them or discover them by accident. The best transmission is often viral: Zak sees that Annabelle has a graph with colored points, so he asks her how she got them. Then, in the discussion, she gets to tell the class and we can step in with the cool enhancement: here, in the paintbrush palette, is where you can change the colors. Everybody will remember."
  },
  {
    "objectID": "01.04-teens2.html#making-groups-section",
    "href": "01.04-teens2.html#making-groups-section",
    "title": "5  800 Children and Teens, part two",
    "section": "5.1 Making Groups",
    "text": "5.1 Making Groups\nThe work in this chapter requires a lot of screen space, so use the following link to open a new tab. The live demos we have been using are not good enough for what you are about to learn!\n\nWe’ll start by making groups, one group for each age. Concentrate on the table.\n\nDrag Age to the left in the table (don’t drag it to a graph!).\nDrop it in the blank area on the left of the table (it will turn yellow when you’re over it).\n\nxxx NB: perfect place for a short video\nNow, on the left, there is one case for each Age. There are fifteen cases in all (why?).\n\n\n\nWe have selected all of the five-year-olds. Note: this is not a live demo! Its just a picture! You will need to work in a separate tab, using the link above.\n\n\n\nClick on one of the ages at the left. What happens in the graph? In the table?\n\nAha: clicking on an age selected all of the people who are that age. Also, you can see in the right side of the table that all of the people of that age are now together in the table—and selected.\n\nWhen you dropped Age on the left, you sorted the table into 15 groups, one group for each age. You can think of it as a hierarchical table: on the left, a table of 15 ages, and on the right, within each age, a table of the people at that age.\n\n\nYou have just done Grouping, our second core data move. Start to look for how grouping your data might help you. Frequently, when a dataset is large and complicated, grouping will help you make sense of it.\nWatch out, though: making too many levels of groups can sometimes make a dataset more complicated than it needs to be!"
  },
  {
    "objectID": "01.04-teens2.html#making-summary-calculations-for-each-group",
    "href": "01.04-teens2.html#making-summary-calculations-for-each-group",
    "title": "5  800 Children and Teens, part two",
    "section": "5.2 Making Summary Calculations for Each Group",
    "text": "5.2 Making Summary Calculations for Each Group\nNow we want the mean height for each of our groups. To do that, we’ll make a new column in the “groups” table on the left, and write a formula for the column:\n\nBe sure the table is selected.\nOn the left-hand side of the table, up at the top on the right, there is a gray circle with a plus sign in it. It might be hidden by some text.\n\n\n\n\nClick the gray circle with the plus sign to create a new column.\n\n\n\nClick the gray plus thingy. A new column appears, with a name ready to be editied.\nGive it a good name such as MeanHeight. Press enter to finish editing. The column should be blank.\nLeft-click on the column (attribute) name; a menu appears. Choose Edit Formula. A formula box appears.\nEnter mean(Height). Press Apply.\n\n\n\n\nThe formula editor.\n\n\nHooray! You see the mean height for each age in the right row in that new column.\n\nDoes it bother you that the ages are not in order? Click on the colum heading for Age to get the menu, then choose Sort Ascending.\n\n\nThe mean height is a summary of each group. This action of summarizing (sometimes also called aggregating) is the third core data move. We now have three: filtering, grouping, and summarizing.\nA summary doesn’t have to be a mean. It might be a median, or a sum, or just the count (a.k.a. frequency) of the cases in the group. It could even be a percentage, like the percentage of people in the group who have a BMI under 30.\n\nCODAP has a number of functions that serve as summaries. Here are four of the most important:\n\n\n\n\n\n\n\nmean(foo)\nthe mean of foo\n\n\nmedian(foo)\nthe median of foo\n\n\nsum(foo)\nadd up the values of foo\n\n\ncount()\nhow many cases there are"
  },
  {
    "objectID": "01.04-teens2.html#finishing-our-investigation",
    "href": "01.04-teens2.html#finishing-our-investigation",
    "title": "5  800 Children and Teens, part two",
    "section": "5.3 Finishing Our Investigation",
    "text": "5.3 Finishing Our Investigation\nThe new column, MeanHeight, is first-class data like every other column. That means you can make a graph using these mean heights. So do it!\n\nMake a new graph; put Age on the horizontal axis and MeanHeight on the vertical.\n\nYou will see the pattern you might expect: people get taller as they age, up to a point.1\nWe still don’t see the gender differences. Here’s what you do. Watch what happens carefully and make sure you understand it.\n\nDrag Gender left in the table and drop it next to Age.\n\nEach item in the left table splits into males and females. So where there were the 15 ages before, now there are 30 age-gender combinations. Also, the right-hand table is now divided into 30 groups, one for each age-gender combination. The MeanHeight column now automatically shows the mean height for the cases in that group.\n\n\n\nA plot of mean height against age. Since both attributes are over to the left where groups are defined, we get only one point per group.\n\n\nThere are also now 30 dots in the graph, one for each group instead of one for each person. But which dots are for the males and which for the females?\n\nDrag Gender from the table and plop it into the middle of the graph. The points color to show which is which. You should see the graph on the right:\n\n\n\n\n\n\n\nOn the left, we plotted everybody’s Height.\n\n\n\n\n\n\n\nOn the right, we plotted MeanHeight for every age-gender group.\n\n\n\n\n\nNotice what a clean, clear story it tells. Boys’ and girls’ heights are about the same—girls a little taller in the tweens— until about age 13, at which point boys keep growing while the girls slow down. The left-hand graph has all the data, but it doesn’t tell the story as clearly as the right-hand graph.\n\nData-move reflection: When we moved Gender left, we changed the grouping, and took advantage of the summarizing that was already in place."
  },
  {
    "objectID": "01.04-teens2.html#commentary",
    "href": "01.04-teens2.html#commentary",
    "title": "5  800 Children and Teens, part two",
    "section": "5.4 Commentary",
    "text": "5.4 Commentary\nThis is the conceptual heart of this unit.\nGrouping and summarizing are at the center af a huge amount of data analysis. The process is surprsingly deep, too: a group of programmers made this capability in CODAP, and even they didn’t realize how important, how useful it turned out to be. We guarantee you that no student, no matter how brilliant, will fully understand the consequences of that drag-left-in-the-table gesture. One miracle is the way that, when you dragged Gender left to join Age, those two attributes combined to define 30 groups instead of 152, and the formula column adjusted so that it consistently calculated mean height for every group.\nAt the same time, it’s only sorting the data into groups and taking the mean. It is not rocket science. It’s not even AP Statistics. I think there are really two reasons this is hard.\nOne is that there’s so much data, we need the computer to help. That means instructing it—by dragging left3— as to what, precisely, we want it to do. That involves computational thinking, having a sense of what kinds of things the computer can do, and the kinds of instructions that work.\nBut the other reason it’s hard is more insidious: you have to want to group by age and gender, and then take the mean of each group.\nSo, how to teach it? As with the first part of our height investigation, I did much of this lesson as a demo, with students following along. I went slowly, asking and answering questions, going back and repeating sections as students got their screens to show what they saw on mine. You can let students know that if they forget any of it when they’re doing future work, they can find a step-by-step description in this book. The next assignment will give them a chance to practice these skills.\nAs a teacher, you will need to be patient but persistent as students gradually come to understand how this works. You can get additional help and perspective in the “data move” chapters on Grouping and Summarizing.\nAs to developing an intuition about what to group by and why, and how to summarize, my conjecture is that by seeing it a few times, many students will start to get it, even without being able to articulate what’s going on. I also believe, though, that we teachers can gently point out and name the data moves we are making, and remark on their consequences. (“And look! By grouping and summarizing, we now have only 30 points instead of 800. What do you think? Is this graph easier to understand?”)\nWith that prodding, we nudge them towards metacognition.\n\nSummarizing loses information\nDo not think for a moment, however, that summarizing is a panacea. It’s a tool, and we must recognize its limitations and weaknesses. Sure, we went from 800 points down to 30. What happened to those 770 missing points?\nThe sweep of the means in our graph is lovely, and elegantly shows the overall pattern of growth of girls and boys, but it ignores the individuals. It ignores variability.\nTherefore, even if there is no time for a whole lesson on this topic, be sure to ring that chime from time to time. Ask, does this graph mean that every seven-year-old is this height? Or show a graph of only sixteen-year-olds. Notice that some girls are taller than a lot of the boys, and that some boys are really short. Invite students to speculate how they could draw a graph that showed the overall story and, at the same time, showed the variability.\nThat is, rescue students from the tyranny of the center. As a society we are often held in thrall. We hear that median home prices in Westview are higher than in Dust Gulch, so we assume that evey home is Dust Gulch is a shack and every one in Westview a mansion. We read that test scores (which are always mean test scores) are higher in Blue Sky Unified, so we assume that our children will get a good education only if we move there."
  },
  {
    "objectID": "01.03-assignment-1.html#how-do-you-know-when-youre-done",
    "href": "01.03-assignment-1.html#how-do-you-know-when-youre-done",
    "title": "4  A First Assignment",
    "section": "4.1 How do you know when you’re done?",
    "text": "4.1 How do you know when you’re done?\n\nYou have explored the dataset about Californians in 2013.\nYou have found a simple relationship between two attributes.\nYou have made a graph that shows that relationship.\nYou have a CODAP document\n\nit includes the graph.\nit has a text box with a narrative\nthe narrative begins with a claim about the data\nthe narrative goes on to explain what you did and why the claim makes sense\nyou have shared the CODAP doc and emailed the link to your instructor\n\n\n(Sharing is in the “hamburger” menu in the upper left of the CODAP window. If you need more details, here is a set of instructions)."
  },
  {
    "objectID": "01.03-assignment-1.html#a-new-dataset",
    "href": "01.03-assignment-1.html#a-new-dataset",
    "title": "4  A First Assignment",
    "section": "4.2 A new dataset!",
    "text": "4.2 A new dataset!\nTime to look at some new data! You will look at data for 1000 Californians in 2013.\n\n… or you can at least start in the live illustration below.`\n\n\n\nHere are some questions to answer to make sure you understand the data set.\n\nThe people are from a specific age range. What is that range?\nWhy do you suppose the dataset has people from that age range?\nDescribe the oldest person in the dataset.\nDescribe the person in the dataset with the highest income."
  },
  {
    "objectID": "01.03-assignment-1.html#some-elaboration",
    "href": "01.03-assignment-1.html#some-elaboration",
    "title": "4  A First Assignment",
    "section": "4.3 Some elaboration",
    "text": "4.3 Some elaboration\nExplore. It’s essential that you mess around with the data. Here, that means make graphs. More than one. Drag different attributes to the axes to make different graphs. Try stuff in the palettes at the right edge of the graph (like the “ruler” and the “eyeball”). Select things in the graphs and see what’s selected in other graphs.\nSimple relationship. For this assignment, that means it’s a relationship between only two attributes. We have already explored more complex relationships: when we filtered the height-and-gender data to show only 10-year-olds, we were looking at three attributes: age, gender, and height. For this assignment, just look at two.\nFor example, with the previous dataset, you might have plotted Height against Weight. Or you might have made a graph of Gender and plopped BMI in the middle.\n\n\n\n\n\n\n\n\n\n\nA Narrative explains what you did. Communication is part of data science (as it is a part of everything). You need to be able to explain yourself clearly and concisely. Use complete sentences. For this assignment, you will not need very many!\nA Claim is a statement about the data that might be true or false. It doesn’t matter which, though we generally make claims we think are true. In the data with heights, with a graph of Height against Weight, you might claim: Heavier people are taller.\nA claim does not have to be dramatic. A null result is still a result! Using the gender-BMI graph, you might claim: There’s no relationship between gender and BMI.\nOf course, your claim will be about this new data set (1000 Californians), not the old one (800 children and teens)."
  },
  {
    "objectID": "01.03-assignment-1.html#commentary",
    "href": "01.03-assignment-1.html#commentary",
    "title": "4  A First Assignment",
    "section": "4.4 Commentary",
    "text": "4.4 Commentary\nData science demands communication. We create graphs and other visualizations in order to get our ideas across. They often illustrate a narrative, a story about the data and about our investigation, written out as text.\nStudents (and teachers) may not be used to writing in a mathy class. Furthermore, the style of writing will be different than what they’re used to in, say, English or History. It’s technical writing, explaining procedures and reasoning. Still, the objective is the same: to communicate clearly.\n\nMini-projects\nAnother challenge is that this kind of assignment is a balancing act: it’s a mini-project, less straightforward than a set of problems to solve. But it’s not a big project; we want students to do it reasonably quickly, and not get weighted down with deadlines, drafts, and expectations. That’s one reason why it’s limited to one text box.\nWhen students are not used to this kind of assignment, the first few are often terrible. That’s OK. They (and you, the teacher) haven’t figured out what you’re looking for. (Though that’s why we had that section, “how do you know when you’re done?”) The work will improve with practice, but only if students do at least three of these. Whence, short.\nThis initial assignment is, as advertised, designed partly to get students through the mechanics: negotiating sharing in CODAP and turning things in by emailing a URL. It’s also conceptually simple, perhaps confusingly simple, in order to make it possible to focus on those mechanics.\n\n\nDo an example?\nTo that end, as a teacher, you could demonstrate this assignment in front of the class. Here are two ideas for graphs to start with:\n\nIncome by Gender\nAge by marital status\n\nPick one, show how to make the graph and rescale it, and brainstorm with the class what you could say in two or three sentences.\nFor example:\nI claim that people who have never been married are generally younger than married people. This makes sense because once you’re married, you can never go back to being never-married. As you can see in the graph, the mean age for never-married people is in fact lower. There is still overlap, however: some unmarried people are pretty old, like, over 40.1\nHaving given an example, you could then tell students to do something like that, only different.\nBe sure to include how to share a CODAP doc.\n\n\nRescaling and Reordering\nMany students will probably do something with income. The problem is that income is highly skewed, and that means that space in the graph is scrunched down at the low end. If you put box plots or other measures on the data and compare males and females, they will not look as different as they might.\nSo to communicate a difference you want to highlight, you might rescale the axes even if it means losing some of the high-income outliers. (This gives you a chance to talk about whether that’s a good idea and how to cope with it.)\n\n\n\nGender by income\n\n\n\n\n\nSame thing, scaled\n\n\nIn any case, to rescale axes in CODAP, grab the numbers on an axis near one end and drag. In the illustration, we grabbed the top axis up at about $350,000 and dragged to the right.\nYou can do the same thing with a categorical axis, but there, dragging re-orders categories.\n\n\nDebriefing\nAfter you look over the assignments, you will want to discuss them with the class. Of course you can use this time to praise innovation and tenacity, and to point out where the work needs improvement. Here are some additional suggestions:\n\nHave some students present their work, especially if it investigated unusual attributes or used interesting new CODAP techniques.\nPoint out areas no students might have explored. For example, our two ideas above are both numerical attributes grouped according to a categorical one. What about two categoricals—what would that look like? (For example, gender and marital status.)\nPoint out CODAP features that need revealing such as rescaling axes or (xxx) fusing dots into bars.\nCompare putting one attribute on each axis as opposed to putting one on an axis and the other “plopped” into the middle as a legend. Actually put someone’s graph up, and then make another graph done the other way.\nAsk about the data in general, e.g., ask about what the income distribution looks like, and what that means in context (there are very few people making a lot of money, and a lot making not much; this is also a great time to discuss or review why median is a good alternative to mean for this data)."
  },
  {
    "objectID": "01.05-assignment-2.html#how-do-you-know-when-youre-done",
    "href": "01.05-assignment-2.html#how-do-you-know-when-youre-done",
    "title": "6  A Second Assignment",
    "section": "6.1 How do you know when you’re done?",
    "text": "6.1 How do you know when you’re done?\nYou have a Google Doc (or whatever format your instructor requires). It contains:\n\nYour name.\nA simple claim. Pick one:\n\nPeople with more education make more money than people with less education.\nPeople who get on BART at SFO generally get off at Powell Street.\n\nA graph with the first-look, obvious results.\nA description of why that simple approach might not be enough to really explain the data\nA description of what you did to further the investigation (“dig deeper”). In this description,\n\nYou have grouped the data by dragging an attribute (or attributes) to the left in the table.\nYou have calculated some summary value (e.g., a mean or a sum) for each group by making a new column with a formula.\n\nThe results of that deeper investigation (with, e.g., a new graph of the data)\nA conclusion: is the story any different now?\nPossibly, ideas for additional “dig deeper” activities with this data set, and…\nA link to a shared CODAP doc (like last time) so the instructor can see what you did.\n\nYour Google Doc is probably no more than two pages long. Be sure to set permissions so your instructor has edit access.\n\nWhere to get the data\nInstead of using a “canned” data set, for this assignment you will use a data portal. In CODAP, this appears as a window where you specify what data you want, and then press a button to get it.\n\n\n… BART is a regional transit system in the San Francisco Bay Area. SFO is the San Francisco International Airport. You can read all about the BART data portal in a separate chapter.\n\n… You can get data on education, income, race, gender, etc., and choose how many cases. Go to the options tab to choose what attributes you want.\n\nACS stands for “American Community Survey,” which is run by the Census Bureau, and collects data between the decennial Census years. The portal for the Census data looks like this:\n\n\n\nACS data portal. Notice how we have requested 1000 people and have selected EmplStatus (employment status) as an attribute.\n\n\n\n\nElaborations\nGoogle Doc. If you do not know yet how to make a Google Doc, it’s time you learned. If you need to know more, ask a friend or Google it.\nIncluding a Graph. How do you get the graph from CODAP into your file? At the moment (early 2020), you can’t just copy and paste. Here are two alternatives:\n\nClick the camera palette in the graph and…\n\nchoose Export Image (you can also Open in Draw Tool).\nchoose Local File and pick where you want to save the file.\nafter saving the file, import it (it’s in .png format) or do a copy/paste into your Doc.\n\nUse a screen-capture utility to get an image of your graph, and then paste it into your doc.\n\nPro tip: Be conscious of space. Noobs often just paste huge graphs into their documents and leave them that way. After pasting, shrink the graph so that it’s a reasonable size. What’s reasonable? When you print it out, it should still be easy to read any text. For a typical CODAP graph, that’s no bigger than about 1/6 of a page. If you know how to wrap text around a graphic, sometimes that can look very professional."
  },
  {
    "objectID": "01.05-assignment-2.html#example-gender-and-income",
    "href": "01.05-assignment-2.html#example-gender-and-income",
    "title": "6  A Second Assignment",
    "section": "6.2 Example: Gender and Income",
    "text": "6.2 Example: Gender and Income\nHere’s an example of the kind of thing we have in mind:\nSuppose we were interested in gender and income. The simple approach is to (duh) plot gender and income. The graph alone looks vaguely like the men get more, but if you put the median on the graph, and rescale it, it’s really obvious:\n\n\n\nTotalIncome split by Gender, with lines showing median income for each group.\n\n\nThis is what we probably expect: men earn more than women. If we look at median values, men earn $17,000, versus $9,200 for the women. But does that tell the whole story? How could we dig deeper? (…as required in the assignment)\nWe might ask:\n\nIs it possible that the incomes really are equal, but we’re looking at it wrong?\nCan we be more nuanced? For example, is there some other factor that affects income?\n\nLooking at the graph, see the large number of people who seem to earn zero—or close to it? That spike at zero is taller for women. Maybe that’s because more women work in the home, and are not paid.\nSo maybe the incomes for people with jobs are equal between men and women, but because more women do not get paid, their median income is lower overall. This reasoning is an example of exploring whether we’re looking at it wrong, and that there is another factor—employment—that affects income. That is, it’s not just gender.\nTo test this idea, let’s just look at people with jobs. It turns out that we can get data for an attribute (a column) called EmplStatus for “employment status.”\n\nThat means you could filter to focus your investigation on people with jobs. That way, you can explore whether men with jobs generally earn more than women with jobs.\n\nTry that in the live illustration below. The “employment status” attribute is at the far right of the table.\n\n\n\nYou should find that the men still earn more. If you hide everyone but those that are “Civilian employed,” the men earn $45,000 to the women’s $30,000. So the fact that more women do unpaid work does not completely explain the difference in income.\nNotice that this will only work if you have downloaded EmplStatus data. If you get partway through your investigation and realize that you wish you had downloaded something else, or something more, you can’t add additional columns to the cases you already have. But remember: starting over is free. Just go back and get fresh data with the attributes you want."
  },
  {
    "objectID": "01.05-assignment-2.html#dont-forget-to-drag-left",
    "href": "01.05-assignment-2.html#dont-forget-to-drag-left",
    "title": "6  A Second Assignment",
    "section": "6.3 Don’t forget to drag left!",
    "text": "6.3 Don’t forget to drag left!\nWe’ve seen what we mean by “dig deeper,” but don’t forget to read the assignment. It also expects you to do that “drag left” grouping move. In this case, that would be dragging Gender left to make groups of males and females. Then you would make a new column (maybe called medInc) in which you would calculate the median income for each gender.\nLook back at the section where we made groups by age if you don’t remember how.\nThen think about how you can use grouping to help with your “dig deeper” work.\n\nWhen you drag Gender to the axis of a graph, CODAP helps you do a grouping data move. Then, when you put the median on the graph, that’s summarizing. For this assignment, though, we want you to make those moves explicitly—grouping by dragging left, summarizing in a new column with a formula— because (a) it’s good practice and (b) it’s more flexible."
  },
  {
    "objectID": "01.05-assignment-2.html#digging-deeper-philosophy-section",
    "href": "01.05-assignment-2.html#digging-deeper-philosophy-section",
    "title": "6  A Second Assignment",
    "section": "6.4 “Digging Deeper” and skepticism",
    "text": "6.4 “Digging Deeper” and skepticism\n“Digging deeper” means adding nuance to your investigation, bringing out important trends and effects that might not be obvious at first. It’s like fleshing out a sketch and making it more detailed.\nBut it’s also about being skeptical. When you make an initial, obvious graph, and draw some conclusion or make some claim, a skeptic steps back and wonders whether that’s really correct. Is there some other explanation for what you are seeing?\nYou can think of this as a “yes, but” attitude. It’s also the role of a “devil’s advocate”—someone whose job is to tear down an argument. If the devil’s advocate succeeds, that’s good, because without that skepticism, the conclusion would have been wrong. And if the devil’s advocate fails, that’s good too, because by passing that test, the original conclusion is stronger.\nSkepticism is not always confrontational, however. You can also think of it as a “yes, and” attitude: the claim may be right as far as it goes, but there may be other important considerations.\nIn the case of gender and height, we saw that, indeed, males were generally taller than females, but only after about age 13."
  },
  {
    "objectID": "01.05-assignment-2.html#commentary",
    "href": "01.05-assignment-2.html#commentary",
    "title": "6  A Second Assignment",
    "section": "6.5 Commentary",
    "text": "6.5 Commentary\nYour job as a student is to try really hard to understand the bit about dragging left to make groups, and making new columns with calculations. If you’re feeling challenged enough already, pick the claim about education and income.\nHere’s some advice for how to explore and get comfortable:\n\nMake a graph, any graph. Try different groupings: drag one attribute left, and see what happens. Then drag the first one back and then try a different one.\nTry making a group (like for education) and adding an additional attribute (like gender) to the left. See what groups exist now.\nSelect groups on the left by clicking in that row. See what gets selected in the graph.\nWhen you make a column and give it a formula, try different formulas and see what happens.\nRemember that the name of your “formula” column is just a name. It’s not the formula. So if the column is named MeanIncome and the formula is median(income) you will get the median. (You should change the name!)\nChange what’s on the graph. Make more graphs! Try graphs with a calculated attribute on an axis.\n\nOh and:\n\nTry dragging TotalIncome to the left and see why that’s a terrible idea.\n\nIf you just mess around for a while like that, you will probably see something that addresses your claim. You may even have created something that will be perfect for your “dig deeper” section.\nTwo last key pieces of advice:\n\nWhen you make a grouping or summarizing move, stop for a moment and be sure you understand what changed. What does the organization of the table mean now? What are the groups? How did the numbers change? What does this one particular number mean?\nDon’t beat yourself up if you still feel a bit “awash.” This is surprisingly complicated, especially if you have more than one attribute on the left. It takes time. You will get it.\n\n\nQuestions and Claims\nIn many school statistics (or science) projects, you begin by posing a “research question.” In this case, the question might be, “who earns more—women or men?” It will come as no surprise that the answer is “men.” That’s what we have heard about our society today (or in 2013, when the data were taken). So “who earns more” might seem to be a silly question.\nInstead of a question, sometimes it makes more sense to make a “claim.” This is a statement about what may be true, that you are going to investigate. Our claim might be, “men earn more that women.”\nFor this assignment, we gave you a claim, so you don’t have to worry about making one up. But think about other claims you might make about your data (after all, in the next assignment you get to pick your claim), or how your claim might change as you dig deeper and add nuance to your analysis.\nIn stating your claim, you might also want to say why you think your claim is true. It’s great to have a reason for your claim, but be careful: In most cases for this unit, you won’t have data that will tell you anything about the reasons behind some effect.\nFor example, you might say that you believe people with more education will earn more because they are qualified for higher-paying jobs. Your data can tell you whether they earn more—we have data about education level and income— but we don’t have any data about the educational requirements of the jobs they have.\nIt’s fine to include reasons if you have them, but keep any claim about your data separate from the reasons just as we did above. Here’s how to do it wrong:\nI claim that people with more education are better-qualified for high-paying jobs, and as a result they earn more.\nSee how that’s different? You can’t make a graph about job qualifications. Keep it separate, even if the separator is just the word “because.”"
  },
  {
    "objectID": "01.06-project.html#whats-the-same",
    "href": "01.06-project.html#whats-the-same",
    "title": "7  A Small Project",
    "section": "7.1 What’s the same?",
    "text": "7.1 What’s the same?\nThe data: pick from\n\n(has income, education, race, etc)\n\n(2015–2018)\n\n\nWhat do I do?\n\nPredict something. Make a claim.\nDo a quick investigation. Display a table or graph. Reflect on what it says.\nDig deeper: Figure out how that can be enhanced (make it more nuanced; or investigate something related, inspired by the original; or play devil’s advocate, etc.)\nCollect data, analyze it, and make a table or graph that speaks to the issue\nReflect on what you found out. Possibly, dig even deeper and reflect again.\nMake a shared link to your CODAP document.\n\n\n\nLogistics\n\nMake it a Google Doc for simplicity.1\nGive your instructor edit permission on the document. They will get an email with the link, so that’s how you can turn it in.\nThe report should include a link to the shared CODAP document."
  },
  {
    "objectID": "01.06-project.html#whats-enhanced",
    "href": "01.06-project.html#whats-enhanced",
    "title": "7  A Small Project",
    "section": "7.2 What’s enhanced?",
    "text": "7.2 What’s enhanced?\n\nYou may pick your topic\n\nAdvice: mess around with the data before you decide. Make graphs. Think about what they mean. A question or a claim will occur to you. Note:\nIf you choose BART data, the BART Data Portal chapter has a section with suggestions for BART topics. That section also has instructions about the “secret meeting game.” Finding the secret meeting is a suitable topic for this investigation.\nIf you choose Census/ACS data, you may have already started looking at social-justice topics in a previous assignment, through income differences. That’s a fruitful direction (though not the only one).\n\n\n\nOther enhancements\n\nIf it makes sense (and it probably will), do a second “dig deeper” cycle.\nCommunication is part of data science:\n\nMake this work stand alone. Do not assume that the reader has read the assignment.\nSimilarly, give it narrative sense\n\nIt’s not a numbered list of steps! Make paragraphs! Make them flow!\nThe opening paragraph, or some text near it, should motivate the investigation. Why is this interesting—even a little bit?\n\nPay attention to space. Usually, if you just paste your graph into the document, it will be bigger than it has to be.\nMake your graphs sing!\n\nRecoding might be useful: making a new column that’s more what you want\nBriefly describe the data at an appropriate place in the report, including the sample size(s).\nWhen you present an analysis, explain what you did. If you used “data moves,” say so! Did you filter? Did you write a summary formula (like with sum( )? What was it?)\nDid you drag a column to the left to make groups? Why did you choose that column?\nDo not exceed three pages.\nVital: somewhere, perhaps in your conclusions, reflect on this experience. What interests you about this? (Or not; be honest!) How is this different from your experience with school math classes?"
  },
  {
    "objectID": "01.06-project.html#commentary",
    "href": "01.06-project.html#commentary",
    "title": "7  A Small Project",
    "section": "7.3 Commentary",
    "text": "7.3 Commentary\nIdeally, the previous assignment prepared students well for this project. Do plan on class time to help them, and remind them of various resources such as their peers and this book.\nDespite any warnings you might have given about keeping the projects small, some students will probably want to do things that go beyond what you have covered in class. Here are three that happened to us:\nRecoding: Suppose they have income and want to get rid of the number. Instead, they want categories, like “none,” “not much,” and “a lot,” based on some cut points in the numeric income attribute. This is called recoding.\nThey will want a new attribute, a new column, whose value depends on the income attribute. (They should not eliminate that original, but make a new one.) There are two basic strategies: make a formula (which in this case will have some if() statements); or do something that involves typing values into cells. If you have 1000 cases, the typing strategy had better be pretty strategic!\nThey will find some ideas about how to do this in the chapter on the calculating data move.\n\nThis data move, which we call calculating, resembles summarizing, but it creates a new value for each case in a collection rather than a value that summarizes a group. Both data moves often—but not always—use formulas.\n\nAnother classic use of recoding comes up with the Hispanic attribute. It has many categories (notHispanic, Mexican, Guatemalan, Cuban, etc…). It would be great, in some situations, to have a simpler version of that column, maybe called LatinX with only two values: Sí and No. That section explains an easy way to do that.\nAdding data from elsewhere: Sometimes you want to connect the data from two datasets together. This is another data move, called joining, which you can read about in its own chapter.\nJoining can be ridiculously complex, but our student’s idea was reasonable and straightforward: they wanted to compare education levels for people from the richest five States with education levels from the poorest five States. The ACS data did not rate States by wealth, but it’s easy to find a listing on Google.\nSo they simply asked only for data from those ten States, and then recoded the state names into a new column with values rich and poor.\nGetting your own data: Some students may find data about something else that excites them online. If they find a csv file with the data they want, they can drag and drop it into CODAP as described in this section on importing files.\nThe danger for you, the teacher, is that they get data for which grouping and summarizing don’t make sense. So you may want to have them get approval if they stray too far from the data sources we suggest.\n\nIncome inequality and Census data\nIf a student chooses Census data and your claim has a social-justice feel to it, there’s a good chance that it’s at least partly about income inequality.\nNote that there are (at least) two different ways we talk about income differences:\n\nSome groups earn more than others. You often hear about this in the news as a “gender gap” or “race gap” in income.\nWithin a group, the difference between the richest and the poorest might be especially large. You often hear about “rising income inequality,” that is, that difference now is bigger than it was in the past. Or we might compare the USA with, say, Sweden, where incomes are more equal. To do this comparison, you need a number: a measure of income inequality. One such measure is the “Gini coefficient.”\n\nTrying to come up with your own measure of income inequality is an interesting challenge.\n\n\nAssessing the Project\nErnie Chen, the teacher-of-record for the Applied Math class where we ran this in 2020, used this simple 20-point plan. Use or modify as you wish:\n\n\n\nClearly-stated claim\n1\n\n\nInitial graph\n3\n\n\nDoes the graph show or refute initial claim?\n1\n\n\nGrouping: dragging to the left\n3\n\n\nSummary column creation\n3\n\n\nEnhancement: dig deeper\n4\n\n\nCommunication/writing/readability\n4\n\n\nReflection\n1"
  },
  {
    "objectID": "02-portal-part.html",
    "href": "02-portal-part.html",
    "title": "Data Portals Overview",
    "section": "",
    "text": "The next few chapters describe some data portals you can use to get data for analysis. In a CODAP data portal, you can pick what attributes you get, which cases you will choose from, and/or haw many cases you will download.\nWe use these portals because some datasets are so huge that it’s impractical to work on all the data at once. The BART data portal, for example, has over 40 million cases. The portals also help you get clean, pre-organized data, so that you can get to the data analysis more quickly.1\n\nMaking these choices is also a data move in itself: it’s filtering in advance.\n\n\nCensus/ACS data portals\nThese two portals—used in the assignments and the project in the lessons—supply you with data about individuals from the United States Census and the American Community Survey (ACS, also part of the Census).\n\n\nBART data portal\nThis portal is a window into public transit in the San Francisco Bay Area. You get ridership numbers for every hour beween every pair of stations—for four years, from 2015 to 2018 as of this writing.\nCan you see the commute in this graph?\n\n\n\nFive days of data from Orinda to Embarcadero Station\n\n\n\n\nNOAA data portals\nThis chapter describes two related portals that get you data direct from the National Oceanic and Atmospheric Administration (NOAA).\nYou pick weather stations and retrieve, for example, temperatures or precipitation. You can get daily or monthly values.\n\n\n\nLos Angeles and San Francisco: average monthly temperatures, 2010–2019.\n\n\n\n\nNHANES data portal\nRemember the data where we got the heights of 800 5- to 19-year-olds? It came from this portal. With the portal, you can change how many people you get, and what information you retrieve.\nDoes it really include data about having sex? You bet:\n\n\n\nFor 300 people over 30, how old they say they were when they first had sex.\n\n\n\n\n\n\n\n\nIn real, professional data science, getting the data in shape for an analysis is often most of the work. There is still plenty for you to do here in organizing your data; making the portals is part of a reasonable balance for learners.↩︎"
  },
  {
    "objectID": "02.01-census-acs.html#the-small-portal",
    "href": "02.01-census-acs.html#the-small-portal",
    "title": "8  The Census/ACS Data Portals",
    "section": "8.1 The “small” portal",
    "text": "8.1 The “small” portal"
  },
  {
    "objectID": "02.01-census-acs.html#the-big-portal",
    "href": "02.01-census-acs.html#the-big-portal",
    "title": "8  The Census/ACS Data Portals",
    "section": "8.2 The “big” portal",
    "text": "8.2 The “big” portal\n\n\nYou can also access this portal from any CODAP document by choosing Microdata Portal in the Plugins menu."
  },
  {
    "objectID": "02.04-nhanes-portal.html",
    "href": "02.04-nhanes-portal.html",
    "title": "11  The NHANES Data Portal",
    "section": "",
    "text": "This brief chapter describes a data portal that gives you access to several thousand cases from the 2003 National Health And Nutrition Examination Survey (NHANES). This is the dataset where we got the 800 children and teens we used in two of the lessons in this book.\n\n\n\n\nThe attributes panel in the NHANES portal.\n\n\nA box at the top of the portal lets you choose your sample size, and a button will get your data. When the data downloads, a CODAP table appears with the data in it.\nThe portal has three panels:\n\nsummary: This describes what you have chosen to download. It includes a list of the attributes you have chosen.\nattributes: (Shown in the illustration.) Here you can choose which attributes you want. You can pick them from several different broad categories. This is similar to the Census/ACS portal design you have already used.\ncases: This lets you limit the range of ages of the people you will get. You do not have to fill in both boxes."
  },
  {
    "objectID": "02.02-BART.html#the-basics",
    "href": "02.02-BART.html#the-basics",
    "title": "9  The BART Data Portal",
    "section": "9.1 The basics",
    "text": "9.1 The basics\nThe data portal—the thing you interact with—lets you specify the stations and the day and hour. You will press a button to get the data, and it will be sent directly into CODAP so you can analyze it\nTry this in the live illustration below:\n\nLook at the panel; notice what day we’re looking at and what stations (Orinda and Embarcadero) we have specified by default.\nPress the get data button. A table will appear with data.\nMake a graph. Put when on the horizontal axis and riders on the vertical.\nNow: Orinda is a suburb. Embarcadero is in downtown San Francisco. What’s going on in the graph?\n\n\n\n\n\nHere are more things to do. This will further orient you to the data and its possibilities:\n\nAs you probably figured out, the graph shows people going to work. Let’s see them coming home. Press the swap button to exchange Orinda and Embarcadero. You should see this:\n\n\n\n\nAfter swapping Orinda and Embarcadero.\n\n\n\nPress get data. The return trips appear in the table and in the graph.\nIt would be great to color-code the points. Plop startAt into the middle of the graph.\nLet’s get data from other days! Change the date to the next day (probably April 19, 2018) and change how much data? to 7 days.\nPress get data. A whole week of data appears.\nNow drop day into the middle of the graph.\nInterpret what you see!\n\nMini-commentary: Notice that your understanding of how the world works informs how you interpret this dataset and the graphs you make. Knowing about weekends, for example, explains why there’s such a big dropoff in ridership."
  },
  {
    "objectID": "02.02-BART.html#more",
    "href": "02.02-BART.html#more",
    "title": "9  The BART Data Portal",
    "section": "9.2 More!",
    "text": "9.2 More!\n\nLimits to downloads\nThe controls for getting data are fairly self-explanatory, but a few things bear noting:\n\nThere is a limit to how much data you are allowed to get in one request. As a consequence, you may have to be strategic in the data you get. For example, a request for data from Embarcadero to any station gets (of course) fifty times as much data as a request to a single station. So while you can get a whole day of that, you can’t get a week or a month in a single request. That’s fine; being prudent about data is part of data science!\nTo use the between any two stations option, you have to restrict the hours to get only a single hour. (Otherwise it’s too much data.)\nIf you make multiple requests, and they overlap, you will wind up with two copies of the common data.\n\n\n\nThe thing about time\nEvery case has several different attributes for time:\n\nwhen is a “date-time”, where the time is the beginning of the hour the data are from.\nday is the day of the week, which is categorical but will appear ordered correctly.\nhour is an integer, the hour, in a 24-hour system.\ndate is the day (as a date) without the time. It’s categorical.\n\nThese nearly-synonymous attributes help you make different kinds of comparisons. For example, if you want to explore weekly commute patterns, you might make a graph like this, using when for the time:\n\n\n\nOne week of data from Orinda to Embarcadero.\n\n\nBut if you want to overlay the days on top of one another, you should use hour. In this graph, we have selected Friday; you can see that not only do fewer people commute, they do so later:\n\n\n\nThe same week of data from Orinda to Embarcadero, but overlaid with Friday selected.\n\n\nWe made these attributes as a convenience for you as a data-science learner. When you bring in your own data, it will probably be in one format, ansod you will have to do all these transformations yourself. If you need to know more, there’s a whole chapter on the issue of dates and times."
  },
  {
    "objectID": "02.02-BART.html#bart-suggestions",
    "href": "02.02-BART.html#bart-suggestions",
    "title": "9  The BART Data Portal",
    "section": "9.3 Suggestions for investigations",
    "text": "9.3 Suggestions for investigations\nHere are some things you can investigate with the BART data:\n\nWhen people take BART from SFO, where do they tend to go?\nMr Erickson thinks that people tend to leave work early on Friday. True? Myth? Does it depend?\nFrom which station do the most people come downtown during the morning commute?\nWithout looking at a schedule, find a day game in 2015 when the Giants were playing at home at AT&T Park. (What station(s) are relevant? Embarcadero and Montgomery.) All you’re trying to do is find a date with a day game. Then check to see if you’re right at this site; get the box score, it has the starting time.\nA’s fans: do the same, check at this site. (Coliseum station.)\nTry to estimate the total number of people who went to Civic Center Station for Pride, June 28, 2015. Find the date for Pride 2018 without looking it up and do the same.\nWhat other events or phenomena can you investigate? Think of one, study it! (Could be a one-time event, a repeating event, or something that happens every day, or…)\nPlay the secret meeting game, described below."
  },
  {
    "objectID": "02.02-BART.html#the-secret-meeting-game",
    "href": "02.02-BART.html#the-secret-meeting-game",
    "title": "9  The BART Data Portal",
    "section": "9.4 The secret meeting game",
    "text": "9.4 The secret meeting game\nA secret meeting is being held weekly near a BART station. Find it!\n\nYou know the meeting is on Tuesdays, you know it’s at Hayward, and you know that 160 people attend. But you don’t know the time. Collect data and make a display (or displays) to support a convincing argument that you know the time. Convince your neighbor!\nLet’s play that again. Go to the options panel. If necessary, abort the game. Then set Thursday, 160, and 12 noon, but choose surprise me for the location. Then solve the problem. How did you do it? What data did you collect, and why? (Remember, the possibilities for location are Orinda, Hayward, San Bruno, and Pleasant Hill.)\nAgain, but this time,\n\nReduce the size of the meeting\nSet surprise me for two of the remaining parameters"
  },
  {
    "objectID": "02.02-BART.html#commentary",
    "href": "02.02-BART.html#commentary",
    "title": "9  The BART Data Portal",
    "section": "9.5 Commentary",
    "text": "9.5 Commentary\nBesides giving you a chance to interact with a truly huge dataset (albeit only a little at a time), the BART data is a great environment for learning how to think about data in order to get what you want.\nSuppose you decide you’re going to investigate the Giants home game against the LA Dodgers on Tuesday, April 21, 2015. You want to know how many people took BART to the game. How can you figure that out?\nYou’ll be looking at people going to Embarcadero and Montgomery stations. You expect to see a bump in the ridership. But at what times? You should probably look at the data to see when the bump is. But what data? To Embarcadero and Montgomery, but from where? Everywhere, right?\nBut if you do that, you’ll have to add up the data from different stations. Sounds like dragging left and making a new column. But what do you drag left? The station? No.\nTo decide, one strategy is to ask yourself, “What graph do I want to make? What will it look like?”\nThe one in my head has a bump, with riders on the vertical axis and hour on the horizontal. That suggests that we want hour on the left side of the table. Then when we add, we’ll get the sum of the riders—for each hour. Yes. That sounds right.\n\n\n\nSum of riders (called total) from anywhere to Embarcadero for Tuesday afternoon, April 21, 2015.\n\n\nYou can see that there are a slew of similar problems to solve. None of them are mathematically sophisticated, but they can all be challenging and confusing. Here are three:\n\nHow do you add the people from Embarcadero and Montgomery together?\nHow do you account for the people who would have been riding BART anyway whether there was a game or not?\nHow do you know if the bump is for a game or for something else?\n\n\nAs you think about these, notice that the solutions use data moves:\n\nfiltering to get the right data,\ngrouping to set up appropriate comparisons, and\nsummarizing (or aggregating) and calculating to add up the riders and subtract out the “background” traffic.\n\n\nThe very idea of the “background” is an interesting topic. It smells very sciency, is hardly ever talked about in ordinary classwork, and requires only common sense to understand and cope with. To tell how many people went to the game on this Tuesday, April 21, 2015, we need to compare the pattern of ridership to some day when there was no game. Does it matter which day we compare? It might. Maybe it would be best to choose a different Tuesday. But we can’t be sure; maybe we should compare a few different non-game days. And so forth.\n\n\n\nSum of riders from anywhere to Embarcadero for three Tuesdays in April..\n\n\nThis is also closely related to the idea of controlling variables, which we talked about in the commentary about gender differences in income (xxx)."
  },
  {
    "objectID": "02.03-noaa-weather.html#the-little-one-by-tim",
    "href": "02.03-noaa-weather.html#the-little-one-by-tim",
    "title": "10  The NOAA Weather Portals",
    "section": "10.1 The little one (by Tim)",
    "text": "10.1 The little one (by Tim)\n\nxxx replace this graphic with a newer one\nThis plugin will let you choose…\n\nfrom a small selection of diverse stations, with a leaning towards California (because it was created for a workshop in the LA area);\nwhether the data are daily or monthly; and\nfrom among a small selection of data, e.g., precipitation or average temperature.\n\nThe software lets you choose dates back to roughly 1900.\n\n\n\nThe ‘little’ NOAA data portal\n\n\nThen there’s this thing about a “spreader.”\nThis is because we have been exploring a concept in data organization called “tidy data.” I will not go into it here; suffice to say:\n\nIf you get only one type of data, you will not have a problem. Just put value on the vertical axis.\nIf you get two temperatures (e.g., tMin and tMax) do the same, and plop what into the middle of the graph.\nIf you mix the data, like you get precipitation and temperature, your graphs will be unruly unless you use the spreader. Experiment, write me an email, or just use the bigger, more modern NOAA data portal!"
  },
  {
    "objectID": "02.03-noaa-weather.html#the-big-one-by-concord",
    "href": "02.03-noaa-weather.html#the-big-one-by-concord",
    "title": "10  The NOAA Weather Portals",
    "section": "10.2 The big one (by Concord)",
    "text": "10.2 The big one (by Concord)\nWhen the nice people at Concord Consortium saw my NOAA Portal they realized they needed something like it for a project they were doing. So they took my code and expanded upon it.\n\n\nYou can also access this portal from any CODAP document by choosing NOAA Weather in the Plugins menu.\n\nThere are differences:\n\nVery cool: It has a map you can use to choose from among a zillion stations.\nYou can only get one station at a time.\nIf you want more than one measurement, they appear in different columns. This is usually what you want. Also, this means you do not have to worry about tidy datasets or spreading.\nThe data go back, apparently, only to 1970.\nI have had trouble entering dates sometimes.\n\n\n\n\n\nThe ‘big’ NOAA data portal. Mt Washington in New Hampshire is selected"
  },
  {
    "objectID": "01.20-teens.html#exploration",
    "href": "01.20-teens.html#exploration",
    "title": "3  800 Children and Teens, part one",
    "section": "3.1 Exploration",
    "text": "3.1 Exploration\n\nYour first task is to explore the data. Here are some questions you can address:\n\nWhat attributes do you expect to be related?\nCan you show that relationship in a graph?\nWhat other relationships can you show?\nTry making more than one graph, and then select points in one of them. What happens? How might that be useful?\nWhat do you think the units are for these values? (especially Weight, Height, and Pulse)\nWhat’s BMI? If you don’t know, look it up."
  },
  {
    "objectID": "01.20-teens.html#a-specific-question-who-is-taller",
    "href": "01.20-teens.html#a-specific-question-who-is-taller",
    "title": "3  800 Children and Teens, part one",
    "section": "3.2 A Specific Question: Who is Taller?",
    "text": "3.2 A Specific Question: Who is Taller?\nWho is taller, males or females?\nStereotypically, we probably agree that, in general, males are taller. But is that really true? Let’s use the data to find out.\nThe next illustration contains a CODAP document that graphs Height against Gender. That’s the obvious way to look at our question.\n\n\n\nIt looks as if the pile of males is a bit higher up in the graph, that is, they’re taller. But how much? Let’s find the mean.\n\nOh, and if you’re reading this book in a browser, that illustration is live. You don’t have to make a separate CODAP window for this bit.\n\n\nClick on the graph to select it.\nAt the right of the graph, click on the “ruler” icon. A panel opens up. We call these things “palettes.”\nClick the checkbox for mean. (Of course you can try other options as well.)\nHover over the mean lines that appear. You can see the values.\n\nYou should find that the average height of males is about 10 (cm) greater than the average height of females. So that shows that our preconception (males are taller) is correct.\nBut, but…\nIf you stop and think a bit, our graph is deeply bogus. It’s a bad analysis. Why?\nTry not to read ahead…\n\nIf you’re a student in a class, discuss with your group.\nIf you’re studying alone, think about this before scrolling down to see what we think."
  },
  {
    "objectID": "01.20-teens.html#making-the-question-more-specific",
    "href": "01.20-teens.html#making-the-question-more-specific",
    "title": "3  800 Children and Teens, part one",
    "section": "3.3 Making the Question More Specific",
    "text": "3.3 Making the Question More Specific\nThe problem is that we haven’t taken Age into account, and Age is much more important than Gender in determining height. The whole long tail of short people—for both males and females–is made up of little kids. If you’re not convinced, drop Age into the middle of the graph.\nGo ahead, we’ll wait.\n\nIn general (the graph says), the short people are younger. Make sure you can explain how the graph shows that. What is it about the colors that says short people are younger?\n\nStill, it’s a confusing graph. Let’s make it simpler.\nInstead of looking at everybody we have, ages 5–19, let’s just look at one age: 10-year-olds. First we’ll filter the graph so it shows only 10-year-olds. Then we’ll compare the heights of those boys and those girls.\nYou get a fresh, live document below. Follow these steps for the filtering:\n\nDrag Age to the horizontal axis so you have a graph of Height against Age.\nTake a moment to discuss (or reflect) on whether that graph makes sense. It tells a story. What is it?\nSelect all the 10-year-olds. Do this by dragging a rectangle around those points. If this is unfamiliar to you, you can probably figure it out by messing around. If that doesn’t work for you, get help!\nWith the graph selected, click on the “eyeball” palette on the right to bring up a menu.\nChoose Hide Unselected Cases. Aha! Now the graph has only 10-year-olds.\n\n\n\n\nNow figure out how to compare the heights of the boys and the girls, this time of only the ten-year-olds. Be sure to put the mean on the graph so you get their average heights. See if you can get this graph:\n\n\n\n\n\nHeights of ten-year-olds, split by gender.\n\n\nSome questions to answer; if you don’t know, don’t be afraid to ask others!\n\nHow did you compare the 10-year-old girls to the boys?\nAre there other ways to compare them in a graph? Sure there are!\nWhich way works better?\nThe heights of females overlap with heights of males. What does that mean?\nWhat are the mean heights of the 10-year-old girls and boys?1 How did you find them?\nFor the whole dataset, males are taller. For 10-year-olds, females are taller. How is that possible? Does it fit with your experience?"
  },
  {
    "objectID": "01.20-teens.html#groupwork-getting-all-the-means",
    "href": "01.20-teens.html#groupwork-getting-all-the-means",
    "title": "3  800 Children and Teens, part one",
    "section": "3.4 Groupwork! Getting all the means",
    "text": "3.4 Groupwork! Getting all the means\nIf you’re in a class, and there is enough time, your instructor will break you into groups.\nEach group will be responsible for a couple of ages. For each age, do what we just did for 10-year-olds: find the mean height for the girls and the boys at that age. Then enter your data on a class table, which may be on a whiteboard, or perhaps online in a shared table such as a Google Sheet.\nThen, when all the groups are done, enter your data into a fresh CODAP document. How do you do that?\n\nBegin with a fresh CODAP document.\nMake a new table (look in the Tables tool).\nCreate the relevant columns (what columns do you need?).\nEnter the data by typing the numbers in to the table cells.\n\nIf you have the data in a Sheet, you could, instead:\n\nBegin with a fresh CODAP document.\nExport the sheet as a .csv file. (in Google, it’s in the File menu. Choose Download and then Comma-separated Values.)\nDrop the file into your CODAP document.\n\nThen plot the means as a function of age. Make sure you can tell the males from the females!\n\nPlotting two things at once\nIf the mean heights for males and females are in different columns in your table, you might first plot the females on the vertical axis and age on the horizontal. But then, if you plot the males in the normal way, the female data will disappear. How do you get them both on the same graph?\nThe trick is this: as you are dragging the males in, wait. With the mouse down, pause and look: there is a gray outline of a plus sign at the top of the axis. Drop the attribute there instead of on the axis; it will add the data to the plot instead of replacing it."
  },
  {
    "objectID": "01.20-teens.html#commentary",
    "href": "01.20-teens.html#commentary",
    "title": "3  800 Children and Teens, part one",
    "section": "3.5 Commentary",
    "text": "3.5 Commentary\nThere are three main phases to this lesson.\n\nFirst, students mess around with the data, making graphs using any attributes they like, looking for relationships. Ideally, a few of them get to show and explain their graphs, and you run a discussion as decscribed in the commentary.\nIn the second phase, we focus on a specific issue: who is taller, females or males? We learn to show means on the graph. When the obvious analysis doesn’t work well, and we are still awash in data, we get even more specific and focus on 10-year-olds. We use a data move, filtering, to do this. Very important.\nIf possible, there’s a third phase where students—probably in groups— find the mean heights for boys and girls at each age, then plot those results.\n\nThe final graph tells a clear story about height and gender and age.\n\nWho is taller? More detail…\nIn the first, exploratory phase, somebody probably made a graph with height and age or height and gender. Explain that we will now focus on this issue.\nIn a class—even online—rather than having students read the instructions and do this alone or in pairs, I do this next part as a demo, and go through the process outlined in the text. I focus on height versus gender, with questions along the way (e.g., Why is height on the vertical axis? Because it’s the response? Right. But also, because height is vertical.)\nI make the graph and show how to put the means on the graph using the ruler palette. We see, by hovering, the different values. We ask, “Are we done? Males are taller than females?”\nAnd if no one says so, we tell them that actually, there is something deeply bogus (or hinky or whatever the current term is) about our graph. It’s a fine graph, but it’s unfair. Why? What’s missing?\nWait time. Wait time.\nSomeone will say “age.” Probe for what they mean. Plop Age onto the graph to see the color gradient. Yeah, the tails are all little kids.\nEarlier someone might have made the graph of Height against Age. You can refer back to it, and make that graph for the class. If they haven’t, you can do it, plopping Gender into the middle.\nWhat do you think about this graph? Is it clear what’s going on? Partly, but it’s hard work to read this graph. It’s still confusing. Let’s find a way to have this make more sense.\nSo we do the filtering move: we select the 10-year-olds and hide the rest, then put Gender on the axis and show the mean heights of each group, reflecting on the realization that, although in general the males are taller, the story is different for particular ages.\nThat’s about as much lecture/demo as we can tolerate, so we switch to a different mode.\n\n\nGroupwork! Getting the means in more detail…\nWhat if we did that procedure—looking at only the ten-year-olds, and recording the mean heights of the males and females— but for every age?\nIt would be really cool. So we do exactly that. Every group gets an age or two to be in charge of. Their task is to find the mean heights of males and females at those ages. Groups post the means in a table on the class whiteboard or in a Google Sheet.\n\n\n\n\n\n\nGroups post mean heights in a Google Sheet.\n\n\n\n\n\n\n\nValues from the sheet, plotted in CODAP.\n\n\n\n\n\n\nThe fact that it was groups gave students a welcome break, a chance to talk, and also reinforced that key skill of filtering. When we were in quarantine, we did this in randomized Zoom breakouts. We had pre-assigned the ages to group numbers.\n\n\nIn addition, this part of the activity gave students practice with computing means; introduced how to enter your own data into CODAP; and most importantly showed them where you can go with this.\n\nI think it’s important that the filtering and typing will be slow and a little laborious, for two reasons: first, of course, when you show them how the computer can do it in the next lesson, they will see how cool and time-saving it is. Second, slowing down highlights this idea of turning the means into values in a table that you can graph. It gives it time to sink in.\n\n\nThe key moment: filtering\n\nFiltering is our first data move. When we display all the data, the Heightvs.Age graph does make sense, and it tells a story, but it’s complicated. Looking at it, we’re pretty “awash in data.”\n\nFor example, we were trying to compare heights across genders, right? But the height-age graph doesn’t have gender (left-hand illustration). What if we drop Gender onto the middle of the graph? You’ll get something like the right-hand illustration (and you should do it yourself).\n\n\n\n\n\n\nHeight by age, no gender.\n\n\n\n\n\n\n\nSame, with Gender. This is confusing. We’re awash.\n\n\n\n\n\nYou can kind of see that males are taller, at least older males are taller, but it’s not really clear because so many of the points are stacked on top of each other. You can’t tell whether purples are hiding under oranges or what.\nBut when we filter, and look at only the 10-year-olds, everything is clearer. Looking at that graph, we are no longer “awash.” It’s all manageable.\n\n\n\n\n\nHeights of ten-year-olds, split by gender (again).\n\n\n\nThe dots are not hiding each other any more.\nThere are fewer dots altogether.\nIt’s a normal kind of graph, a kind we might be more used to reading.\n\nOr, more deeply, this graph reduces the dimensionality of the problem. Before the filtering, we really needed to show three attributes at once: Height, Age, and Gender. But our graph has only two dimensions.\nBy focusing only on the 10-year-olds, we eliminate the Age attribute: it’s now irrelevant because everybody we’re looking at is 10 years old. That means we can use that horizontal, Age axis for Gender instead.\nWe’re still interested in age—after all, it has a big influence on height— but we have decided to ignore it, temporarily, strategically. We used the filtering data move to help us be less awash, to help us see something familiar.\nThat familiarity can also be an inspiration for what to do next, for how to “dig deeper” into the data."
  },
  {
    "objectID": "01.10-start.html#commentary-a.k.a-tldr",
    "href": "01.10-start.html#commentary-a.k.a-tldr",
    "title": "2  Getting Started with CODAP",
    "section": "2.1 Commentary (a.k.a TL;DR)1",
    "text": "2.1 Commentary (a.k.a TL;DR)1\nWe will include sections like this from time to time. Here is some meta-commentary, that is, commentary about this commentary:\n\nYou can learn a lot in this book even if you skip these sections. But doing things—actually interacting with the data—is more important than reading.\nTeachers: we will have suggestions for how to present this material and why we think it’s important.\nStudents: this may tell you what the teacher cares about in case you want to focus on something.\n\nEnough meta. Onward!\n\nOverview\nThe file has data about things like LifeSpan, Speed, and Diet for 27 species of mammal. Doing the activity, you make graphs of these attributes. This dataset is problematic in a number of ways, and does not smell like data science. Nevertheless, it’s a good place to start.\n\nWhat we’re calling attributes you could just as well call variables or even column headings. Sometimes I will use variable when my audience is likely to understand what I mean, and when attribute might make them pause longer than I want. Why “attributes?” See this section from a chapter in the distant future.\n\nWhen you first make a graph, the points appear randomly (but they still represent the data; try clicking on one and see what happens in the table). To organize the points, that is, to make a meaningful graph, do what we always tell students to do when making graphs by hand: label your axis, that is, drag a label—the name of an attribute—from the column heading to an axis of the graph.\n\n\nThe “Starting CODAP” Lesson\nYou could conceivably assign this for homework so that students “hit the ground running.” But it also works well simply to give them the link in class and let them do the task together.\n\nEncourage talking to others.\nWalk around and make sure everybody is eventually making graphs; at the end everyone should have bivariate graphs—different attributes on the two axes. These could be scatter plots or parallel dot plots.\nIf students are having trouble, have other students help them before you do.\n\nThe underlying point is that students do not need you to explain things. They can figure out a lot by themselves, and in collaboration with other students.\nThey should be secure in making graphs in just a few minutes. Stop them from spending too much time with this data set, and maybe lead the class in a discussion. Some possible issues:\n\nDid anybody get lots of colored points in your graph? How did you do that? (“Plop” an attribute into the middle of a graph)\nThe graph of Speed and the graph of Diet are qualitatively different. How? Why?\nWhat happens when you click on a point in a graph? (The same case gets selected in the table—and in any other graphs)\nWhat happens when you hover over a point in a graph?\nWhat does one row of the table represent? (Importantly: a species, not the common response, an animal.)\nHow do you change the size of the points in a graph? (It’s in the “paintbrush” palette. But maybe don’t tell them; Let somebody discover it and tell the class.)\nWhy are there no birds in this dataset?\n\n\nThese discussions have multiple purposes. They help students listen to each other and feel part of a community. And they also let you highlight software features that students will need to know about. Ideally, students highlight these features to each other, so it’s not always your voice.\n\n\nThis is important because students will go in different directions. If we wrote instructions for everything students might need, we would forever be teaching features and never get to exploring; furthermore, many students would be bored. So we delay learning about some features until students need them or discover them by accident. The best transmission is often viral: Zak sees that Annabelle has a graph with colored points, so he asks her how she got them. Then, in the discussion, she gets to tell the class and we can step in with the cool enhancement: here, in the paintbrush palette, is where you can change the colors. Everybody will remember."
  },
  {
    "objectID": "01.30-assignment-1.html#how-do-you-know-when-youre-done",
    "href": "01.30-assignment-1.html#how-do-you-know-when-youre-done",
    "title": "4  A First Assignment",
    "section": "4.1 How do you know when you’re done?",
    "text": "4.1 How do you know when you’re done?\n\nYou have explored the dataset about Californians in 2013.\nYou have found a simple relationship between two attributes.\nYou have made a graph that shows that relationship.\nYou have a CODAP document\n\nit includes the graph.\nit has a text box with a narrative\nthe narrative begins with a claim about the data\nthe narrative goes on to explain what you did and why the claim makes sense\nyou have shared the CODAP doc and emailed the link to your instructor\n\n\n(Sharing is in the “hamburger” menu in the upper left of the CODAP window. If you need more details, here is a set of instructions)."
  },
  {
    "objectID": "01.30-assignment-1.html#a-new-dataset",
    "href": "01.30-assignment-1.html#a-new-dataset",
    "title": "4  A First Assignment",
    "section": "4.2 A new dataset!",
    "text": "4.2 A new dataset!\nTime to look at some new data! You will look at data for 1000 Californians in 2013.\n\n… or you can at least start in the live illustration below.`\n\n\n\nHere are some questions to answer to make sure you understand the data set.\n\nThe people are from a specific age range. What is that range?\nWhy do you suppose the dataset has people from that age range?\nDescribe the oldest person in the dataset.\nDescribe the person in the dataset with the highest income."
  },
  {
    "objectID": "01.30-assignment-1.html#some-elaboration",
    "href": "01.30-assignment-1.html#some-elaboration",
    "title": "4  A First Assignment",
    "section": "4.3 Some elaboration",
    "text": "4.3 Some elaboration\nExplore. It’s essential that you mess around with the data. Here, that means make graphs. More than one. Drag different attributes to the axes to make different graphs. Try stuff in the palettes at the right edge of the graph (like the “ruler” and the “eyeball”). Select things in the graphs and see what’s selected in other graphs.\nSimple relationship. For this assignment, that means it’s a relationship between only two attributes. We have already explored more complex relationships: when we filtered the height-and-gender data to show only 10-year-olds, we were looking at three attributes: age, gender, and height. For this assignment, just look at two.\nFor example, with the previous dataset, you might have plotted Height against Weight. Or you might have made a graph of Gender and plopped BMI in the middle.\n\n\n\n\n\n\n\n\n\n\nA Narrative explains what you did. Communication is part of data science (as it is a part of everything). You need to be able to explain yourself clearly and concisely. Use complete sentences. For this assignment, you will not need very many!\nA Claim is a statement about the data that might be true or false. It doesn’t matter which, though we generally make claims we think are true. In the data with heights, with a graph of Height against Weight, you might claim: Heavier people are taller.\nA claim does not have to be dramatic. A null result is still a result! Using the gender-BMI graph, you might claim: There’s no relationship between gender and BMI.\nOf course, your claim will be about this new data set (1000 Californians), not the old one (800 children and teens)."
  },
  {
    "objectID": "01.30-assignment-1.html#commentary",
    "href": "01.30-assignment-1.html#commentary",
    "title": "4  A First Assignment",
    "section": "4.4 Commentary",
    "text": "4.4 Commentary\nData science demands communication. We create graphs and other visualizations in order to get our ideas across. They often illustrate a narrative, a story about the data and about our investigation, written out as text.\nStudents (and teachers) may not be used to writing in a mathy class. Furthermore, the style of writing will be different than what they’re used to in, say, English or History. It’s technical writing, explaining procedures and reasoning. Still, the objective is the same: to communicate clearly.\n\nMini-projects\nAnother challenge is that this kind of assignment is a balancing act: it’s a mini-project, less straightforward than a set of problems to solve. But it’s not a big project; we want students to do it reasonably quickly, and not get weighted down with deadlines, drafts, and expectations. That’s one reason why it’s limited to one text box.\nWhen students are not used to this kind of assignment, the first few are often terrible. That’s OK. They (and you, the teacher) haven’t figured out what you’re looking for. (Though that’s why we had that section, “how do you know when you’re done?”) The work will improve with practice, but only if students do at least three of these. Whence, short.\nThis initial assignment is, as advertised, designed partly to get students through the mechanics: negotiating sharing in CODAP and turning things in by emailing a URL. It’s also conceptually simple, perhaps confusingly simple, in order to make it possible to focus on those mechanics.\n\n\nDo an example?\nTo that end, as a teacher, you could demonstrate this assignment in front of the class. Here are two ideas for graphs to start with:\n\nIncome by Gender\nAge by marital status\n\nPick one, show how to make the graph and rescale it, and brainstorm with the class what you could say in two or three sentences.\nFor example:\nI claim that people who have never been married are generally younger than married people. This makes sense because once you’re married, you can never go back to being never-married. As you can see in the graph, the mean age for never-married people is in fact lower. There is still overlap, however: some unmarried people are pretty old, like, over 40.1\nHaving given an example, you could then tell students to do something like that, only different.\nBe sure to include how to share a CODAP doc.\n\n\nRescaling and Reordering\nMany students will probably do something with income. The problem is that income is highly skewed, and that means that space in the graph is scrunched down at the low end. If you put box plots or other measures on the data and compare males and females, they will not look as different as they might.\nSo to communicate a difference you want to highlight, you might rescale the axes even if it means losing some of the high-income outliers. (This gives you a chance to talk about whether that’s a good idea and how to cope with it.)\n\n\n\nGender by income\n\n\n\n\n\nSame thing, scaled\n\n\nIn any case, to rescale axes in CODAP, grab the numbers on an axis near one end and drag. In the illustration, we grabbed the top axis up at about $350,000 and dragged to the right.\nYou can do the same thing with a categorical axis, but there, dragging re-orders categories.\n\n\nDebriefing\nAfter you look over the assignments, you will want to discuss them with the class. Of course you can use this time to praise innovation and tenacity, and to point out where the work needs improvement. Here are some additional suggestions:\n\nHave some students present their work, especially if it investigated unusual attributes or used interesting new CODAP techniques.\nPoint out areas no students might have explored. For example, our two ideas above are both numerical attributes grouped according to a categorical one. What about two categoricals—what would that look like? (For example, gender and marital status.)\nPoint out CODAP features that need revealing such as rescaling axes or (xxx) fusing dots into bars.\nCompare putting one attribute on each axis as opposed to putting one on an axis and the other “plopped” into the middle as a legend. Actually put someone’s graph up, and then make another graph done the other way.\nAsk about the data in general, e.g., ask about what the income distribution looks like, and what that means in context (there are very few people making a lot of money, and a lot making not much; this is also a great time to discuss or review why median is a good alternative to mean for this data)."
  },
  {
    "objectID": "01.40-teens2.html#making-groups-section",
    "href": "01.40-teens2.html#making-groups-section",
    "title": "5  800 Children and Teens, part two",
    "section": "5.1 Making Groups",
    "text": "5.1 Making Groups\nThe work in this chapter requires a lot of screen space, so use the following link to open a new tab. The live demos we have been using are not big enough for what you are about to learn!\n\nWe’ll start by making groups, one group for each age. Concentrate on the table.\n\nDrag Age to the left in the table (don’t drag it to a graph!).\nDrop it in the blank area on the left of the table (it will turn yellow when you’re over it).\n\nxxx NB: perfect place for a short video\nNow, on the left, there is one case for each Age. There are fifteen cases in all (why?).\n\n\n\nWe have selected all of the five-year-olds. Note: this is not a live demo! Its just a picture! You will need to work in a separate tab, using the link above.\n\n\n\nClick on one of the ages at the left. What happens in the graph? In the table?\n\nAha: clicking on an age selected all of the people who are that age. Also, you can see in the right side of the table that all of the people of that age are now together in the table—and selected.\n\nWhen you dropped Age on the left, you sorted the table into 15 groups, one group for each age. You can think of it as a hierarchical table: on the left, a table of 15 ages, and on the right, within each age, a table of the people at that age.\n\n\nYou have just done Grouping, our second core data move. Start to look for how grouping your data might help you. Frequently, when a dataset is large and complicated, grouping will help you make sense of it.\nWatch out, though: making too many levels of groups can sometimes make a dataset more complicated than it needs to be!"
  },
  {
    "objectID": "01.40-teens2.html#making-summary-calculations-for-each-group",
    "href": "01.40-teens2.html#making-summary-calculations-for-each-group",
    "title": "5  800 Children and Teens, part two",
    "section": "5.2 Making Summary Calculations for Each Group",
    "text": "5.2 Making Summary Calculations for Each Group\nNow we want the mean height for each of our groups. To do that, we’ll make a new column in the “groups” table on the left, and write a formula for the column:\n\nBe sure the table is selected.\nOn the left-hand side of the table, up at the top on the right, there is a gray circle with a plus sign in it. It might be hidden by some text.\n\n\n\n\nClick the gray circle with the plus sign to create a new column.\n\n\n\nClick the gray plus thingy. A new column appears, with a name ready to be editied.\nGive it a good name such as MeanHeight. Press enter to finish editing. The column should be blank.\nLeft-click on the column (attribute) name; a menu appears. Choose Edit Formula. A formula box appears.\nEnter mean(Height). Press Apply.\n\n\n\n\nThe formula editor.\n\n\nHooray! You see the mean height for each age in the right row in that new column.\n\nDoes it bother you that the ages are not in order? Click on the colum heading for Age to get the menu, then choose Sort Ascending.\n\n\nThe mean height is a summary of each group. This action of summarizing (sometimes also called aggregating) is the third core data move. We now have three: filtering, grouping, and summarizing.\nA summary doesn’t have to be a mean. It might be a median, or a sum, or just the count (a.k.a. frequency) of the cases in the group. It could even be a percentage, like the percentage of people in the group who have a BMI under 30.\n\nCODAP has a number of functions that serve as summaries. Here are four of the most important:\n\n\n\n\n\n\n\nmean(foo)\nthe mean of foo\n\n\nmedian(foo)\nthe median of foo\n\n\nsum(foo)\nadd up the values of foo\n\n\ncount()\nhow many cases there are"
  },
  {
    "objectID": "01.40-teens2.html#finishing-our-investigation",
    "href": "01.40-teens2.html#finishing-our-investigation",
    "title": "5  800 Children and Teens, part two",
    "section": "5.3 Finishing Our Investigation",
    "text": "5.3 Finishing Our Investigation\nThe new column, MeanHeight, is first-class data like every other column. That means you can make a graph using these mean heights. So do it!\n\nMake a new graph; put Age on the horizontal axis and MeanHeight on the vertical.\n\nYou will see the pattern you might expect: people get taller as they age, up to a point.1\nWe still don’t see the gender differences. Here’s what you do. Watch what happens carefully and make sure you understand it.\n\nDrag Gender left in the table and drop it next to Age.\n\nEach item in the left table splits into males and females. So where there were the 15 ages before, now there are 30 age-gender combinations. Also, the right-hand table is now divided into 30 groups, one for each age-gender combination. The MeanHeight column now automatically shows the mean height for the cases in that group.\n\n\n\nA plot of mean height against age. Since both attributes are over to the left where groups are defined, we get only one point per group.\n\n\nThere are also now 30 dots in the graph, one for each group instead of one for each person. But which dots are for the males and which for the females?\n\nDrag Gender from the table and plop it into the middle of the graph. The points color to show which is which. You should see the graph on the right:\n\n\n\n\n\n\n\nOn the left, we plotted everybody’s Height.\n\n\n\n\n\n\n\nOn the right, we plotted MeanHeight for every age-gender group.\n\n\n\n\n\nNotice what a clean, clear story it tells. Boys’ and girls’ heights are about the same—girls a little taller in the tweens— until about age 13, at which point boys keep growing while the girls slow down. The left-hand graph has all the data, but it doesn’t tell the story as clearly as the right-hand graph.\n\nData-move reflection: When we moved Gender left, we changed the grouping, and took advantage of the summarizing that was already in place."
  },
  {
    "objectID": "01.40-teens2.html#commentary",
    "href": "01.40-teens2.html#commentary",
    "title": "5  800 Children and Teens, part two",
    "section": "5.4 Commentary",
    "text": "5.4 Commentary\nThis is the conceptual heart of this unit.\nGrouping and summarizing are at the center af a huge amount of data analysis. The process is surprsingly deep, too: a group of programmers made this capability in CODAP, and even they didn’t realize how important, how useful it turned out to be. We guarantee you that no student, no matter how brilliant, will fully understand the consequences of that drag-left-in-the-table gesture. One miracle is the way that, when you dragged Gender left to join Age, those two attributes combined to define 30 groups instead of 152, and the formula column adjusted so that it consistently calculated mean height for every group.\nAt the same time, it’s only sorting the data into groups and taking the mean. It is not rocket science. It’s not even AP Statistics. I think there are really two reasons this is hard.\nOne is that there’s so much data, we need the computer to help. That means instructing it—by dragging left3— as to what, precisely, we want it to do. That involves computational thinking, having a sense of what kinds of things the computer can do, and the kinds of instructions that work.\nBut the other reason it’s hard is more insidious: you have to want to group by age and gender, and then take the mean of each group.\nSo, how to teach it? As with the first part of our height investigation, I did much of this lesson as a demo, with students following along. I went slowly, asking and answering questions, going back and repeating sections as students got their screens to show what they saw on mine. You can let students know that if they forget any of it when they’re doing future work, they can find a step-by-step description in this book. The next assignment will give them a chance to practice these skills.\nAs a teacher, you will need to be patient but persistent as students gradually come to understand how this works. You can get additional help and perspective in the “data move” chapters on Grouping and Summarizing.\nAs to developing an intuition about what to group by and why, and how to summarize, my conjecture is that by seeing it a few times, many students will start to get it, even without being able to articulate what’s going on. I also believe, though, that we teachers can gently point out and name the data moves we are making, and remark on their consequences. (“And look! By grouping and summarizing, we now have only 30 points instead of 800. What do you think? Is this graph easier to understand?”)\nWith that prodding, we nudge them towards metacognition.\n\nSummarizing loses information\nDo not think for a moment, however, that summarizing is a panacea. It’s a tool, and we must recognize its limitations and weaknesses. Sure, we went from 800 points down to 30. What happened to those 770 missing points?\nThe sweep of the means in our graph is lovely, and elegantly shows the overall pattern of growth of girls and boys, but it ignores the individuals. It ignores variability.\nTherefore, even if there is no time for a whole lesson on this topic, be sure to ring that chime from time to time. Ask, does this graph mean that every seven-year-old is this height? Or show a graph of only sixteen-year-olds. Notice that some girls are taller than a lot of the boys, and that some boys are really short. Invite students to speculate how they could draw a graph that showed the overall story and, at the same time, showed the variability.\nThat is, rescue students from the tyranny of the center. As a society we are often held in thrall. We hear that median home prices in Westview are higher than in Dust Gulch, so we assume that evey home is Dust Gulch is a shack and every one in Westview a mansion. We read that test scores (which are always mean test scores) are higher in Blue Sky Unified, so we assume that our children will get a good education only if we move there."
  },
  {
    "objectID": "01.50-assignment-2.html#how-do-you-know-when-youre-done",
    "href": "01.50-assignment-2.html#how-do-you-know-when-youre-done",
    "title": "6  A Second Assignment",
    "section": "6.1 How do you know when you’re done?",
    "text": "6.1 How do you know when you’re done?\nYou have a Google Doc (or whatever format your instructor requires). It contains:\n\nYour name.\nA simple claim. Pick one:\n\nPeople with more education make more money than people with less education.\nPeople who get on BART at SFO generally get off at Powell Street.\n\nA graph with the first-look, obvious results.\nA description of why that simple approach might not be enough to really explain the data\nA description of what you did to further the investigation (“dig deeper”). In this description,\n\nYou have grouped the data by dragging an attribute (or attributes) to the left in the table.\nYou have calculated some summary value (e.g., a mean or a sum) for each group by making a new column with a formula.\n\nThe results of that deeper investigation (with, e.g., a new graph of the data)\nA conclusion: is the story any different now?\nPossibly, ideas for additional “dig deeper” activities with this data set, and…\nA link to a shared CODAP doc (like last time) so the instructor can see what you did.\n\nYour Google Doc is probably no more than two pages long. Be sure to set permissions so your instructor has edit access.\n\nWhere to get the data\nInstead of using a “canned” data set, for this assignment you will use a data portal. In CODAP, this appears as a window where you specify what data you want, and then press a button to get it.\n\n\n… BART is a regional transit system in the San Francisco Bay Area. SFO is the San Francisco International Airport. You can read all about the BART data portal in a separate chapter.\n\n… You can get data on education, income, race, gender, etc., and choose how many cases. Go to the options tab to choose what attributes you want.\n\nACS stands for “American Community Survey,” which is run by the Census Bureau, and collects data between the decennial Census years. The portal for the Census data looks like this:\n\n\n\nACS data portal. Notice how we have requested 1000 people and have selected EmplStatus (employment status) as an attribute.\n\n\n\n\nElaborations\nGoogle Doc. If you do not know yet how to make a Google Doc, it’s time you learned. If you need to know more, ask a friend or Google it.\nIncluding a Graph. How do you get the graph from CODAP into your file? At the moment (early 2020), you can’t just copy and paste. Here are two alternatives:\n\nClick the camera palette in the graph and…\n\nchoose Export Image (you can also Open in Draw Tool).\nchoose Local File and pick where you want to save the file.\nafter saving the file, import it (it’s in .png format) or do a copy/paste into your Doc.\n\nUse a screen-capture utility to get an image of your graph, and then paste it into your doc.\n\nPro tip: Be conscious of space. Noobs often just paste huge graphs into their documents and leave them that way. After pasting, shrink the graph so that it’s a reasonable size. What’s reasonable? When you print it out, it should still be easy to read any text. For a typical CODAP graph, that’s no bigger than about 1/6 of a page. If you know how to wrap text around a graphic, sometimes that can look very professional."
  },
  {
    "objectID": "01.50-assignment-2.html#example-gender-and-income",
    "href": "01.50-assignment-2.html#example-gender-and-income",
    "title": "6  A Second Assignment",
    "section": "6.2 Example: Gender and Income",
    "text": "6.2 Example: Gender and Income\nHere’s an example of the kind of thing we have in mind:\nSuppose we were interested in gender and income. The simple approach is to (duh) plot gender and income. The graph alone looks vaguely like the men get more, but if you put the median on the graph, and rescale it, it’s really obvious:\n\n\n\nTotalIncome split by Gender, with lines showing median income for each group.\n\n\nThis is what we probably expect: men earn more than women. If we look at median values, men earn $17,000, versus $9,200 for the women. But does that tell the whole story? How could we dig deeper? (…as required in the assignment)\nWe might ask:\n\nIs it possible that the incomes really are equal, but we’re looking at it wrong?\nCan we be more nuanced? For example, is there some other factor that affects income?\n\nLooking at the graph, see the large number of people who seem to earn zero—or close to it? That spike at zero is taller for women. Maybe that’s because more women work in the home, and are not paid.\nSo maybe the incomes for people with jobs are equal between men and women, but because more women do not get paid, their median income is lower overall. This reasoning is an example of exploring whether we’re looking at it wrong, and that there is another factor—employment—that affects income. That is, it’s not just gender.\nTo test this idea, let’s just look at people with jobs. It turns out that we can get data for an attribute (a column) called EmplStatus for “employment status.”\n\nThat means you could filter to focus your investigation on people with jobs. That way, you can explore whether men with jobs generally earn more than women with jobs.\n\nTry that in the live illustration below. The “employment status” attribute is at the far right of the table.\n\n\n\nYou should find that the men still earn more. If you hide everyone but those that are “Civilian employed,” the men earn $45,000 to the women’s $30,000. So the fact that more women do unpaid work does not completely explain the difference in income.\nNotice that this will only work if you have downloaded EmplStatus data. If you get partway through your investigation and realize that you wish you had downloaded something else, or something more, you can’t add additional columns to the cases you already have. But remember: starting over is free. Just go back and get fresh data with the attributes you want."
  },
  {
    "objectID": "01.50-assignment-2.html#dont-forget-to-drag-left",
    "href": "01.50-assignment-2.html#dont-forget-to-drag-left",
    "title": "6  A Second Assignment",
    "section": "6.3 Don’t forget to drag left!",
    "text": "6.3 Don’t forget to drag left!\nWe’ve seen what we mean by “dig deeper,” but don’t forget to read the assignment. It also expects you to do that “drag left” grouping move. In this case, that would be dragging Gender left to make groups of males and females. Then you would make a new column (maybe called medInc) in which you would calculate the median income for each gender.\nLook back at the section where we made groups by age if you don’t remember how.\nThen think about how you can use grouping to help with your “dig deeper” work.\n\nWhen you drag Gender to the axis of a graph, CODAP helps you do a grouping data move. Then, when you put the median on the graph, that’s summarizing. For this assignment, though, we want you to make those moves explicitly—grouping by dragging left, summarizing in a new column with a formula— because (a) it’s good practice and (b) it’s more flexible."
  },
  {
    "objectID": "01.50-assignment-2.html#digging-deeper-philosophy-section",
    "href": "01.50-assignment-2.html#digging-deeper-philosophy-section",
    "title": "6  A Second Assignment",
    "section": "6.4 “Digging Deeper” and skepticism",
    "text": "6.4 “Digging Deeper” and skepticism\n“Digging deeper” means adding nuance to your investigation, bringing out important trends and effects that might not be obvious at first. It’s like fleshing out a sketch and making it more detailed.\nBut it’s also about being skeptical. When you make an initial, obvious graph, and draw some conclusion or make some claim, a skeptic steps back and wonders whether that’s really correct. Is there some other explanation for what you are seeing?\nYou can think of this as a “yes, but” attitude. It’s also the role of a “devil’s advocate”—someone whose job is to tear down an argument. If the devil’s advocate succeeds, that’s good, because without that skepticism, the conclusion would have been wrong. And if the devil’s advocate fails, that’s good too, because by passing that test, the original conclusion is stronger.\nSkepticism is not always confrontational, however. You can also think of it as a “yes, and” attitude: the claim may be right as far as it goes, but there may be other important considerations.\nIn the case of gender and height, we saw that, indeed, males were generally taller than females, but only after about age 13."
  },
  {
    "objectID": "01.50-assignment-2.html#commentary",
    "href": "01.50-assignment-2.html#commentary",
    "title": "6  A Second Assignment",
    "section": "6.5 Commentary",
    "text": "6.5 Commentary\nYour job as a student is to try really hard to understand the bit about dragging left to make groups, and making new columns with calculations. If you’re feeling challenged enough already, pick the claim about education and income.\nHere’s some advice for how to explore and get comfortable:\n\nMake a graph, any graph. Try different groupings: drag one attribute left, and see what happens. Then drag the first one back and then try a different one.\nTry making a group (like for education) and adding an additional attribute (like gender) to the left. See what groups exist now.\nSelect groups on the left by clicking in that row. See what gets selected in the graph.\nWhen you make a column and give it a formula, try different formulas and see what happens.\nRemember that the name of your “formula” column is just a name. It’s not the formula. So if the column is named MeanIncome and the formula is median(income) you will get the median. (You should change the name!)\nChange what’s on the graph. Make more graphs! Try graphs with a calculated attribute on an axis.\n\nOh and:\n\nTry dragging TotalIncome to the left and see why that’s a terrible idea.\n\nIf you just mess around for a while like that, you will probably see something that addresses your claim. You may even have created something that will be perfect for your “dig deeper” section.\nTwo last key pieces of advice:\n\nWhen you make a grouping or summarizing move, stop for a moment and be sure you understand what changed. What does the organization of the table mean now? What are the groups? How did the numbers change? What does this one particular number mean?\nDon’t beat yourself up if you still feel a bit “awash.” This is surprisingly complicated, especially if you have more than one attribute on the left. It takes time. You will get it.\n\n\nQuestions and Claims\nIn many school statistics (or science) projects, you begin by posing a “research question.” In this case, the question might be, “who earns more—women or men?” It will come as no surprise that the answer is “men.” That’s what we have heard about our society today (or in 2013, when the data were taken). So “who earns more” might seem to be a silly question.\nInstead of a question, sometimes it makes more sense to make a “claim.” This is a statement about what may be true, that you are going to investigate. Our claim might be, “men earn more that women.”\nFor this assignment, we gave you a claim, so you don’t have to worry about making one up. But think about other claims you might make about your data (after all, in the next assignment you get to pick your claim), or how your claim might change as you dig deeper and add nuance to your analysis.\nIn stating your claim, you might also want to say why you think your claim is true. It’s great to have a reason for your claim, but be careful: In most cases for this unit, you won’t have data that will tell you anything about the reasons behind some effect.\nFor example, you might say that you believe people with more education will earn more because they are qualified for higher-paying jobs. Your data can tell you whether they earn more—we have data about education level and income— but we don’t have any data about the educational requirements of the jobs they have.\nIt’s fine to include reasons if you have them, but keep any claim about your data separate from the reasons just as we did above. Here’s how to do it wrong:\nI claim that people with more education are better-qualified for high-paying jobs, and as a result they earn more.\nSee how that’s different? You can’t make a graph about job qualifications. Keep it separate, even if the separator is just the word “because.”"
  },
  {
    "objectID": "01.60-project.html#whats-the-same",
    "href": "01.60-project.html#whats-the-same",
    "title": "7  A Small Project",
    "section": "7.1 What’s the same?",
    "text": "7.1 What’s the same?\nThe data: pick from\n\n(has income, education, race, etc)\n\n(2015–2018)\n\n\nWhat do I do?\n\nPredict something. Make a claim.\nDo a quick investigation. Display a table or graph. Reflect on what it says.\nDig deeper: Figure out how that can be enhanced (make it more nuanced; or investigate something related, inspired by the original; or play devil’s advocate, etc.)\nCollect data, analyze it, and make a table or graph that speaks to the issue\nReflect on what you found out. Possibly, dig even deeper and reflect again.\nMake a shared link to your CODAP document.\n\n\n\nLogistics\n\nMake it a Google Doc for simplicity.1\nGive your instructor edit permission on the document. They will get an email with the link, so that’s how you can turn it in.\nThe report should include a link to the shared CODAP document."
  },
  {
    "objectID": "01.60-project.html#whats-enhanced",
    "href": "01.60-project.html#whats-enhanced",
    "title": "7  A Small Project",
    "section": "7.2 What’s enhanced?",
    "text": "7.2 What’s enhanced?\n\nYou may pick your topic\n\nAdvice: mess around with the data before you decide. Make graphs. Think about what they mean. A question or a claim will occur to you. Note:\nIf you choose BART data, the BART Data Portal chapter has a section with suggestions for BART topics. That section also has instructions about the “secret meeting game.” Finding the secret meeting is a suitable topic for this investigation.\nIf you choose Census/ACS data, you may have already started looking at social-justice topics in a previous assignment, through income differences. That’s a fruitful direction (though not the only one).\n\n\n\nOther enhancements\n\nIf it makes sense (and it probably will), do a second “dig deeper” cycle.\nCommunication is part of data science:\n\nMake this work stand alone. Do not assume that the reader has read the assignment.\nSimilarly, give it narrative sense\n\nIt’s not a numbered list of steps! Make paragraphs! Make them flow!\nThe opening paragraph, or some text near it, should motivate the investigation. Why is this interesting—even a little bit?\n\nPay attention to space. Usually, if you just paste your graph into the document, it will be bigger than it has to be.\nMake your graphs sing!\n\nRecoding might be useful: making a new column that’s more what you want\nBriefly describe the data at an appropriate place in the report, including the sample size(s).\nWhen you present an analysis, explain what you did. If you used “data moves,” say so! Did you filter? Did you write a summary formula (like with sum( )? What was it?)\nDid you drag a column to the left to make groups? Why did you choose that column?\nDo not exceed three pages.\nVital: somewhere, perhaps in your conclusions, reflect on this experience. What interests you about this? (Or not; be honest!) How is this different from your experience with school math classes?"
  },
  {
    "objectID": "01.60-project.html#commentary",
    "href": "01.60-project.html#commentary",
    "title": "7  A Small Project",
    "section": "7.3 Commentary",
    "text": "7.3 Commentary\nIdeally, the previous assignment prepared students well for this project. Do plan on class time to help them, and remind them of various resources such as their peers and this book.\nDespite any warnings you might have given about keeping the projects small, some students will probably want to do things that go beyond what you have covered in class. Here are three that happened to us:\nRecoding: Suppose they have income and want to get rid of the number. Instead, they want categories, like “none,” “not much,” and “a lot,” based on some cut points in the numeric income attribute. This is called recoding.\nThey will want a new attribute, a new column, whose value depends on the income attribute. (They should not eliminate that original, but make a new one.) There are two basic strategies: make a formula (which in this case will have some if() statements); or do something that involves typing values into cells. If you have 1000 cases, the typing strategy had better be pretty strategic!\nThey will find some ideas about how to do this in the chapter on the calculating data move.\n\nThis data move, which we call calculating, resembles summarizing, but it creates a new value for each case in a collection rather than a value that summarizes a group. Both data moves often—but not always—use formulas.\n\nAnother classic use of recoding comes up with the Hispanic attribute. It has many categories (notHispanic, Mexican, Guatemalan, Cuban, etc…). It would be great, in some situations, to have a simpler version of that column, maybe called LatinX with only two values: Sí and No. That section explains an easy way to do that.\nAdding data from elsewhere: Sometimes you want to connect the data from two datasets together. This is another data move, called joining, which you can read about in its own chapter.\nJoining can be ridiculously complex, but our student’s idea was reasonable and straightforward: they wanted to compare education levels for people from the richest five States with education levels from the poorest five States. The ACS data did not rate States by wealth, but it’s easy to find a listing on Google.\nSo they simply asked only for data from those ten States, and then recoded the state names into a new column with values rich and poor.\nGetting your own data: Some students may find data about something else that excites them online. If they find a csv file with the data they want, they can drag and drop it into CODAP as described in this section on importing files.\nThe danger for you, the teacher, is that they get data for which grouping and summarizing don’t make sense. So you may want to have them get approval if they stray too far from the data sources we suggest.\n\nIncome inequality and Census data\nIf a student chooses Census data and your claim has a social-justice feel to it, there’s a good chance that it’s at least partly about income inequality.\nNote that there are (at least) two different ways we talk about income differences:\n\nSome groups earn more than others. You often hear about this in the news as a “gender gap” or “race gap” in income.\nWithin a group, the difference between the richest and the poorest might be especially large. You often hear about “rising income inequality,” that is, that difference now is bigger than it was in the past. Or we might compare the USA with, say, Sweden, where incomes are more equal. To do this comparison, you need a number: a measure of income inequality. One such measure is the “Gini coefficient.”\n\nTrying to come up with your own measure of income inequality is an interesting challenge.\n\n\nAssessing the Project\nErnie Chen, the teacher-of-record for the Applied Math class where we ran this in 2020, used this simple 20-point plan. Use or modify as you wish:\n\n\n\nClearly-stated claim\n1\n\n\nInitial graph\n3\n\n\nDoes the graph show or refute initial claim?\n1\n\n\nGrouping: dragging to the left\n3\n\n\nSummary column creation\n3\n\n\nEnhancement: dig deeper\n4\n\n\nCommunication/writing/readability\n4\n\n\nReflection\n1"
  },
  {
    "objectID": "02.10-census-acs.html#the-small-portal",
    "href": "02.10-census-acs.html#the-small-portal",
    "title": "8  The Census/ACS Data Portals",
    "section": "8.1 The “small” portal",
    "text": "8.1 The “small” portal"
  },
  {
    "objectID": "02.10-census-acs.html#the-big-portal",
    "href": "02.10-census-acs.html#the-big-portal",
    "title": "8  The Census/ACS Data Portals",
    "section": "8.2 The “big” portal",
    "text": "8.2 The “big” portal\n\n\nYou can also access this portal from any CODAP document by choosing Microdata Portal in the Plugins menu."
  },
  {
    "objectID": "02.20-BART.html#the-basics",
    "href": "02.20-BART.html#the-basics",
    "title": "9  The BART Data Portal",
    "section": "9.1 The basics",
    "text": "9.1 The basics\nThe data portal—the thing you interact with—lets you specify the stations and the day and hour. You will press a button to get the data, and it will be sent directly into CODAP so you can analyze it\nTry this in the live illustration below:\n\nLook at the panel; notice what day we’re looking at and what stations (Orinda and Embarcadero) we have specified by default.\nPress the get data button. A table will appear with data.\nMake a graph. Put when on the horizontal axis and riders on the vertical.\nNow: Orinda is a suburb. Embarcadero is in downtown San Francisco. What’s going on in the graph?\n\n\n\n\n\nHere are more things to do. This will further orient you to the data and its possibilities:\n\nAs you probably figured out, the graph shows people going to work. Let’s see them coming home. Press the swap button to exchange Orinda and Embarcadero. You should see this:\n\n\n\n\nAfter swapping Orinda and Embarcadero.\n\n\n\nPress get data. The return trips appear in the table and in the graph.\nIt would be great to color-code the points. Plop startAt into the middle of the graph.\nLet’s get data from other days! Change the date to the next day (probably April 19, 2018) and change how much data? to 7 days.\nPress get data. A whole week of data appears.\nNow drop day into the middle of the graph.\nInterpret what you see!\n\nMini-commentary: Notice that your understanding of how the world works informs how you interpret this dataset and the graphs you make. Knowing about weekends, for example, explains why there’s such a big dropoff in ridership."
  },
  {
    "objectID": "02.20-BART.html#more",
    "href": "02.20-BART.html#more",
    "title": "9  The BART Data Portal",
    "section": "9.2 More!",
    "text": "9.2 More!\n\nLimits to downloads\nThe controls for getting data are fairly self-explanatory, but a few things bear noting:\n\nThere is a limit to how much data you are allowed to get in one request. As a consequence, you may have to be strategic in the data you get. For example, a request for data from Embarcadero to any station gets (of course) fifty times as much data as a request to a single station. So while you can get a whole day of that, you can’t get a week or a month in a single request. That’s fine; being prudent about data is part of data science!\nTo use the between any two stations option, you have to restrict the hours to get only a single hour. (Otherwise it’s too much data.)\nIf you make multiple requests, and they overlap, you will wind up with two copies of the common data.\n\n\n\nThe thing about time\nEvery case has several different attributes for time:\n\nwhen is a “date-time”, where the time is the beginning of the hour the data are from.\nday is the day of the week, which is categorical but will appear ordered correctly.\nhour is an integer, the hour, in a 24-hour system.\ndate is the day (as a date) without the time. It’s categorical.\n\nThese nearly-synonymous attributes help you make different kinds of comparisons. For example, if you want to explore weekly commute patterns, you might make a graph like this, using when for the time:\n\n\n\nOne week of data from Orinda to Embarcadero.\n\n\nBut if you want to overlay the days on top of one another, you should use hour. In this graph, we have selected Friday; you can see that not only do fewer people commute, they do so later:\n\n\n\nThe same week of data from Orinda to Embarcadero, but overlaid with Friday selected.\n\n\nWe made these attributes as a convenience for you as a data-science learner. When you bring in your own data, it will probably be in one format, ansod you will have to do all these transformations yourself. If you need to know more, there’s a whole chapter on the issue of dates and times."
  },
  {
    "objectID": "02.20-BART.html#bart-suggestions",
    "href": "02.20-BART.html#bart-suggestions",
    "title": "9  The BART Data Portal",
    "section": "9.3 Suggestions for investigations",
    "text": "9.3 Suggestions for investigations\nHere are some things you can investigate with the BART data:\n\nWhen people take BART from SFO, where do they tend to go?\nMr Erickson thinks that people tend to leave work early on Friday. True? Myth? Does it depend?\nFrom which station do the most people come downtown during the morning commute?\nWithout looking at a schedule, find a day game in 2015 when the Giants were playing at home at AT&T Park. (What station(s) are relevant? Embarcadero and Montgomery.) All you’re trying to do is find a date with a day game. Then check to see if you’re right at this site; get the box score, it has the starting time.\nA’s fans: do the same, check at this site. (Coliseum station.)\nTry to estimate the total number of people who went to Civic Center Station for Pride, June 28, 2015. Find the date for Pride 2018 without looking it up and do the same.\nWhat other events or phenomena can you investigate? Think of one, study it! (Could be a one-time event, a repeating event, or something that happens every day, or…)\nPlay the secret meeting game, described below."
  },
  {
    "objectID": "02.20-BART.html#the-secret-meeting-game",
    "href": "02.20-BART.html#the-secret-meeting-game",
    "title": "9  The BART Data Portal",
    "section": "9.4 The secret meeting game",
    "text": "9.4 The secret meeting game\nA secret meeting is being held weekly near a BART station. Find it!\n\nYou know the meeting is on Tuesdays, you know it’s at Hayward, and you know that 160 people attend. But you don’t know the time. Collect data and make a display (or displays) to support a convincing argument that you know the time. Convince your neighbor!\nLet’s play that again. Go to the options panel. If necessary, abort the game. Then set Thursday, 160, and 12 noon, but choose surprise me for the location. Then solve the problem. How did you do it? What data did you collect, and why? (Remember, the possibilities for location are Orinda, Hayward, San Bruno, and Pleasant Hill.)\nAgain, but this time,\n\nReduce the size of the meeting\nSet surprise me for two of the remaining parameters"
  },
  {
    "objectID": "02.20-BART.html#commentary",
    "href": "02.20-BART.html#commentary",
    "title": "9  The BART Data Portal",
    "section": "9.5 Commentary",
    "text": "9.5 Commentary\nBesides giving you a chance to interact with a truly huge dataset (albeit only a little at a time), the BART data is a great environment for learning how to think about data in order to get what you want.\nSuppose you decide you’re going to investigate the Giants home game against the LA Dodgers on Tuesday, April 21, 2015. You want to know how many people took BART to the game. How can you figure that out?\nYou’ll be looking at people going to Embarcadero and Montgomery stations. You expect to see a bump in the ridership. But at what times? You should probably look at the data to see when the bump is. But what data? To Embarcadero and Montgomery, but from where? Everywhere, right?\nBut if you do that, you’ll have to add up the data from different stations. Sounds like dragging left and making a new column. But what do you drag left? The station? No.\nTo decide, one strategy is to ask yourself, “What graph do I want to make? What will it look like?”\nThe one in my head has a bump, with riders on the vertical axis and hour on the horizontal. That suggests that we want hour on the left side of the table. Then when we add, we’ll get the sum of the riders—for each hour. Yes. That sounds right.\n\n\n\nSum of riders (called total) from anywhere to Embarcadero for Tuesday afternoon, April 21, 2015.\n\n\nYou can see that there are a slew of similar problems to solve. None of them are mathematically sophisticated, but they can all be challenging and confusing. Here are three:\n\nHow do you add the people from Embarcadero and Montgomery together?\nHow do you account for the people who would have been riding BART anyway whether there was a game or not?\nHow do you know if the bump is for a game or for something else?\n\n\nAs you think about these, notice that the solutions use data moves:\n\nfiltering to get the right data,\ngrouping to set up appropriate comparisons, and\nsummarizing (or aggregating) and calculating to add up the riders and subtract out the “background” traffic.\n\n\nThe very idea of the “background” is an interesting topic. It smells very sciency, is hardly ever talked about in ordinary classwork, and requires only common sense to understand and cope with. To tell how many people went to the game on this Tuesday, April 21, 2015, we need to compare the pattern of ridership to some day when there was no game. Does it matter which day we compare? It might. Maybe it would be best to choose a different Tuesday. But we can’t be sure; maybe we should compare a few different non-game days. And so forth.\n\n\n\nSum of riders from anywhere to Embarcadero for three Tuesdays in April..\n\n\nThis is also closely related to the idea of controlling variables, which we talked about in the commentary about gender differences in income (xxx)."
  },
  {
    "objectID": "02.30-noaa-weather.html#the-little-one-by-tim",
    "href": "02.30-noaa-weather.html#the-little-one-by-tim",
    "title": "10  The NOAA Weather Portals",
    "section": "10.1 The little one (by Tim)",
    "text": "10.1 The little one (by Tim)\n\nxxx replace this graphic with a newer one\nThis plugin will let you choose…\n\nfrom a small selection of diverse stations, with a leaning towards California (because it was created for a workshop in the LA area);\nwhether the data are daily or monthly; and\nfrom among a small selection of data, e.g., precipitation or average temperature.\n\nThe software lets you choose dates back to roughly 1900.\n\n\n\nThe ‘little’ NOAA data portal\n\n\nThen there’s this thing about a “spreader.”\nThis is because we have been exploring a concept in data organization called “tidy data.” I will not go into it here; suffice to say:\n\nIf you get only one type of data, you will not have a problem. Just put value on the vertical axis.\nIf you get two temperatures (e.g., tMin and tMax) do the same, and plop what into the middle of the graph.\nIf you mix the data, like you get precipitation and temperature, your graphs will be unruly unless you use the spreader. Experiment, write me an email, or just use the bigger, more modern NOAA data portal!"
  },
  {
    "objectID": "02.30-noaa-weather.html#the-big-one-by-concord",
    "href": "02.30-noaa-weather.html#the-big-one-by-concord",
    "title": "10  The NOAA Weather Portals",
    "section": "10.2 The big one (by Concord)",
    "text": "10.2 The big one (by Concord)\nWhen the nice people at Concord Consortium saw my NOAA Portal they realized they needed something like it for a project they were doing. So they took my code and expanded upon it.\n\n\nYou can also access this portal from any CODAP document by choosing NOAA Weather in the Plugins menu.\n\nThere are differences:\n\nVery cool: It has a map you can use to choose from among a zillion stations.\nYou can only get one station at a time.\nIf you want more than one measurement, they appear in different columns. This is usually what you want. Also, this means you do not have to worry about tidy datasets or spreading.\nThe data go back, apparently, only to 1970.\nI have had trouble entering dates sometimes.\n\n\n\n\n\nThe ‘big’ NOAA data portal. Mt Washington in New Hampshire is selected"
  },
  {
    "objectID": "02.40-nhanes-portal.html",
    "href": "02.40-nhanes-portal.html",
    "title": "11  The NHANES Data Portal",
    "section": "",
    "text": "This brief chapter describes a data portal that gives you access to several thousand cases from the 2003 National Health And Nutrition Examination Survey (NHANES). This is the dataset where we got the 800 children and teens we used in two of the lessons in this book.\n\n\n\n\nThe attributes panel in the NHANES portal.\n\n\nA box at the top of the portal lets you choose your sample size, and a button will get your data. When the data downloads, a CODAP table appears with the data in it.\nThe portal has three panels:\n\nsummary: This describes what you have chosen to download. It includes a list of the attributes you have chosen.\nattributes: (Shown in the illustration.) Here you can choose which attributes you want. You can pick them from several different broad categories. This is similar to the Census/ACS portal design you have already used.\ncases: This lets you limit the range of ages of the people you will get. You do not have to fill in both boxes."
  },
  {
    "objectID": "03.10-Filtering.html#selection-as-filtering",
    "href": "03.10-Filtering.html#selection-as-filtering",
    "title": "12  Filtering and Selection",
    "section": "12.1 Selection as Filtering",
    "text": "12.1 Selection as Filtering\nYou might not think of selection as filtering, but in CODAP it can serve that purpose. That’s because of CODAP’s synchronous selection. Whatever you select in one view of the data gets selected in all views. So if you select points in one graph, they will be selected in all graphs and in the table.\nThe live example below shows some US Census data from 2010. We see a table, a graph of Age, and a graph of Marital_status.\n\nClick on the “Widowed” bar to select all the widowed people. Notice what happens in the Age graph.\nClick on “Never married.” See how the selection changes.\nNotice that one of the never-married people is in the oldest clump. Who is that? Select that single point in the graph (the bottom one at age 94; you may need to click elsewhere first to de-select all those widows). See how he is selected in the table? Looking in the table, we can see that he lives in New Jersey.\n\n\n\n\nThe point is that when you select points in a graph or the table, the visual selection—the highlighting on the screen—lets you focus on the selected cases. Although this is not what we usually think of as filtering, it’s a dynamic way to accomplish the same thing—and even a little more, because you can naturally compare the selected points to the plain ones.\n\nNote: it was easy to select the whole group because we “fused” the dots into bars. It would still work if they were dots, but instead of clicking on the bar to select, you would have had to drag a rectangle around the points you wanted. Want to learn more about making bars? Here’s a link. (xxx)\n\nIn this process we often want to select whole groups, as you did when you clicked on the bar for widows. It’s worth taking a moment to remember how to select groups in the table.\nTo do that effectively, you have to rearrange the table. The next example has the same data, but no graph of marital status.\nWe can still select the widows. Try this:\n\nIn the table, drag the Marital_status column head leftward in the table, and drop it in the clear patch at the left edge of the table. The “drop zone” will turn yellow when you’re over the right place\nYou have reorganized the table, making it hierarchical. Now, when you click on the word “Widowed” in the left column, you will select all of the widows and widowers, in the table and in the Age graph.\n\n\n\n\n\nThat “drag left” gesture is an example of grouping, a different data move we discuss in more detail in the next chapter. If you make groups in this way, it’s easy to select the group in order to highlight it in different views of the data."
  },
  {
    "objectID": "03.10-Filtering.html#hiding",
    "href": "03.10-Filtering.html#hiding",
    "title": "12  Filtering and Selection",
    "section": "12.2 Filtering by Hiding",
    "text": "12.2 Filtering by Hiding\nCODAP lets you hide data in graphs. Often, a filtering move involves either\n\nSelecting the points you want to hide, and hiding them, or\nselecting the points you want to see and hiding the rest.\n\nBoth of those involve selecting, which is why we mentioned selection first.\nIn the next example, we’re comparing income by gender. We see that the median income for men is higher than that for women. In the graph, you can see that there are lots of people that make almost no money. Maybe the low median for women is because there are so many women who do not work outside the home, and have no “official” income.\nWe can filter using a different attribute; in our table, we have Employment_status, which has values that will let us look at only those people who are employed.\n\nIn the table, drag Employment_status to the left and drop it in the yellow zone at the left edge of the table.\nIn the Employment_status column, select the employed people by clicking on “Employed”.\nClick on the title bar of the income graph to select it. The “palettes” appear on the right side of the graph.\nGo to the “eyeball” palette on the right and click it. A menu appears.\nChoose Hide Unselected Cases.\n\nWhen you do that, only the employed people remain in that graph. The median incomes for men and women both increase—but men still generally earn more.\n\n\n\nNow suppose we decide that we want to exclude people under about 25 because they haven’t had a chance to get enough education to earn more.\n\nIn the Age graph, select all the people up to about age 25. Do this by dragging a rectangle around the cases you want to select. If you’ve never done this before, it may take a couple of tries to develop a good strategy. One is to start at the upper right of the rectangle you want, then drag down and to the left.\nChange back to the income graph by clicking in its title bar.\nIn the eyeball palette, choose Hide Selected Cases.\n\nAgain, the incomes readjust.\nYou can always get cases back by going to the eyeball and choosing Show All Cases.\n\n\n\n\n\n\nWarning\n\n\n\nWhen we hid the unemployed people in that first (income) graph, they only vanished in that graph. They were still there in the other graph. That’s why we switched back to the income graph when we had selected the younger people in the age graph; if we had done the “hide” in the age graph, they would have disappeared there and remained in the income graph."
  },
  {
    "objectID": "03.10-Filtering.html#setting-aside",
    "href": "03.10-Filtering.html#setting-aside",
    "title": "12  Filtering and Selection",
    "section": "12.3 Filtering by Setting Aside",
    "text": "12.3 Filtering by Setting Aside\n“Setting aside” in CODAP is very much like hiding, but it’s conceptually different in an important way:\n\nWhen you hide cases, you’re telling CODAP not to show them in one particular graph. When you set aside cases, CODAP will ignore them in all graphs and calculations.\n\nThat means you could use hiding to, for example, make one graph of just the employed people and then another, separate graph of just the unemployed.\nBut if you have data you want to filter out for your entire analysis—for example, if you’re studying employed people over 25, and you don’t want to keep having to hide all the unemployed and the graduate students, you should set aside those cases and proceed.\nYou can always get set aside cases back.\nSet cases aside by\n\nselecting them, then\nmaking sure the table is selected (e.g., by clicking in its title bar), and finally\ngoing to the eyeball palette of the table and choosing an appropriate option.\n\n\nRemember: to hide cases, use the eyeball palette of the graph. To set them aside, do it in the table."
  },
  {
    "objectID": "03.10-Filtering.html#initial-choice-section",
    "href": "03.10-Filtering.html#initial-choice-section",
    "title": "12  Filtering and Selection",
    "section": "12.4 Importing Data: Initial Choice as Filtering",
    "text": "12.4 Importing Data: Initial Choice as Filtering\nWhen you import data from a big data site, or from a CODAP plugin data portal such as the one for BART data or US Census data, you often have to specify which data you want. This is essential because CODAP, at least, cannot handle the entire dataset. It’s too big. For example, as of this writing, the BART data has 40 million records.\nA data portal typically has controls, at the outset, where you choose what data you want. You make your choices, and then press some button labeled, essentially, get my data. The site creates a data request and then, if the request is OK, your data appears. If it’s a CODAP plugin, the data shows up in a table. If it’s on the web, it may create a .csv file or the equivalent, that you can import into your favorite data analysis program. If that’s CODAP, simply drag the file into the window.\nAnyway, about those initial choices:\n\nYou might get to control what attributes (a.k.a. variables, columns in your tabe) will appear in the data you get.\nYou might get to control what cases (rows in the table) will be part of the dataset.\n\nIt’s good to keep that distinction in mind; and here we’re talking about the second of those bullets: controlling which cases you get.\nFor example, with the BART data plugin, you can specify what day you want data from, and for which stations. That way you get April 18, 2018 from Orinda to Embarcadero (19 cases) instead of all 40 million. Much more manageable. Less awash.\nMaking that choice up front is kind of like doing a massive “set aside” operation. If you could start with all 40 million cases, you might first select the day and choose Set Aside Unselected Cases; then select the starting station, Orinda, and set aside the others; and finally select the destination, Embarcadero.\nBut the choosing the data first makes the subsequent analysis feel different. When you use the portal, you’re proactively deciding what you want instead of eliminating what you don’t want. It’s more like adding clay to a model than sculpting in stone. As a consequence, data analysis using a data portal is more accretion than exclusion. You get a little data and see what it looks like; you wonder whether April 19 is the same as April 18, so you get that too, adding to the data you’re working with; then you wonder whether a weekend day is different, and add the 21st (which was a Saturday). Then you decide you want a year of Wednesdays, but only between 5 and 11 AM…and so forth.\nThe plus side of this, from a pedagogical point of view (whether you are teaching yourself or others) is that the learner has to think about the process a little more, and do some planning. They get to apply their knowledge of the world as part of imagining what the data might look like. Furthermore, they get to start with something familiar, wading into the ocean of data at the beach instead of falling in miles from shore.\nThe downside is that if you only look at the data you know you want, you might miss an important and relevant phenomenon that happened outside the data you requested. More insidiously, some students might not get the data they need in order to control for other variables. More on that elsewhere xxx."
  },
  {
    "objectID": "03.10-Filtering.html#commentary",
    "href": "03.10-Filtering.html#commentary",
    "title": "12  Filtering and Selection",
    "section": "12.5 Commentary",
    "text": "12.5 Commentary\nA huge dataset is totally characteristic of data science. And the hugeness of a dataset certainly contributes to feeling awash in data.\nFiltering, by its very nature, reduces the number of cases you’re looking at, so it can be an antidote to awash-ness if you filter with a purpose.\nxxx more?"
  },
  {
    "objectID": "03.20-Grouping.html#using-graph-axes",
    "href": "03.20-Grouping.html#using-graph-axes",
    "title": "13  Grouping",
    "section": "13.1 Using graph axes",
    "text": "13.1 Using graph axes\nThe simplest way to group cases in CODAP is to use a graph. When you drag a categorical attribute to an axis, CODAP separates the data into groups—one for each category— and plots them separately in parallel graphs.\nFor example: if there is a continuous attribute on the other axis, you get parallel dot plots; this lets you compare the distribution across the different groups.\n\n\n\nParallel dot plot example: income by education\n\n\nIn the illustration, Education is the categorical attribute; it has six categories (graduate, etc.), so we have split the data set into six groups. These appear as six parallel dot plots of TotalIncome, the numeric, continuous attribute."
  },
  {
    "objectID": "03.20-Grouping.html#using-legends",
    "href": "03.20-Grouping.html#using-legends",
    "title": "13  Grouping",
    "section": "13.2 Using legends",
    "text": "13.2 Using legends\nDragging an attribute to the middle of a graph (a.k.a. plopping) colors the points according to the value of that “legend” attribute. A legend appears on the graph so you can decode the colors. If the legend attribute is categorical, the colors are distinct. If it’s numeric, you get a gradient of colors that separate the data into quintiles, that is, CODAP groups the data into five categories by value. If you click on the gradient or on the little paint chips in the legend, you select all of the points in that group.\nThis is a subtler form of grouping; the cases don’t move into separate zones in the graph, but the colors and/or selection help you see the properties of that group alone (which is filtering, in a way, right?) or compare that group to others.\n\n\n\n\n\n\nRoller coaster data about height, grouped by coaster type. We can see that, generally, steel coasters are taller.\n\n\n\n\n\n\n\nSame data, grouped by speed. Because speed is numeric, we get a gradient—actually, five bins—that show us that taller coasters are faster.\n\n\n\n\n\nBy the way: the gray dots in the speed graph are cases for which the value of speed is missing.\n\n\n\n\n\n\nTip\n\n\n\nDon’t forget that if you have a legend, you can select cases by clicking boxes in the legend itself."
  },
  {
    "objectID": "03.20-Grouping.html#binning-as-grouping",
    "href": "03.20-Grouping.html#binning-as-grouping",
    "title": "13  Grouping",
    "section": "13.3 Binning",
    "text": "13.3 Binning\nIf you have a graph with a numeric axis, you can group the data into bins. This is easier to do than to explain.\nThe live illustration below has a dataset with 1000 people from the 2017 American Community Survey. We have already made a graph of Age. Do the following:\n\nSelect the graph so its palettes appear on the right.\nThe fourth one down shows a little graph. Click it. The Configuration inspector appears.\nClick the checkbox for Group into Bins. The points group into bins, probably 20-year spans.\nGet the configuration inspector again; more controls have appeared. Change the bin width to 10. Press tab to make the graph change.\nClick the checkbox for Fuse Dots into Bars. Oooooh! Sometimes you will want the dots fused into bars; other times you won’t.\n\nThink: what can you tell about the people from this graph alone? Is it easier or harder than with the plain dot plot, ungrouped, un-fused?\n\n\n\nLet’s see how else we might use such a graph.\n\nPlop Marital_status into the middle of the graph.\n\nAha. You can see, in the bars, the distribution of marital status in each age range.\nExplore!\n\nWhat do you see when you plop different attributes onto the graph?\nUn-check Fuse Dots into Bars and/or Group into Bins. What helps or obscures any relationships you see with the plopped attribute?\n\nBin labels You probably saw that the labels for the bins are something like [10–20) and [20–30). Notice the difference between the brackets [ and parentheses ). That’s a common notation that means, “from 10 up to but not including 20,” then “from 20 up to but not including 30,” and so forth. As of this writing, there is no way to use those labels anywhere else, for example, as category labels."
  },
  {
    "objectID": "03.20-Grouping.html#grouping-in-the-table",
    "href": "03.20-Grouping.html#grouping-in-the-table",
    "title": "13  Grouping",
    "section": "13.4 Grouping in the table",
    "text": "13.4 Grouping in the table\nYou learned this already in the chapter where you calculated the mean heights of males and females at each age. You split the dataset into groups by dragging the “grouping” attribute (Age, and then Gender) leftwards in the table. You get one group for every value in the grouping attribute. So if you drag Gender, you get a group for males and a group for females. If you drag Age, you get one group for each age. And if you drag them both, you get a group for each age-gender combination.\nThe groups collect together in the right-hand table, and are connected by the curvy lines to the values on the left. The calculations (e.g., mean(Height)) at the “upper level” apply separately to each group. If you don’t remember that, or you missed it, go back and do it now.\n\n\n\nTable with grouping by date.\n\n\nIn the illustration above, we have selected the Saturday, and so all four data points “within” Saturday get selected.\n\nWhich attribute should you drag left?\nDragging left to group is not immediately intuitive. We all make many mistakes when we’re learning what to drag. You will learn a lot by trial and error; fortunately, errors are easy to fix. Just use Undo—or simply drag the attribute back over to the right.\nHere are some things to think about:\n\nThe attribute to drag is almost always categorical. An exception we used with our height investigation was Age. It’s numeric, but it acts categorical. We made a group out of each age.\nThe values in that attribute’s column are the labels of the groups. Suppose you want to study education. Look for the column with names for the groups you want. When you see stuff like high school and some college, that might be the one.\nIf you see a lot of duplicate values in a column— like lots of 12-year-olds, or lots of people with bachelor’s degrees— that column might be a candidate for grouping.\nSuppose you know what groups you want, but you don’t see the right attribute. For example, you want to group people by decade— you know, people in their twenties, thirties, forties, etc.— but you only have whole-number ages. Then you need to check out the “Calculating” chapter; you need a new column with the decade, which you can make either before or after you group."
  },
  {
    "objectID": "03.20-Grouping.html#commentary",
    "href": "03.20-Grouping.html#commentary",
    "title": "13  Grouping",
    "section": "13.5 Commentary",
    "text": "13.5 Commentary\nWe’ve expended so much effort describing how to group cases in CODAP, let’s step back a moment and remember what grouping is for. Grouping has at least two purposes:\n\nGrouping makes it easy to tell which cases are in which groups. So we often group before we filter, in order to make it easy to select the entire group—and filter it in or out.\nGrouping makes it possible to perform summary calculations, for example, means, in order to compare groups to one another.\n\nIn addition, we sometimes just group and don’t explcitly do anything else. We compare the groups, often visually in a graph, making our own private, implicit summaries.\nVocabulary Earlier in our thinking, we talked more directly about how the CODAP drag-left-to-group move changes the data. We described it as reorganizing the data set or making hierarchy. We could also have called it classifying. Those terms are okay, but grouping is much more accessible.\nAre there examples of reorganizing that are not grouping? Sure: sorting, for example, alphabetically. And more esoteric reorganizations include transposing a dataset—which is beyond the scope of a gentle introduction to data science.\nGrouping and filtering Maybe this is too esoteric, but consider: grouping is really a way to do a lot of filtering at the same time instead of sequentially. For example, grouping by marital status is kind of like filtering for “never married,” then for “married”, etc., all the way to “widowed.”\nOne thing that makes grouping different is that, the way we are using the term, grouping is (mutually) exclusive: every case is in at most only one group in a set. (Cases with missing values might not be in any group.)"
  },
  {
    "objectID": "03.30-Calculating.html#a-simple-example-unit-conversion",
    "href": "03.30-Calculating.html#a-simple-example-unit-conversion",
    "title": "14  Calculating and Recoding",
    "section": "14.1 A simple example: unit conversion",
    "text": "14.1 A simple example: unit conversion\nYou have a bunch of data about the heights of children and teens. The problem is that the Weight attribute is measured in kilograms. For some reason, your audience will not understand this new-fangled metric system, so you decide to make your graphs in pounds instead.\nThe basic plan is simple: create a new column, give it a name (e.g., WtLb), and then give the new column a formula (Weight * 2.2). Here’s the step-by-step; try it out in the live illustration below.\n\nBe sure the table is selected.\nClick the gray circle with the plus sign near its upper right. A new column appears for the new attribute. Its name is selected, ready for entering.\nType the new name: WtLb. Then press enter to complete the edit.\nClick on the new name. A menu appears. Choose Edit Formula….\nIn the formula box, enter Weight * 2.2 and press Apply. Notice that the formula you enter is only the part to the right of the equals sign.\n\n\n\n\nNow you can change the graph to WtLb by dragging the new attribute name to replace the old one.\nSmall extra: Notice that WtLb has no units in parentheses. Let’s fix that.\n\nClick WtLb just like we did to get the formula editor.\nThis time, choose Edit Attribute Properties… Notice what we have here. Cool stuff.\nEnter pounds in the unit box.\nPress Apply. Shazam! The units appear!\n\n\nFathom aficionados from the ancient years will justly lament the loss of units arithmetic. Good old Fathom actually knew the conversions."
  },
  {
    "objectID": "03.30-Calculating.html#recode-numeric-to-categorical-section",
    "href": "03.30-Calculating.html#recode-numeric-to-categorical-section",
    "title": "14  Calculating and Recoding",
    "section": "14.2 Making Numeric Attributes Categorical",
    "text": "14.2 Making Numeric Attributes Categorical\nYou can use formulas to convert numerical attributes to categorical. Suppose you’re investigating income, but graphs with income on them quickly become complicated: they stretch over a wide range and they’re badly skewed.\nSo for exploratory purposes, you’d like to divide the population into two groups:\n\nThose making $40,000 or more, which you will label \"$$\", and\neverybody else, which you will label \"$\".\n\nHere’s how to do it; use the example live document below.\n\nWith the table selected, click the gray circle with the plus sign to make a new attribute.\nGive it a good name (e.g., money) and press enter to complete the edit.\nClick the new name to get the menu, and choose Edit formula….\nIn the formula editor, enter if(TotalIncome >= 40000, \"$$\", \"$\"). Press Apply. The column should fill with values. Don’t leave out the quote marks!\nMake a graph that uses your attribute. For example, put EmplStatus on one axis, and then plop money into the middle. Aha! Employed people are more likely to be in the $$ group!\n\n\n\n\nNotice how that if() function works. It’s like the one in most spreadsheets, and takes three arguments. Consider this formula for an attribute called kindOfPet:\nif(sound = \"woof\", \"dog\", \"cat\")\n\nThe first is a “Boolean” expression, either true or false.\nThe second is what you get if the expression is true (the pet is a dog).\nThe third is what you get if the expression is false (the pet is a cat).\n\nUse money as you would any attribute. You can use it to make graphs, and you can use it in other calculations.\n\n\n\n\n\n\nGotcha! (still true, May 2023)\n\n\n\nHowever: If you drag money to the left to make groups—a perfectly reasonable thing to do—CODAP will not do what you want. Why not? CODAP tries to apply that formula to the entire group (as it would median(TotalIncome)) and it can’t because there are many different values of TotalIncome in the group, so it can’t tell what value to assign to money.\nTherefore, before you drag, click on the column header, and in the menu that appears, choose Delete Formula (Keeping Values). Now the money column—filled with $$ and $—is just as if you had typed in every value separately.\nJust be aware: now, if you change someone’s TotalIncome, their money value will not change, because you deleted the formula. Similarly, if you import more data, there is no formula to fill in new values for money."
  },
  {
    "objectID": "03.30-Calculating.html#recoding-categorical-section",
    "href": "03.30-Calculating.html#recoding-categorical-section",
    "title": "14  Calculating and Recoding",
    "section": "14.3 Reducing the number of categories in a categorical attribute",
    "text": "14.3 Reducing the number of categories in a categorical attribute\nSometimes, a categorical attribute has many categories, and you want to group them together. With States, for example, you might want to group them by “region,” so you would put California, Oregon, and Washington into a region called “Pacific,” and so forth for the rest of the states.\nLet’s practice this skill with something simpler, using the data we just used above. We are going to reduce the six categories in Education to two categories: collegiate, which means they went to college at all, and no college, which means they never did.\nWe could make a formula, but it would be ugly and complicated. Instead, we will simply type the new values in.\nWon’t that take a long time? We have 1000 cases! No, because we’ll type them only once. Do this using the live illustration below:\n\nDrag Education left to make (six) groups by education level. Select the table.\nClick the “left” gray circle (the “add attribute” circle), just above and to the right of Education. This makes a new column.\nName it college.\nIn that column, enter the appropriate values for each category. That is, double-click the cell and actually type collegiate or no college. You can use Copy and Paste if you like.\nYou have now re-coded Education so that the “improved” values are in college. You no longer need Education to make your groups, so drag it back to the right. Now you have only two groups (the college groups) on the left.\n\n\n\n\nYou can use this new attribute as you would any other:\n\nMake a graph with Gender on the horizontal axis.\nPlop college into the middle of the graph.\nIn the “configuration” palette (below the ruler palette on the graph), choose Fuse Dots Into Bars.\nHover over the bars; what do you learn from the percentages that appear?\n\nAlso,\n\nMake a graph of TotalIncome with the new attribute, college, on the other axis. A useful, telling graph! Stay in school, kids…"
  },
  {
    "objectID": "03.30-Calculating.html#by-the-way-always-make-a-new-column",
    "href": "03.30-Calculating.html#by-the-way-always-make-a-new-column",
    "title": "14  Calculating and Recoding",
    "section": "14.4 By the way: Always make a new column!",
    "text": "14.4 By the way: Always make a new column!\nThis is a practice you should ingrain into your data-analysis muscle memory: Avoid destroying data.\nSeems obvious, but the techniques we mentioned here sometimes lead students into that trap.\nSo: When you’re recoding Education into anyCollege, you might be tempted to (do not do this!):\n\nDrag Education left to make six groups be education level.\nEdit graduate, bachelor's, and some college to be collegiate.\nEdit the rest to be no college.\n\nDo you see the problem? You have overwritten the original data. Suppose, later, you decide you want your collegiate group to be only people who have finished a bachelor’s degree? You no longer have anyone with some college to move from one category to the other.\nTherefore, leave the original data as it is, and put any new data in a new column. Then if you change your mind, you don’t have to start over."
  },
  {
    "objectID": "03-datamove-part.html#and-two-more",
    "href": "03-datamove-part.html#and-two-more",
    "title": "Data Moves",
    "section": "And two more",
    "text": "And two more\nAs I suggested in the introduction, data moves are not all there is to data science. You’ll find two more chapters bundled with the data moves listed above, in their own chapters:\n\nPreparing and cleaning data\nIn this book, students use data we provide, or data they choose from data portals. When you bring in your own data, it’s often “unruly.” That requires a whole new set of skills.\n\n\nVisualization and communication\nThis is a giant topic to which we cannot do justice. Still, there are a few things that need to be said."
  },
  {
    "objectID": "03.40-Summarizing.html#using-the-ruler-palette-in-a-graph",
    "href": "03.40-Summarizing.html#using-the-ruler-palette-in-a-graph",
    "title": "15  Summarizing",
    "section": "15.1 Using the “ruler” palette in a graph",
    "text": "15.1 Using the “ruler” palette in a graph\nThe easiest way to calculate a summary is to click in the ruler palette attached to a graph. Depending on what kind of attribute (numerical, categorical) is on an axis, different measures are available.\nIf the attribute is numerical (like Height) you can display things like the mean or median, and additional goodies such as box plots, which implicitly show the median and the quartiles. To see the numerical value of one of these measures, hover over the line that appears in the graph. If the attribute is categorical (like Marital_status), you can display counts and percentages. And if there are two numerical axes you can display least-squares regression lines, which give you slope, intercept, and r-squared."
  },
  {
    "objectID": "03.40-Summarizing.html#writing-formulas-section",
    "href": "03.40-Summarizing.html#writing-formulas-section",
    "title": "15  Summarizing",
    "section": "15.2 Writing formulas",
    "text": "15.2 Writing formulas\nIn the chapter on grouping, you saw how to use dragging to the left to define groups, as in the following figure. We made the degree attribute–the one we used for grouping— using the technique described here. Thus, we have two groups: those with college degrees and those without.\n\n\n\nTable with grouping by whether the person has a college degree.\n\n\nLet’s investigate the income difference between these two groups. We need a number, a summary value, to characterize the income of each group. We’ll pick the median of the income.\nWe have done this already in the second demo lesson about height, age, and gender, Children and Teens, Part 2. There, we had made our groups (by dragging Age and Gender to the left) and made a new column, with a formula, mean(Height).\nLet’s apply that same pattern to this situation.\n\n\n\n\n\n\nNote\n\n\n\nIn the chapter on calculating, you learned about writing formulas to re-express existing data in a new column. This chapter is about formulas for “stuff like averages.” These formulas will be different in some important ways. The key difference is that a function like mean()— which you use to compute the average value—applies to an entire group of cases, whereas a function like abs()—for absolute value—applies to one case at a time.\n\n\nTo calculate the median income of each group, start by making a new column on the left side of the table. Let’s call it medInc. Then give it a formula: median(Income).\nIf you want to try this yourself,\n\nthat will appear in a new tab.\n\n\n\n\n\n\nMaking the new attribute…\n\n\n\n\n\n\n\n…and giving it a formula\n\n\n\n\n\nWhen you click Apply, CODAP fills in the medInc column with the values.\n\n\n\nThe medians appear in the medInc column.\n\n\nThat formula will keep computing the median income even if you change the grouping. The next illustration shows what you see if you drag Gender to the left as well, which makes four groups, one for each combination of males and females, with and without degrees.\nYou can, of course, make graphs using those new columns. They might have very few points, but they help tell a story.\n\n\n\n\n\n\nMedians in place with Gender as well as degree\n\n\n\n\n\n\n\nAnd a graph of those values\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBy the way, this shows a common statistical blunder: the red lines are medians, but not the medians of the incomes for the ‘yes’-degree group and the ‘no’-degree group. They show the medians of the dots we see, that is, the points halfway between the males and females."
  },
  {
    "objectID": "03.40-Summarizing.html#summarizing-categoricals-section",
    "href": "03.40-Summarizing.html#summarizing-categoricals-section",
    "title": "15  Summarizing",
    "section": "15.3 Summarizing Categorical Attributes",
    "text": "15.3 Summarizing Categorical Attributes\nWe often think of summaries as means or medians—or more elaborate statistical quantities such as percentiles or standard deviations.\nAlas, those functions don’t make sense for categorical attributes. (What’s the mean gender of a group?) What should we do? Let’s think about a situation where we want to compare groups; then we’ll think about how to use a categorical attribute to compare them.\nIn fact, let’s use the\n\n\nWe divided up our sample of people into two groups: those with college degrees and those without. Back then, we compared their median incomes; and income is numeric. But what about employment? That’s categorical—let’s use it.\nWhat does your stereotype say? Mine says that people with college degrees are more likely to have a job. That would mean that a greater proportion (or percentage if you multiply by 100) of the “degree” group would be listed as “civilian employed.”\nWe can do this just like before, but we’ll need a (slightly) more elaborate formula. The basic idea is:\n\nCount how many people have a value of Empl equal to \"Civ Employed\".\nCount how many people there are altogether.\nDivide the first number by the second.\n\nHere is what the formula looks like, and how it turned out.1 Our college grads are more likely to have a job:\n\n\n\n\n\n\n\n\n\n\nThe formula for the proportion employed, propEmpl, and the result."
  },
  {
    "objectID": "03.40-Summarizing.html#more-functions-section",
    "href": "03.40-Summarizing.html#more-functions-section",
    "title": "15  Summarizing",
    "section": "15.4 More Functions",
    "text": "15.4 More Functions\nOkay, we’ve talked about mean() and median(), sum() and count(). What else have we got that we can use to summarize data?\nWhen you open up the formula editor, there is a button labeled –Insert Function–. Click it to produce a menu. It has seven categories of functions to choose from; each category has several options. The left illustration shows the statistical submenu. You can see a whole slew of functions.\nEach function has an information button to the right of the name. The right-hand illustration shows what the info looks like for the (rather complicated) percentile() function.\n\n\n\n\n\n\nLeft: the statistical menu under –Insert Function– in the formula editor.\n\n\n\n\n\n\n\nRight: information on percentile().\n\n\n\n\n\nIf you read this carefully, you can conclude that to get the 10th percentile in weight, you would use percentile(Weight, 0.1)."
  },
  {
    "objectID": "03.40-Summarizing.html#implied-filters-section",
    "href": "03.40-Summarizing.html#implied-filters-section",
    "title": "15  Summarizing",
    "section": "15.5 Implied Filters: Booleans inside your functions",
    "text": "15.5 Implied Filters: Booleans inside your functions\nYou are used to using functions like this:\nmean(Weight)\nBut CODAP functions are more flexible than that. You could write, for example,\nmean(Weight / Height^2, Empl = \"Civ Employed\")\nand you will get the average BMI for all of the people with jobs2.\nThat is, you can take the mean of an expression—not just a plain attribute— and CODAP will calculate that value for every case before taking the mean.\nThe second argument, after the comma, is a filter. Only those cases for which the expression is true will be in the calculation.\nThat is, you can perform many calculations and apply filters entirely in formulas, without ever hiding or setting aside or making new columns.\nThis is a perilous idea. Sure, you could study income inequality and use a formula like,\nmedian(Income/FamilySize, Gender = \"Female\" AND Age > 24 AND Empl = \"Civ Employed\")\nThe problem is that the formula is invisible. If you accidentally use 34 instead of 24 for the males’ formula, you might never notice. And you can’t see whether your idea of “income per person” makes sense. It’s far better to make a column for incomePerPerson so you can graph it, and play with it, and see whether it really expresses what you want.\nThat said, those invisible filters are really useful sometimes.\n\nThe count() function\ncount() is a very powerful function. It’s also subtler than it looks.\nLet’s look at the expression count(Empl = \"Civ Employed\").\nFirst, notice the quotes around the string, \"Civ Employed\". When you want CODAP to recognize or use a specific string (of characters), you have to enclose them in double quotes.\nSecond, when you’re looking at a specific string, it has to be exactly correct. It’s case-sensitive; count(Empl = \"civ employed\") will not work.3\nBut third, and the real point here, is that the expression Empl = \"Civ Employed\" is not an attribute—it’s a Boolean expression that’s either true or false. Notice how different that is from a formula like mean(Height). The thing in parentheses, Height, is just an attribute."
  },
  {
    "objectID": "03.40-Summarizing.html#summarizing-commentary",
    "href": "03.40-Summarizing.html#summarizing-commentary",
    "title": "15  Summarizing",
    "section": "15.7 Commentary: CODAP’s “atomic” case orientation",
    "text": "15.7 Commentary: CODAP’s “atomic” case orientation\nThis is a good place to say that CODAP is designed to work well when the cases are “atomic” bits of data, and that the user builds summaries such as means from those atoms.\nYou will often use datasets that are pre-summarized, for example, COVID data where each row, each case, is a State or a country, and the attributes are date, number-of-cases, cases-per-capita, and so forth.\nAfter date, those are all summaries—calculations that somebody else made for us using, presumably, a database where the rows were individual COVID cases.\nThis is fine, but you will often find that the data-analysis situation is more complicated; that you are more “awash.” You have to be more careful that your calculations make sense. For example, if you take the median of cases-per-capita, what does that really mean?\nGraphs can be troubling as well; sometimes, where a dot seems perfect for an atomic case— a person in the Census, say— we often want a bar or something else to show groups. You can do that! See this section on how to fuse dots into bars.\nThe Map tool is another example of CODAP’s branching out: when the case is a State or a country, you can use a map to show values as colors on the map."
  },
  {
    "objectID": "03.50-Joining.html#just-adding-data",
    "href": "03.50-Joining.html#just-adding-data",
    "title": "16  Joining",
    "section": "16.1 Just adding data",
    "text": "16.1 Just adding data\nIf you want to add a single case to an existing dataset, the easy solution is simply to type it into the empty row at the bottom of the table. If your datasets are medium-to-large, that’s not likely to happen very often—but it does happen, so it’s good to know.\nIf the table is already organized hierarchically, you will need to click on the number of a case that’s in the same category as your new data, and choose Insert Case from the menu that appears.\nIf you have two or more datasets that are structured identically and you want to combine them, CODAP has no easy command. One solution is to export them both as csv, combine them in a text editor, and then re-import them.\nOf course, the two datsets need to have exactly the same attributes in the same order. You should also be sure to remove the attribute names that were ideally in the first line of the second file!"
  },
  {
    "objectID": "03.50-Joining.html#a-true-join-and-a-simple-solution-grouping-and-typing",
    "href": "03.50-Joining.html#a-true-join-and-a-simple-solution-grouping-and-typing",
    "title": "16  Joining",
    "section": "16.2 A true join and a simple solution: grouping and typing",
    "text": "16.2 A true join and a simple solution: grouping and typing\nThis one came up in class. A student was using a Census data portal and was studying how income inequality had changed over time. They got income data from 1950, 1980, and 2010. The problem was, the incomes in 1950 were a lot smaller that in 2010, so in the graphs, it was hard to make the comparison. They wanted to know if they could somehow, you know, like, adjust it.\nWith a little prompting, they looked up the consumer price index in those three years. Then it was a matter of grouping by year (dragging left, which made three cases, one for each year), creating a new column for the CPI, and simply typing the CPI into those three empty cells. That is, they did not need to write a complicated formula, and they didn’t need to type thousands of entries.\nFinally, they made a new column next to their income attribute and created a formula for the adjusted income. The very idea of adjusting the income like that is really powerful, and of course happens all the time in everyday statistics. See this chapter if you want to know more.\n\nThe astute reader will notice that the drag-left-to-make-a-new-column-and-just-type technique here is the same one we used for recoding some categorical variables, which we counted as a calculating data move (see this section). It’s interesting that we can use the same CODAP feature— dynamically reorganizing the hierarchical structure of a dataset— to do different data moves.\n\nxxx do we need illustrations of this process? A live place to try out adjusting?"
  },
  {
    "objectID": "03.50-Joining.html#a-true-hairy-joining-example",
    "href": "03.50-Joining.html#a-true-hairy-joining-example",
    "title": "16  Joining",
    "section": "16.3 A true, hairy, joining example",
    "text": "16.3 A true, hairy, joining example\n\nThis section’s example will show you how the sausage is made. If you just want to eat the sausage, skip to the next section!\n\nWhat if it’s more than just three years, though? What if there are hundreds of things from one dataset to type into another? That’s impractical, you’re likely to make mistakes, and it would be really tedious.\nFor that situation, you need the computer to help. Sadly, to get it right can be hard. Computer commands to do this are really fussy, and the syntaxes are often Byzantine. Furthermore, there are different ways to join two datasets. You’ll see references to “inner joins” and “outer joins” and the like.\nWe will do only one kind here, using the BART scenario from the beginning of this chapter. Let’s describe the setup:\nOur main dataset (riders) contains records of individual people exiting the BART system. It tells the time–to the second—that they tagged out of the exit gate, plus the number of the station they entered and the number of the station they exited. Here is an example of what’s in it:\n\n\n\nnum\ntime\nexit\nenter\n\n\n\n\n65139\n08:05:57\n14\n20\n\n\n65138\n08:05:57\n14\n22\n\n\n65156\n08:05:58\n14\n18\n\n\n\nThat’s all great, but if I’m doing the data analysis, I want the names of the stations, not their numbers.\nOur second dataset (stations) is a list of all the BART stations. It includes the names of the stations and the numbers BART uses to designate the stations in the person records. Here is a snippet:\n\n\n\ncode\nfinCode\nname\n\n\n\n\n14\nBK\nDowntown Berkeley\n\n\n18\n12\n12th Street\n\n\n19\nLM\nLake Merritt\n\n\n20\nFV\nFruitvale\n\n\n21\nCL\nColiseum\n\n\n22\nSL\nSan Leandro\n\n\n\nReading the tables, we can see that all of our passengers got off at Downtown Berkeley—and all within a second of each other—but they got on at various stations: 12th Street, Fruitvale, and San Leandro.\nSo our plan is this: for each person record, remember the number of the station they exited from. Find that record in the station database, and remember the name of the station in that record. Finally, back in the person database, write that name in a new column.\nTry this in the live illustration below ( or in this separate tab):\n\nMake a new column in riders; call it enterName.\nGive it this weird formula; you can find the function lookupByKey in the Lookup Functions section of the formula editor:\n\nlookupByKey(\"stations\", \"StationName\", \"StationCode\", enter) \nYou should see the new column fill with station names. Of course, you should see what this remarkable data looks like; make a new graph and put time on the horizontal axis. If you like, color the points by station. (You will also have to make the graph really big to see the points!)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat a cool graph, right? Be sure you think about what’s happening to make the graph this way. It does make sense.\nThen, notice that some piles are much bigger than others. Why is that? (It will help to understand the geography of BART. Here is a map. Find Downtown Berkeley!)\n\n\nNow, about that crazy function. How in the world did we figure out what to put inside the parentheses? No normal mortal can remember, so if you want to construct such a formula yourself, do this:\n\nOpen the formula editor for enterName and delete the formula.\nIn the — Insert Function— button menu, choose Lookup Functions. You’ll see the left side of this next figure:\n\n\n\n\n\n\n\nLeft: the lookup function menu.\n\n\n\n\n\n\n\nRight: what you get in the editor when you choose the function.\n\n\n\n\n\nThe info button—the little i in the circle—will give you detailed (albeit still confusing) help with the arguments of the function. The arguments are the four things that go inside the parentheses, separated by commas.\nThen, when you actualy choose the lookupByKey item from that menu, it will insert the function into the function editor, with helpful text that you can edit… which is what you see on the right side in the figure.\nThose four arguments are:\n\n\n\nargument\nwhat it means\n\n\n\n\n\"otherDataSet\"\nThe name of the other dataset, in quotes because it’s a string: \"stations\"\n\n\n\"returnedAttrName\"\nThe name of the attribute in the other dataset that you want. That’s \"StationName\"\n\n\n\"keyAttrName\"\nThe name of the attibute in the other dataset you’re using as a “key”: \"StationCode\"\n\n\nkeyValue\nThe attribute in this dataset that matches that key. Note, not a string, so no quotes: enter\n\n\n\nThe concept of the key is, well, key. It’s what’s the same between the two datasets.\nThink about it from the computer’s point of view: for each case in the riders table, you have to find the right case in the stations dataset. How do you know which one? You do exactly as we described above when we were thinking about it by hand: you find the code for the station (which is in enter) and match it with the code in the other dataset (which is in StationCode). You also need to know what value to get from that stations record; for us, that’s the name, StationName. The other attribute is the name of the dataset you’re looking into, of course.\nThis whole dance is tortuous, I know. But once you have it all set up correctly, it can bring in thousands of data values quickly and reliably.\nHere are two applications you might not immediately think of, to whet your appetite:\n\nSome datasets have geographic information, such as the latitude and longitude of BART stations. If you have attributes named lat and long in your dataset, points will plot on a map!\nYou can do a lookup into your own dataset. Suppose you have a dataset of people, including people from several generations of the same families, and you give each one an ID. Now suppose you have information about eye color, or the ability to roll your tongue. You can make attributes for mother and father, and use lookup to study the genetics of these traits."
  },
  {
    "objectID": "03.50-Joining.html#drag-to-join-section",
    "href": "03.50-Joining.html#drag-to-join-section",
    "title": "16  Joining",
    "section": "16.4 An easier way to do a true join",
    "text": "16.4 An easier way to do a true join\nAlthough the procedures in the previous section will help you understand the details of how to do a join in CODAP, there is an easier way: you can simply drag an attribute from one table to another. If you do this the right way, CODAP writes all those messy formulas for you. Let’s do an example.\nThe live illustration below has two tables. One has numbers of vehicle registrations for each State (plus DC). The other has a bunch of data on accidents in each State, and that data includes the population.\nNow suppose we wanted to know how many registered vehicles there were per person in each State. We’d take the number of registrations and divide it by the population. But they’re in different tables, and we are so lazy that we do not want to type in 51 large numbers.\nWe want to join the tables.\nWhen you do this, you have to think, “join by what?”\nIn this case, we want to join them by State, that is, we want to extend each row in table A, appending all the information from a row in table B. Which row? The one that has the same value for the name of the State.\nHere is what you do:\n\nDrag the column heading for State from one table, and drop it on the State heading in the other one.\n\n\n\n\n\nCheck that it worked. Make a new column that performs the calculation. Make a graph of the result! Delaware had the most cars per person in 2018, at about 0.45.\n\nIf you look at the formulas for the new columns, you’ll see that they are just lookupByKey() formulas like the ones we laboriously constructed in the previous section.\n\n16.4.1 The Berkeley problem revisited\nLet’s use this sweet dragging technique to solve the problem of finding the names of the stations where our passengers entered the BART system. The same live illustration appears below that we saw in an earlier section. But this time…\n\nDrag StationCode from the stations table and drop it on enter in the riders table.\n\nNew columns will appear in riders that include the name of the station, as before.\n\n\n\n\n\nComplications with dragging to join\nIs that all? Yes, but of course it can get more complicated than that.\nFor example, with the vehicle data, you might have noticed that the new information for DC did not get copied. This is because one table has District of Columbia for the name and the other one has Dist. of Col.. It didn’t match exactly, so it did not copy the data over.\nAlso, with the vehicles, the order of the drag did not matter because there were the same number of States in both tables.\nBut in the Berkeley-BART example, the 900-plus-case riders table had many copies of each station code in the enter column, because many people might have gotten on there to go to Berkeley.\nIn that case, we wanted to add the station-name information to the riders table, so we dragged from stations to riders.\nIf you do that drag in the other direction, CODAP will just add information from the first record it finds. In that case, for every station, you would get information (only) from the first passenger to travel from that station to Berkeley."
  },
  {
    "objectID": "03.60-preparing-and-cleaning.html#missing-data",
    "href": "03.60-preparing-and-cleaning.html#missing-data",
    "title": "17  Preparing and Cleaning Data, a.k.a. Munging",
    "section": "17.1 Missing Data",
    "text": "17.1 Missing Data\nSometimes, a piece of data is missing. Sometimes, for some reason, there is no value for that attribute in that case. If you’re measuring the heights of all the students in the school, but Alicia was absent that day, her height cell may be empty.\nCODAP’s signal for a missing value is, in fact, an empty cell. If a case has a missing value, and that attribute is used in a plot or calculation, CODAP will not plot the point or use the value. It will be as if that case was not in the data set.\nWhen you export data to csv format or import a csv file, a missing value will appear as—nothing! You will see commas right next to each other:\nname,height,age\nTim,74,65\nAlicia,,12\nAloysius,68,16\nPenelope,65,17\nMost of the time, the way CODAP treats missing data is exactly what you want. Unfortunately, not everyone agrees on how to cope with missing data. Some of the Census data portals, for example, use a huge number to indicate “missing.” Two things to note:\n\nTherefore you need to go in and change all those big numbers to nothingness, so CODAP will treat them as missing, and\nIf you forget or don’t notice, you’ll get massively wrong results.\n\nIn the live illustration below, we have 1000 cases from the Census data portal. A student (Aloysius) has grouped the data by Sex1 and computed an aggregate value: the mean income. He concludes that women make more than men because their mean income is nearly 1.9 million dollars while the men make only about 1.6 million.2\nAloysius should notice that the numbers are ridiculous and that something serious is wrong. He should immediately try to figure out how this bogus number appeared. Which leads to the second big mistake: Aloysius should have made a graph first in order to get a feel for the data. So do the following in the live illustration:\n\nMake a graph of Sex by Income. Notice the huge stack of points at a high value.\nSelect one of those points and look at the table.\n\nYou’ll see that some people have incomes of $9,999,999. In fact, if you look further, you’ll see that many of them are children.\nWhat’s going on? The Census data uses that number, 9999999 as their code for “missing.” You could just select them and hide them or set them aside. But let’s actually change them to missing.\nThere are two basic strategies: do it in CODAP or do it elsewhere. Let’s do CODAP first. We want to change all of those 9999999s at the same time:\n\nDrag Income to the far, far left, making a new level of hierarchy. We seldom group by a numeric value, but this is a perfect time to do so.\nFind the 9999999 value, and edit it (double click) to be…nothing. Empty. Note: not zero, that’s entirely different!\nDrag Income back to the far right. Presto! Reasonable values! (Men’s mean is $47077.74; women get $28249.67.)\nScroll in the table to notice the empty cells in the Income column.\n\n\n\n\nYou could also have made a separate “income” column, so as not to eliminate the original data.\nAn alternative strategy—which is sometimes better than using CODAP for things like this— is to export the data from the table, edit the csv file in a text editor, and then import it into a fresh CODAP document. See below."
  },
  {
    "objectID": "03.60-preparing-and-cleaning.html#topcodes",
    "href": "03.60-preparing-and-cleaning.html#topcodes",
    "title": "17  Preparing and Cleaning Data, a.k.a. Munging",
    "section": "17.2 Topcodes",
    "text": "17.2 Topcodes\nSome attributes have special values that mean, “the value here is bigger than the maximum value we care about.” An example is the poverty level as reported in Census data. In some Census data, this value is the “percent of the poverty level” for that person, computed from their income and their family size (The poverty level is higher for bigger families).\nIn that system, a value of 100 means “at the poverty level.”\nBut once you’re above a certain level, you’re not even close to being impoverished, so they put a number in to indicate, “out of poverty.” In some datasets, that value is 501, that is, your income is more than five times the poverty level.\nOne consequence is that if there is a topcode, you must not calculate the mean poverty level for a group, or, at least, you have to interpret it correctly!"
  },
  {
    "objectID": "03.60-preparing-and-cleaning.html#cleaning-with-a-text-editor",
    "href": "03.60-preparing-and-cleaning.html#cleaning-with-a-text-editor",
    "title": "17  Preparing and Cleaning Data, a.k.a. Munging",
    "section": "17.3 Cleaning with a Text Editor",
    "text": "17.3 Cleaning with a Text Editor\nThis topic could be a whole book, so we will have to be satisfied with first steps.\nA text editor is an essential tool for data preparation and cleaning. CODAP will help a lot, but with real, messy data, a good text editor will save you hours with a large data set. The key skill is getting good with global replace.\n\n\n\n\n\n\nNote\n\n\n\nThe tool we’re talking about should be a plain text editor so that the result is a genuine plain text csv file. A Word document (.docx) will not work. Word (or Pages or any other word-processor) can indeed “save as plain text” or “export to plain text,” but you may have a decent plain-text editor on your computer.\n\n\nGlobal replace is your friend. Usually it’s in the Edit menu, often part of Find. If you want to change all the values 9999999 to be missing (as in the example above), do a find for 9999999, put nothing in the Replace box, and choose Replace all.\nSometimes you have to be tricky. Here’s a common situation: you’ve copied a table from the web and pasted it into your editor. You want to turn it into a csv file: comma-separated values.\nHere is some earthquake data in a table in Wikipedia:\n\n\n\nEarthquake distribution, 2009–2019.\n\n\nIf I just copy that part of the web page and paste it sinto a text editor, I see this:\nMagnitude   2009    2010    2011    2012    2013    2014    2015    2016    2017    2018    2019\n8.0–9.9 1   1   1   2   2   1   1   0   1   1   1\n7.0–7.9 16  21  19  15  17  11  18  16  6   16  9\n6.0–6.9 144 151 204 129 125 140 124 128 106 117 133\n5.0–5.9 1,896   1,963   2,271   1,412   1,402   1,475   1,413   1,502   1,451   1,675   1,489\n4.0–4.9 6,805   10,164  13,303  10,990  9,795   13,494  13,239  12,771  11,296  12,777  11,349\nTotal   8,862   12,309  15,798  12,548  11,341  15,121  14,795  14,420  12,860  14,586  12,985\nNotice these important facts:\n\nThe spaces between the values are tabs.\nThe numbers use commas to indicate thousands.\n\nIf I drop this in as a csv, CODAP might interpret the tabs correctly, but the commas will cause problems, either making more values or turning numeric values into categoricals. We want to get rid of the commas and change the tabs to…commas.\nTherefore, order matters. If you change tabs to commas first, you will have this:\nMagnitude,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019\n8.0–9.9,1,1,1,2,2,1,1,0,1,1,1\n7.0–7.9,16,21,19,15,17,11,18,16,6,16,9\n6.0–6.9,144,151,204,129,125,140,124,128,106,117,133\n5.0–5.9,1,896,1,963,2,271,1,412,1,402,1,475,1,413,1,502,1,451,1,675,1,489\n4.0–4.9,6,805,10,164,13,303,10,990,9,795,13,494,13,239,12,771,11,296,12,777,11,349\nTotal,8,862,12,309,15,798,12,548,11,341,15,121,14,795,14,420,12,860,14,586,12,985\nwhich is a disaster! (Make sure you understand why!)\nSo we must get rid of the commas first! Do a global replace with a comma in the Find box and nothing in Replace. Then do a Replace all.\nMagnitude   2009    2010    2011    2012    2013    2014    2015    2016    2017    2018    2019\n8.0–9.9 1   1   1   2   2   1   1   0   1   1   1\n7.0–7.9 16  21  19  15  17  11  18  16  6   16  9\n6.0–6.9 144 151 204 129 125 140 124 128 106 117 133\n5.0–5.9 1896    1963    2271    1412    1402    1475    1413    1502    1451    1675    1489\n4.0–4.9 6805    10164   13303   10990   9795    13494   13239   12771   11296   12777   11349\nTotal   8862    12309   15798   12548   11341   15121   14795   14420   12860   14586   12985\nIt turns out that CODAP would interpret this just fine; it approves of tabs as separators.\nBut suppose we want to replace all the tabs with commas. The tricky part here is that if you try to type a tab in the Find box, the cursor will probably jump to the Replace box. No problem. Copy a tab from the document and Paste it into the Find box. Put a comma in Replace, choose Replace all, and you’re done.\nMagnitude,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019\n8.0–9.9,1,1,1,2,2,1,1,0,1,1,1\n7.0–7.9,16,21,19,15,17,11,18,16,6,16,9\n6.0–6.9,144,151,204,129,125,140,124,128,106,117,133\n5.0–5.9,1896,1963,2271,1412,1402,1475,1413,1502,1451,1675,1489\n4.0–4.9,6805,10164,13303,10990,9795,13494,13239,12771,11296,12777,11349\nTotal,8862,12309,15798,12548,11341,15121,14795,14420,12860,14586,12985\nNow, that is in perfect shape for import—–even though it has other problems!\nIt takes a while to learn to wield the power of global replace. You will use Undo a lot while you’re learning. There are more powerful tools, such as using regular expressions in your replace commands, but plain old Replace all will take you far."
  },
  {
    "objectID": "03.60-preparing-and-cleaning.html#clean-does-not-mean-useful",
    "href": "03.60-preparing-and-cleaning.html#clean-does-not-mean-useful",
    "title": "17  Preparing and Cleaning Data, a.k.a. Munging",
    "section": "17.4 “Clean” does not mean “useful”…",
    "text": "17.4 “Clean” does not mean “useful”…\nI cannot tell you how many times I have cleaned a file and imported it only to realize that the problems went deeper than just cleaning.\nIn the file we just looked at, there are two problems, one small, one big.\nThe first is small: there is a row for Total. We should get rid of it. CODAP is an analysis package. We do our own totals. More to the point, if we did do a total, we’d get a total that included the Total.\nA more insidous problem is how the table is organized. It’s often good to ask yourself, “what is a case here?”\nEach row of the table is a range of magnitudes. Each column is a year. Really, we’d probably rather the years were rows; then it would be easy to make a “total quakes” with a formula that just added the numbers for the magnitude ranges.\n\nTransposition\nThis is a job for transposition: making the rows into columns and the columns into rows. CODAP’s Transformers plugin can help you transpose data, but Google sheets also does it (as does any spreadsheet program such as Excel or Numbers). So let’s see how to do this using sheets.\n\nMake sure your data’s filename ends in .csv.\nImport your csv file into Sheets (File/Import) and select the area your data is in.\n\n\n\n\nEarthquake distribution, as seen in a Google Sheet.\n\n\n\nCopy the area.\nSelect some empty cell.\nIn the Edit menu, choose Paste Special, and then Transposed. The data appears with rows and columns reversed. It is also selected.\n\n\n\n\nEarthquake distribution after transposition.\n\n\n\nWith the new data area selected, Copy again.\nMake a new, empty document in CODAP.\nIn the Tables tool, choose new from clipboard.\n\nShazam! You have the transposed table in CODAP.\n\nThe first attribute will be called Magnitude. Because of the transposition, that’s now wrong! Change its name to something appropriate, in our case, Year.\n\n\n\n\nEarthquake time series in CODAP, showing the pattern of 4’s and 5’s.\n\n\n\n\nStacking and tidification\nSuppose you survey some friends, asking each one if they are a cat person or a dog person. Your data might look like this:\n\nCat vs Dog survey results\n\n\n\n\n\n\nCat\nDog\n\n\n\n\nAbby\nChris\n\n\nBlaine\nFrank\n\n\nEmily\n\n\n\n\nThis is a perfectly reasonable format for collecting your data, but it can give data anaysis software some trouble. The problem is, what is a case? It looks as if the first case is Abby and Chris, so is a case a pair? Then what about Emily? And what connects Abby to Chris except that they were the first people surveyed who preferred cats and dogs, respectively.\nCODAP (and for many other programs) would prefer that you format the data like this:\n\n\n\n\n\n\n\nName\nLikes\n\n\n\n\nAbby\nCats\n\n\nBlaine\nCats\n\n\nChris\nDogs\n\n\nEmily\nCats\n\n\nFrank\nDogs\n\n\n\nNow a case is a person, with two attributes: a name and a preferred pet.\nNotice that this is not the same thing as the transposed data, above. This time, we have taken the attribute names and made them the values of the second attribute, Likes, and taken all the values of both attributes (Cat and Dog) and made them the values of Name.\nSo: if your data come that first way, how do you get them into the second, preferred form? We call the procedure stacking, and there’s a plugin for that.\nWe’ll expand this eventually! xxx Also xxx to show how to enter the data in hierarchy."
  },
  {
    "objectID": "03.70-visualization.html#codap-graphs-in-general",
    "href": "03.70-visualization.html#codap-graphs-in-general",
    "title": "14  Visualizing and Communicating",
    "section": "14.1 CODAP graphs in general",
    "text": "14.1 CODAP graphs in general\nIn a graph, by default, CODAP represents each case as a dot.\nAs we mentioned up in the commentary for the summarizing data move, CODAP is oriented to individual cases that are “atomic” bits of data, for example, people in a Census, or moments of time in a record of some phenomenon. So dots are pretty good representations.\nEvery graph has, basically, two axes. When you put an attribute on an axis, each dot moves so that its location corresponds to its value for that attribute.\nIf the attribute is numeric or a date, the axis has numbers or dates, and CODAP centers the dot on that value. If the attribute is categorical, CODAP places the dot in a bin labeled by the categorical value. By separating the dots into bins, CODAP is grouping them: categorical plots give you a data move for free.\nIf you plop an attribute into the middle of a graph, the points color depending on the values of that “legend” attribute. That gives you a kind of third dimension in your graph, which you can use to show additional relationships, or to emphasize one of the attributes on an axis.\nPeople seem to understand all of that intuitively, and it gets them pretty far. But there’s more."
  },
  {
    "objectID": "03.70-visualization.html#codap-graphing-tips",
    "href": "03.70-visualization.html#codap-graphing-tips",
    "title": "14  Visualizing and Communicating",
    "section": "14.2 CODAP Graphing Tips",
    "text": "14.2 CODAP Graphing Tips\nThe next few sections describe some of the features of CODAP graphs that you might not find on your own.\n\n14.2.1 Palettes\nBlue-green palettes are attached to the right side of a graph. There is a vertical array of icons; each one has a number of controls or choices inside it.\nPalettes also appear when you select a table, a map, or a slider. We’ll describe those features here as well, even though this chapter is about visualization.\nThe palettes appear only when the graph or table is selected. That is, there is only ever one set of palettes on the screen at a time.\n\n\n\n\nicon\n\n\ndescription\n\n\n\n\n\n\n\nRescale tool. Press to show all points or rescale column widths.\n\n\n\n\n\n\n\n“Eyeball” palette. In a graph, use it to show or hide points.\nIn a table, use it to set cases aside.\n\n\n\n\n\n\n\n“Ruler” palette. In a graph, display summaries (e.g., means, percents, standard deviation).\nIn a table, make new attributes or export data.\n\n\n\n\n\n\n\n“Paintbrush” palette. Change the appearance of points in a graph. Control color, stroke, and size.\n\n\n\n\n\n\n\n“Snapshot” palette. Take a picture of the graph, and then export that picture. Edit the picture first if you wish in the Draw Tool.\n\n\n\n\n\n\n\nConfiguration palette. In a graph with a numeric axis, bin the data. Where appropriate, fuse dots into bars.\n\n\n\n\n\n\n\nTrash can. In a table (only), delete cases.\n\n\n\n\n\n\n\n\n\n\nGotcha!\n\n\n\nYou will frequently select cases, i.e., select points in Graph A, and then want to use the eyeball tool in graph B. If you just click in Graph B, however, you may change or eliminate the selection of points! The solution is to click on Graph B’s title bar. Then the selection from Graph A is preserved.\n\n\n\n\n14.2.2 Re-ordering categoricals\nIf you make a graph with a categorical attribute, by default CODAP orders the alphabetically. Sometimes, this is maddeningly unhelpful!\nIn many cases, the solution is quick and easy: grab the categorical value by the name, and drag it where you want it.\nxxx add example\n\n\n14.2.3 Fusion Power: Bar charts\nIf you have made a categorical graph, you can change the default “dot” display into a bar chart, which may be more familiar.\nAlso, sometimes, when you have created summary values (such as the means of something for several groups)), it may be effective and appropriate to display those aggregate values as bars.\nMake bars using the “configure” palette  and choose the option Fuse Dots into Bars.\nxxx add example\n\n\n14.2.4 Showing data directly as bars\nxxx\n\n\n14.2.5 Histograms\nHow do you make a histogram in CODAP? You might expect (correctly) that you will fuse dots into bars at some point, but first you need to bin the data.\n\n\n\n\n\n\nNote\n\n\n\nI am making a distinction here between bar charts and histograms. We just made bar charts when we fused dots with categorical values. Notice that they had spaces between the bars.\nIn a histogram, the data are numerical, and there is no gap between the bars.\n\n\nThe live illustration below has 1000 heights of people between 25 and 35 years old. We want to make the dot plot into a histogram. Do the following:\n\nSelect the graph and open the configuration palette \nChoose Group into Bins.\n\nThe graph adjusts into bins; notice that each bin is 10 cm wide.\n\nAgain in the configuration palette, choose Fuse Dots into Bars.\n\nNow you have a histogram! Use the configuration palette to experiment with the other controls, such as the bin width.\n\n\n\nYou would think that if you put Gender on the vertical axis, that it would split the histogram. But as of May 2023, you will only get the binned version—not fused. Still useful, though!\n\n\n14.2.6 Connecting lines\nxxx add example"
  },
  {
    "objectID": "03.70-visualization.html#student-graphs-and-commentary",
    "href": "03.70-visualization.html#student-graphs-and-commentary",
    "title": "14  Visualizing and Communicating",
    "section": "14.3 Student graphs and commentary",
    "text": "14.3 Student graphs and commentary\n\n\n\nA student graph about mobility.\n\n\nThis graph leaves us in an “awash” state. It does show the data, and a careful reader can figure out some patterns, but it’s too busy, too complicated, too much work for the reader to understand what’s going on. As a good communicator, you want your audience to get your message as easily as possible. Sure, you want them to think, but you want them to start thinking already seeing what you see in the data.\nSo consider two things we could do about it. One is to turn dots into bars, and leverage the legend. The other is to make some a new summary attribute or two, and plot those.\n\nNotice that both choices involve grouping and summarizing. The bars-and-legend version gives you one plausible graphic scheme for free; yet the DIY choice—making attributes—is more flexible, though for the moment it limits you to dots.\n\nxxx more examples in the hopper\nxxx include facet splitting, earlier in this chapter"
  },
  {
    "objectID": "04-codap-part.html#why-attributes",
    "href": "04-codap-part.html#why-attributes",
    "title": "CODAP Topics",
    "section": "CODAP Terminology",
    "text": "CODAP Terminology\nBefore we turn you loose, however, a few words on terminology. In a sense, what you call something doesn’t matter, especially if you are comfortable calling it something else, and you don’t create any ambiguity. We will not send the vocabulary police to your door. But we in the inside-CODAP world use specific terms to refer to things. Since we have thought pretty hard about these terms, I will share some of those with you now.\nCODAP is case-centric. Everything revolves around the case. A case is a single unit of data, a row in the table, a point on a graph. If it’s Census microdata, a case represents one person. When students are confused about their data, the problem is frequently that they do not understand what the case is in their data set. And when you are designing an investigation that’s worthy of making you awash in data, “What is a case?” is often a crucial question.\nAttributes are properties of the case. They appear as columns in tables. You could call them variables, but that word is already overloaded in math class. Besides, in general, they don’t vary. So we call them attributes. If a case is a person, that person might have attributes like height and political-party.\nValues are the actual data that appear in the table cells. Every case has a value for each attribute. For example, suppose a case is a person with an attribute of name. That attribute might have a value of Aloysius. In a graph, that value determines where the dot goes. Some values might be missing. That’s OK; those points won’t appear in the graph. Students often confuse attributes with values; they will say, “make a graph of high school grad” when they mean, “make a graph of Education.”\n\nHow that affects graphs\nxxx sliders\nxxx case card view"
  },
  {
    "objectID": "04.10-types-of-attributes.html#how-codap-gets-it-wrong",
    "href": "04.10-types-of-attributes.html#how-codap-gets-it-wrong",
    "title": "19  Types of Attributes",
    "section": "19.1 How CODAP gets it wrong",
    "text": "19.1 How CODAP gets it wrong\nFor the most part, CODAP interprets your data values the way you expect. But there are exceptions.\nNumeric variables that are really categorical. In the original Titanic data set, the survived attribute is 0 for “died” and 1 for “lived.” That’s just the way the data were entered; it was easier to enter ones and zeros than typing in “died” or “lived” so many times.\nOr imagine a dataset about students with an attribute called class that holds their graduation year, so that a value of 2025 means, “Class of 2025.” This is a numeric variable, but really its underlying purpose is categorical: every case gets put into one of (usually four) categories.\nCategorical variables that are really numeric. Sometimes, data are entered with text and numbers, but you really care only about the numbers. For example, in some Census data, if you ask for number of children, you get values such as, “0 children present” and “1 child present,” but then “2”, “3”, and so forth:\n\n\n\nCensus data. Notice the values of the Number_of_children attribute.\n\n\nOften, any text that’s not a number will fool CODAP into thinking it’s categorical. In the illustration, all it takes is the dollar signs to mess things up. The graph is not only categorical, it’s in the order of the table instead of numerical order!\n\n\n\nDollar signs can fool CODAP into thinking these numbers are categories."
  },
  {
    "objectID": "04.10-types-of-attributes.html#how-to-fix-it",
    "href": "04.10-types-of-attributes.html#how-to-fix-it",
    "title": "19  Types of Attributes",
    "section": "19.2 How to fix it",
    "text": "19.2 How to fix it\nThere are three ways to address this problem.\n\nAttribute properties box\nOne is to change the type in the attribute properties box. Click on the column heading to get the menu, then choose Edit Attribute Properties…. The box appears; choose a type in the type menu.\nYou can also change the name, specify units, give a description, tell how many decimal places you want, and restrict editing!\n\n\n\n\n\n\nUsing the type menu…\n\n\n\n\n\n\n\n…to change the money attribute to numeric.\n\n\n\n\n\n\n\nIn the graph\nThe axis labels on a graph are also menus. You can use the menus to change which attribute is displayed, but there’s more: down at the bottom, you can change the way the graph displays the attribute with the commands Treat as Categorical or Treat as Numeric. As an example, here is some Titanic data where 1 means “survived” and 0 means “died”:\n\n\n\n\n\n\nLeft: survived is treated as numeric\n\n\n\n\n\n\n\nRight: survived treated as categorical\n\n\n\n\n\n\n\nMaking a new attribute\nContinuing with the Titanic example, suppose we wanted to display the values as survived and perished instead of as 1 and 0.\nThen we could use the technique described in the section about recoding categorical attributes.\nTo recap,\n\nDrag survived to the left to make groups of each value.\nMake a new attribute next to survived (maybe call it didTheyLive or something suitable).\nType survived next to the 1 and perished next to the 0.\nSubstitute the new attribute for survived on a graph.\n\nIt will look like this:\n\n\n\n\nTitanic survival, with a new attribute giving values we understand."
  },
  {
    "objectID": "04.10-types-of-attributes.html#in-the-cracks-ordered-categoricals-discrete-numerics",
    "href": "04.10-types-of-attributes.html#in-the-cracks-ordered-categoricals-discrete-numerics",
    "title": "19  Types of Attributes",
    "section": "19.3 In the cracks: Ordered categoricals, discrete numerics",
    "text": "19.3 In the cracks: Ordered categoricals, discrete numerics\nSome sets of values, by their nature, share some properties of both categorical and numeric attributes.\nFor example, ages in Census data are always whole numbers. Even though people can be 20.75 years old, in the data you’re 20 until you turn 21. One consequence of this is that, although will generally group data (by dragging left in the table) with categorical attributes, and never with decimals, there are situations when whole-number numerics are perfectly good for grouping. That’s exactly what we did in one of our first lessons with age and height.\nAnd some categorical values, e.g., {frosh, soph, junior, senior} have a natural order. When CODAP has no clue about the order, it alphabetizes them. But you can change that by re-ordering the values on a graph. You might do that when exploring education in Census data."
  },
  {
    "objectID": "04.20-save-import-export-share.html#saving",
    "href": "04.20-save-import-export-share.html#saving",
    "title": "20  Files: Copy, Save, Import, Export, Share",
    "section": "20.1 Saving a CODAP file",
    "text": "20.1 Saving a CODAP file\nThe hamburger menu—three horizontal lines in the upper-left corner of the CODAP screen—has the usual things you would find in a File menu: Open, Save, and so forth.\n\n\n\n\n\nThe hamburger menu\n\n\nWhen you choose Save from the hamburger menu to save your CODAP file, you have two main choices: save it as a Google doc, or save it as a local file. Each has its pros and cons.\nCODAP’s interface to your Google drive is a bit unorthodox. As of early 2023, the navigation is rudimentary, and there is no search feature. If you are one of us who had trouble remembering where you put your Google docs, you will keep having trouble with Google docs saved by CODAP.\nThe interface for local files also looks unfamiliar. If you want to save locally, click Local file and then the download button, and that brings up the familiar Save widgetry, where you can name the file and decide what folder to put it in."
  },
  {
    "objectID": "04.20-save-import-export-share.html#sharing",
    "href": "04.20-save-import-export-share.html#sharing",
    "title": "20  Files: Copy, Save, Import, Export, Share",
    "section": "20.2 Sharing a CODAP file",
    "text": "20.2 Sharing a CODAP file\nThe Share… item in the hamburger menu has two sub-items: Get Link to Shared View and Update Shared View.\nWhen you first choose Get Link to Shared View you get a dialog that says “Shared view: disabled” in great big letters. Do not be deterred. A slightly smaller, blue button is marked “Enable sharing.” Press that.\nThe box changes to say, “Shared view: enabled.” It also presents several other controls and links.\nThe most important is the small link marked Copy. When you press that, the URL for your shared view is copied to the clipboard.\nThis link points to a snapshot of your document at this moment. Anyone who presses the link will get a copy of your CODAP document in this exact state.\n\n\n\n\n\n\nTip\n\n\n\nAs we suggested above, this is perfect for teachers preparing a lesson. Make your CODAP document exactly the way you want students to see it, then enable sharing and copy the link. Paste the link onto your class web page, and students will be transported to exactly the place you want to start.\n\n\nHere is a “gotcha”: you enable sharing for your CODAP document, you get the link, you publish it. But your graph, you realize, is in the wrong place. So you move the graph and save your document. Does the link change? Nope. The sharing link points to a snapshot of the document at the moment you enabled sharing.\nBut that’s what the other item in the Share… menu is for: Update Shared View. In our example, if you choose that after you move the graph, the link will now point to the adjusted CODAP document.\n\nThe relationship between sharing and saving\nSharing and saving are not the same thing. You can share a document without saving it. Will someone with the link get the right document if it isn’t saved? Yes.\n\n\n\n\n\n\nAnother gotcha:\n\n\n\nSuppose you’re a teacher preparing a lesson or a student preparing a presentation. You make the CODAP document you want and enable sharing. Then you post the link to the document on the web site or in a Google doc. All is well.\nBut the next morning—still before the class—you realize that one calculation is wrong. Fortunately, you have the link, so you go to it, open the shared document, and fix the calculation.\nYour problems are not over, however. You want to update the shared view, but the menu item is disabled. Why? Because the shared view you opened is an entirely new document, not the one you shared. Unless you actually saved that document—not just shared it—you will have to share it again, and re-paste the link into the web site.\nNot only that, but if you did save it, you (of course) have to make the fix in the saved document, not in the document you get from the sharing link."
  },
  {
    "objectID": "04.20-save-import-export-share.html#import-section",
    "href": "04.20-save-import-export-share.html#import-section",
    "title": "20  Files: Copy, Save, Import, Export, Share",
    "section": "20.3 Importing data",
    "text": "20.3 Importing data\nThe simplest way to import data is to drop a csv file right into your CODAP window. CODAP reads the file and makes a table out of it. Ideally, the first line of the csv has the names of attributes.\nIf there is a lot of data—more than 5000 cases—you will see a dialog like this one:\n\n\n\nDropping a csv with over 185,000 cases results in this dialog, saving you a long wait.\n\n\n\nUsing the Import command\nYou might also notice that there is an Import item in the hamburger menu. If you choose it, you’ll see that there are two paths: Local File and URL.\nLocal File: Drop in or navigate to a csv or a codap file. If it’s a csv, CODAP makes a new dataset with its own table. If it’s codap, the saved CODAP file replaces the one you’re working on.\nURL: Enter or drop in a URL. You can use this to put a web page into your CODAP document. Interestingly, you can also drop a URL into your document, with the same effect.\nBy the way, this is also how you can install a plugin. Drop the plugin’s URL into your document (or paste it into the URL box). read more about this in the plugins chapter.\n\nBut note: importing a URL will not magically make data on a web page appear in a table! For that you need techniques in this section, below.\n\n\nImporting from the clipboard\nIf you have the data in a text file or a spreadsheet, but it would be inconvenient to save it as a csv, try this:\n\nSelect the data you want and Copy it to the clipboard. (Be sure to include the attribute names in the first line if you can.)\nThen, if you want a new dataset:\n\nIn CODAP, click on the Table icon in the toolbar.\nChoose - - new from clipboard - - in the menu that appears.\n\nThis is exactly the same as making a csv and dragging it in. If there is a lot of data, you will see the dialog (pictured above) that lets you choose how much to import.\nOr, if you want to add data to an existing dataset:\n\nClick the “ruler” icon in the table of the dataset you want to add to.\nChoose Import Case Data from Clipboard\n\nYou will see a dialog asking if you want the clipboard to replace the data in the table or to append the clipboard data to an existing dataset.\n\n\n\nImporting data from a web page\nYou will often find data you want on a web page. How do you get it into CODAP?\nIt’s not always easy. For one thing, people put all kinds of non-data things into tables of data on their web pages. Or they may format them strangely. That is, you may have to munge the data before it’s ready for you to use. If things are a mess after you import them, check out this chapter on preparing and cleaning data.\nBut if everything is good, one strategy is to select the table on the web page, copy it, then paste it into a spreadsheet (such as Google sheets). From there you can make a csv file or just go to the Table tool to import the data into CODAP.\nYou can also paste it into a plain text editor, and make a csv (or tab-separated) file, which you can drop into CODAP.\nxxx ideas to come?"
  },
  {
    "objectID": "04.20-save-import-export-share.html#export-section",
    "href": "04.20-save-import-export-share.html#export-section",
    "title": "20  Files: Copy, Save, Import, Export, Share",
    "section": "20.4 Exporting data",
    "text": "20.4 Exporting data\nYou can also export a csv file from CODAP. Click the ruler palette in the table. You’ll find the command there. If you have made your table hierarchical, you can choose which level to export— or you can export them all.\nThis is a good way to share data with somebody who isn’t using CODAP. Also, sometimes you will want to take your work from CODAP to some other program, in order to do something CODAP can’t do."
  },
  {
    "objectID": "04.30-screen-space.html#shrink-your-table",
    "href": "04.30-screen-space.html#shrink-your-table",
    "title": "21  Screen Space",
    "section": "21.1 Shrink your table",
    "text": "21.1 Shrink your table\nWe want to see all the data at once, and for some reason, we seem to think that making the table as large as possible is the way to do it. There are two things wrong with this: first, in a data science context, there’s usually way too much data to see this way; and second, looking at a table is (usually) a terrible way to see all the data. That’s why we have graphs.\nSo the first soution to your lack of screen space is to shrink your table.\nThe problem with that is that you still need to see the attribute names—the column headings—in order to drag them to graphs.\nTherefore, make your tables wide but short. You probably need to see only a row or two of the data in order to remember what kind of values are in which columns.\n\n\n\n\n\n\nLeft: a big table.\n\n\n\n\n\n\n\nRight: a short table—there’s room for work!"
  },
  {
    "objectID": "04.30-screen-space.html#case-card-view-section",
    "href": "04.30-screen-space.html#case-card-view-section",
    "title": "21  Screen Space",
    "section": "21.2 Case card view",
    "text": "21.2 Case card view\nIf you have a lot of attributes, even if your table is wide, they might not fit on the screen. That means you’re constantly scrolling back and forth to get to the attributes you want.\nYou might need case card view. It’s an alternative to the table that shows you the values for one case, but oriented vertically instead of horizotally. Often, this takes up much less space than the table, and shows you more attributes.\n\n\n\nUsing the case card view: even more room for work.\n\n\nTo get case card view,\n\nclick the little icon in the table’s upper left corner. Try it in the live illustration below.\nThen change the analysis. For example, drag Marital and plop it in the Age graph (replacing N_Married) to see how marital status is related to age."
  },
  {
    "objectID": "04.30-screen-space.html#minimizing-and-renaming-components",
    "href": "04.30-screen-space.html#minimizing-and-renaming-components",
    "title": "21  Screen Space",
    "section": "21.3 Minimizing and renaming components",
    "text": "21.3 Minimizing and renaming components\nIn the upper right of every component—table, graph, text object, map, whatever—you’ll see an X and a big minus sign. The X lets you delete the component, of course. The minus sign lets you minimize it, that is, shrink it so only its title bar shows. Click the minus sign again to make it full size.\nTry that on the components in the illustration in the previous section.\nOf course, if you have a bunch of minimized things you can’t tell them apart…or can you?\nClick once on the title of a component to select the title; then you can edit it to be anything you want. This can be a good practice, especially if you are preparing graphs for a presentation. You can name your graph something like “how education relates to employment” instead of “people.”"
  },
  {
    "objectID": "04.30-screen-space.html#the-tiles-menu",
    "href": "04.30-screen-space.html#the-tiles-menu",
    "title": "21  Screen Space",
    "section": "21.4 The tiles menu",
    "text": "21.4 The tiles menu\nSuppose you have a lot of graphs, and you can’t find the one you want. Even if you’ve given them names, it can be hard. Maybe they’re overlapping, or even scrolled off the screen.\nNo problem! Use the little-used Tiles menu near the upper right of the screen. It shows you a list of all of your components. Not only that, but all you have to do is hover over a title, and the component pops to the front so you can see what it is\n\n\n\nThe tiles menu shows all of your components."
  },
  {
    "objectID": "04.30-screen-space.html#using-choosy",
    "href": "04.30-screen-space.html#using-choosy",
    "title": "21  Screen Space",
    "section": "21.5 Using choosy",
    "text": "21.5 Using choosy\nChoosy is a very sweet tool that you can find in the Plugins menu.\nIt does a number of things, but the one that’s important here is that it makes it easy to hide attributes. You know how you can use hide or set aside to hide rows in a table? This lets you hide columns, which is a lifesaves if your dataset has lots of attributes.\nFor the whole story of choosy, see xxx"
  },
  {
    "objectID": "04.40-graphing-tips.html#palettes-section",
    "href": "04.40-graphing-tips.html#palettes-section",
    "title": "22  Graphing Tips",
    "section": "22.1 Palettes",
    "text": "22.1 Palettes\nIf you click on a graph, a blue-green panel appears on its right with some icons on it. Clicking on one of those reveals a menu-like palette of additional tools."
  },
  {
    "objectID": "04.50-dates-and-times.html#data-that-codap-automatically-understands",
    "href": "04.50-dates-and-times.html#data-that-codap-automatically-understands",
    "title": "19  Dates and Times",
    "section": "19.1 Data that CODAP automatically understands",
    "text": "19.1 Data that CODAP automatically understands\nIf your data is formatted in a way that CODAP understands, of you’re using a CODAP plugin, and the programmer was nice to you, using dates and times is easier.\nIn that case, you will typically put that date or time attribute on the horizontal axis of a graph and something else of the vertical. The date axis will behave just like any numerical axis except that it will be marked in years or months or hours or minutes, as appropriate.\nThe live illustration below has the S&P 500 index from 2006 to 2009—so it includes the massive crash in 2008.\nMake a graph of the closing prices:\n\nMake a new graph.\nPut date on the horizontal axis.\nPut close on the vertical axis"
  },
  {
    "objectID": "04.50-dates-and-times.html#an-example-using-less-well-behaved-data",
    "href": "04.50-dates-and-times.html#an-example-using-less-well-behaved-data",
    "title": "19  Dates and Times",
    "section": "19.2 An example using less well-behaved data",
    "text": "19.2 An example using less well-behaved data\nLet’s do another example. The illustration shows part of a Wikipedia entry about earthquakes in 2019.\n\n\n\n\n10 biggest earthquakes in 2019 (Wikipedia)\n\n\n\nWe have entered the dates and magnitudes in the live illustration below. We’ll pretend that different people entered the dates. Most of them used an increasingly-standard format, yyyy-mm-dd, for example, 1776-07-04 for the signing of the Declaration of Independence. Others used other formats. What could possibly go wrong?\n\nIn the live illustration below, make a graph of date. Can you tell that CODAP is treating this attribute as categorical? For one thing, the dates are not in order.\nThe problem is in the second entry. Double-click it and edit that date, May 14 2019, to include a comma after the 14 so it’s May 14, 2019. Notice that the graph does not change. Did your edit work?\nMake a new graph and put date on the axis. Aha! Now we have a genuine “date” axis with months and everything.\n\n\n\n\nThat date-like axis is your cue that CODAP really, truly recognizes the data as a date.\nYour takeaway should be that CODAP will accept several different formats, but not all the formats that make sense. Be especially careful if you’re not in the USA or are using data from across the pond: does 7/5/2024 mean July 5 or May 7? In the USA it’s July 5. To be sure you get what you want, do not rely solely on documentaion. Things change. Try it."
  },
  {
    "objectID": "04.50-dates-and-times.html#using-other-tools-to-massage-the-data",
    "href": "04.50-dates-and-times.html#using-other-tools-to-massage-the-data",
    "title": "23  Dates and Times",
    "section": "23.3 Using other tools to massage the data",
    "text": "23.3 Using other tools to massage the data\nThe previous example worked fine; we made an edit and everything was OK. But in our work we’ll often have hundreds or thousands of data points. The truth is that the Big Dogs (e.g., Google) have lots more programming power that Concord Consortium, who make CODAP. Sometimes you may need to turn to your trusty spreadsheet to wrangle your data.\nGoogle Sheets understands many more date formats that CODAP. If you put your data, temporarily, in a Google Sheet, you can update its format there and then re-import it into CODAP. Select the whole column of dates, then go to the “123” format menu, and choose Date. The column will re-format to be consistent.\n\n\n\n\n\n\nBefore (in Sheets)\n\n\n\n\n \n\n\n\n\n\nthe menu\n\n\n\n\n \n\n\n\n\n\nAfter (in Sheets)\n\n\n\n\n\nOnce it is correct in Sheets, you can simply copy the data, switch to CODAP, and use the Table menu to import the data."
  },
  {
    "objectID": "04.50-dates-and-times.html#another-example-lunar-eclipses",
    "href": "04.50-dates-and-times.html#another-example-lunar-eclipses",
    "title": "19  Dates and Times",
    "section": "19.4 Another example: lunar eclipses",
    "text": "19.4 Another example: lunar eclipses\nThis example shows how much we can rely on Google for date formatting.\nThe live example below has data—including dates and times—for 36 lunar eclipses from 2020 to 2036. The first two attributes, day and time are from the original source (Wikipedia); so are some other attributes for exploring: type, node (Ascending or Descending), and saros1\nThe other two attributes, day_formatted and date_with_time were created using Google sheets:\n\nday_formatted is exactly like day but formatted (in Sheets) as a date.\ndate_with_time is constructed simply by adding time to day, that is, its formula—in Sheets— is something like = A2 + B2.\n\n\nMake a graph with day on the horizontal axis. It looks fine until you look more closely and see that, for example, August 2026 comes before March 2026. What gives? CODAP thinks the values are categorical, and listed them alphabetically.\nOK, CODAP: use Edit Attribute Properties… and change day to be a date. You’ll see quotes appear around the values in the table.\nMake a new graph with day. Yikes! We get a “date” axis but no points! CODAP did not recognize strings like \"2033 Oct 08\" as dates.\nPretend you went to Sheets and came back with the other columns. Replace day on the graph with day_formatted. Now it looks like a regular graph!\nExplore the other attributes such as saros by plotting them against a date. Be sure to try them using the original day as well, so you can see how easy it would be to get confused."
  },
  {
    "objectID": "04.50-dates-and-times.html#date-and-time-functions",
    "href": "04.50-dates-and-times.html#date-and-time-functions",
    "title": "19  Dates and Times",
    "section": "19.5 Date and Time functions",
    "text": "19.5 Date and Time functions\nSometimes you have a date or a time or a date/time, but you don’t want the whole thing. Maybe you have a bunch of birthdays and you want to see the distribution of months or days-of-the-week. (Are fewer children really born on weekends? Yes.) If you have a well-formatted date field, you can extract part of the date using a formula.\nThe amazingly cool data in the next example is a small part of a huge dataset from BART. In contrast to the (we hope familiar) BART data portal, this does not tell you ridership by hour—it gives information on individual riders. There are so many cases in this data that we have extracted data for only 30 minutes: 8:00 to 8:30 AM, on September 30, 2015. And only for passengers exiting at the Downtown Berkeley station. Even so, that’s over 900 cases.\n\nMake a nice, wide graph with time on the horizontal axis. Position it below the table where there’s room.\n\nThe attribute time is to the nearest second. Suppose we wanted just the minute on the horizontal axis, not the entire date and time.\n\n\n\n\nMake a new attribute; call it minute.\nOpen the formula editor for minute.\nYou have forgotten the name of the function you want. Press the - - - Insert Function - - - button.\nChoose Date/Time Functions.\nNotice all the functions! Before you choose minutes(dateTime), press the info button on the right to see what the function will do.\nPress minutes(dateTime) in the list to insert into the formula editor without having to type it.\nFinish editing the function: edit it to be minutes(time). Press Apply to make the function work.\nTest it! Replace time on the axis with minute. Ta-daa!\n\nIt’s worth shopping around in that Date/Time Functions section of the formula editor. You will find all kinds of useful functions. For example, date(year, month, day, ...) will construct a CODAP date/time value if you give it the year, month, day, and whatever else you want. And you don’t have to fill everything in."
  },
  {
    "objectID": "04.60-maps.html#maps-points-section",
    "href": "04.60-maps.html#maps-points-section",
    "title": "24  Maps",
    "section": "24.1 Locations: Maps that show points",
    "text": "24.1 Locations: Maps that show points\nIf your dataset has locations in latitude and longitude, CODAP will make a map where each location shows up as a point. To make this work, the location attributes must be named latitude or lat; and longitude, long, or lon. These are all in decimal degrees. If you have degrees, minutes, and seconds, you have to convert. This can be an interesting data-munging task.\nIn the live illustration below, each case is a building in New York City. Notice that it includes lat and lon.\n\nMake a map simply by clicking the Map tool. Notice how it zooms automatically to enclose the points.\nDrag Height into the map to color the points.\nChange the point size and color in the layers palette on the right edge of the map.\nTry selecting a building in the table, then zooming in to the map to see its location.\n\n\n\n\nNot all location maps are so urban! The next example has data from Movebank.org, an animal-tracking data repository. It shows the motion of a grey wolf in Alberta, Canada over a couple of months. Animal tracks like these are therefore time series, that is, they have latitude and longitude–and time.\nHere are some things to try:\n\nMake a graph with time on the horizontal axis. It’s just a dot plot, but it will show you when the data are. The points are not evenly spaced.\nSelect points in the graph to see where the wolf was at that time.\nUse the ruler palette to connect the points. Consider in what situations this is useful.\nSelect a day or two of points when the data are dense. Hide the unselected points; zoom in, restore the points, rescale the map to zoom back out.\nUse the temperature and altitude attributes in the map and/or in the graph.\nUse the greatCircleDistance function (it’s in “other” functions) to calculate the distance between points.\nSubtract prev(time) from time to get the time between points in seconds. Use that with the distance to compute the wolf’s average speed during the interval.\nThink about the stories the data tell about this wolf."
  },
  {
    "objectID": "04.60-maps.html#maps-areas-section",
    "href": "04.60-maps.html#maps-areas-section",
    "title": "24  Maps",
    "section": "24.2 Areas: Maps that show regions",
    "text": "24.2 Areas: Maps that show regions\nMaps do more than show dots at specific locations. We also use maps to show data “by area.” The live illustration has a dataset with information on land use in the lower 48 States.\n\nClick the Map tool to make a map. The States appear.\nDrag attributes into the map to color the states.\nNotice the %crop attribute, which is computed. Try that one! What do you see?\nMake other computed attributes as you see fit."
  },
  {
    "objectID": "04.60-maps.html#setting-up-boundaries-1-section",
    "href": "04.60-maps.html#setting-up-boundaries-1-section",
    "title": "24  Maps",
    "section": "24.3 Setting up Boundaries for CODAP Maps: the simple case",
    "text": "24.3 Setting up Boundaries for CODAP Maps: the simple case\nAreas work great as long as the dataset has been set up with the appropriate boundaries. As an example, we have prepared a dataset with median home prices by State from March 2023.\nIf you have a dataset with a column that has the names or IDs of some common boundary type—let’s say US States, as in our example—and you drag an attribute into the map, nothing happens.\n\nIn the live demo below, try dropping Avg. Median Sale Price into the map. It doesn’t work!\n\nThe problem is that our document does not have the boundaries in it. Here is how to fix that:\n\nMake a new attribute and name it boundaries.\nOpen up the formula editor for your new attribute.\n\n\n\n\n\nYou need a special function. Under —Insert Function–, choose the Lookup functions section.\nChoose lookupBoundary. You will see this:\n\n\n\n\nafter choosing lookupBoundary\n\n\nAs you can see, there are two arguments: boundary_set and boundary_key. For boundary_set, you need a special name, which you may not remember.\n\nSelect boundary_set and open the - - -Insert Value- - - menu. The lower part of that list has special names such as \\(\\pi\\)—and also the various boundary sets that are currently installed. For our example, choose US_state_boundaries. (You can type it in if you like; CODAP will help with typeahead.)\nFinally, replace boundary_key with the name of the attribute that has the “key,” in this case, the one that has the names of states in it. In our case, this is Region.\n\nA this point, the map might show some color—but not show the entire US! If that’s the case, press the rescale button  on the Map.\nNow, if we drag Avg. Median Sale Price into the map, we should see a map colored by sale price, like this:\n\n\n\nMedian home prices in March 2023. Notice that North Dakota and Wyoming are missing.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe State attribute does not have to have the full name of the State in it. Attributes with the standard two-letter postal abbreviation work too."
  },
  {
    "objectID": "04.60-maps.html#setting-up-boundaries-2-section",
    "href": "04.60-maps.html#setting-up-boundaries-2-section",
    "title": "24  Maps",
    "section": "24.4 Setting up Boundaries from Scratch",
    "text": "24.4 Setting up Boundaries from Scratch\nIf you want to use a set of boundaries that are not listed in CODAP’s “boundary sets,” you have more work to do.\nThe boundaries CODAP uses are in a file format called “geoJSON.” That’s the magic word you use when searching for the boundaries you need. Let’s do an example.\nRight now, as I write this section, it’s November 2020, and Georgia is really important in the Presidential race. We want to make a map that shows, by county, who is ahead (we see these all the time on TV and on news sites).\nSo we search for “Georgia county geoJSON” and find a file to download; this one is called GA-13-georgia-counties.json. It begins like this:\n{\"type\":\"Topology\",\n    \"transform\":\n    {\n        \"scale\":[0.00176728378633945,0.0012459509163533049],\n        \"translate\":[-85.60593932520246,30.360363878941108]},\n        \"arcs\":[[[105,2986],[92,16],[120,-4]],[[317,2998],[-1,-11],\n            [71,0]],[[387,2987],[-32,-105],[-40,1],[0,-39]],[[315,2844],\nand goes on for quite a while. What are all those numbers in arcs? The boundaries of the counties. So there are thousands of numbers representing points on the map.\nWe also look for vote tallies; we find them in an HTML table and process that (see the chapter on munging) to get a file called Georgia-votes-bycounty.csv. It begins like this:\nCOUNTY,BIDEN,TRUMP\nAppling,1779,6526\nAtkinson,825,2300\nBacon,625,4018\nBaker,652,897\nBaldwin,9140,8903\nand continues with one case per county.\nThen (you don’t have to do this right now; we did it for the example that’s coming up),\n\nDrag the boundaries file (ending in .json) into CODAP. You will see a map of Georgia, or whatever you found a map of. You will also see a table of the objects whose boundaries appear; in our case, these are Georgia counties. Notice that the table has their names in the NAME attribute.\nDrag the vote tallies file into CODAP. This creates a second, more straightforward table, with the votes and the county names.\n\nAt this point you will see something like the document in the live illustration below. We have renamed the datasets geography and votes to clarify things. Notice the boundary attribute at the far right of the geography table. Those are tiny representations of the boundary shapes the map needs to work properly.\nWe have two tables; we need all the data in one table: the votes table. This is a job for a joining data move!\n\nDrag the NAME attribute from geography and drop it onto the COUNTY attribute in votes. (Both columns contain the same thing: the names of the counties.)\n\nNow votes has the boundary attribute. We no longer need the geography dataset, but we can’t just delete it without some preparation.\n\nScroll right to find the boundary attribute in the votes table.\nClick on the boundary column header in the votes table. A menu appears. Choose Delete Formula (Keeping Values).\nIn the Tables tool, delete the geography dataset. You may now delete, hide, or get rid of the formulas of any other geographical attributes in the votes table, as you see fit.\n\n\n\n\n\nIf you don’t have one already, make a map. You’ll see the Georgia counties!\nNow drag a data attribute (such as Biden) into the map. Yay! We get a colored map!\n\n\n\n\n\n\n\nWhy delete geography?\n\n\n\nYou can make a map without doing the steps about un-linking the boundary attribute and deleting the geography dataset. However, the map will be a little confused because it’s not clear which dataset is providing the data.\nI have seen the colors be a little weird until I got rid of the second table.\n\n\n\nNow you can set yourself a task, such as making a CNN-worthy map of Georgia where Trump counties are red and Biden couties are blue, and the depth of the color reflects the size of the lead.\n\n\n\n\n\n\n\nTeachers, note\n\n\n\nThis is not just a mechanical exercise! The problem of what constitutes a big lead is an interesting one; what formula should you use to decide? Should it be by percentage or by absolute vote?"
  },
  {
    "objectID": "04.60-maps.html#a-small-gotcha-about-boundaries",
    "href": "04.60-maps.html#a-small-gotcha-about-boundaries",
    "title": "24  Maps",
    "section": "24.5 A small gotcha about boundaries",
    "text": "24.5 A small gotcha about boundaries\nWeeks or months from now, you’ll be making a map with boundaries and forget the details. One of them is that you need a boundaries attribute in the table. Another is exactly what formula to use for the boundary. It will be something like, lookupBoundary(US_state_boundaries, State).\nAnd then a thing we haven’t talked about: the name of the boundaries attribute. If you call it boundaries, it’s fine. But if you call it something else, the map won’t work…unless you “type” the attribute as a boundary.\nSuppose you called the attribute border by mistake. Do this:\n\nGo to the attribute’s menu and choose Edit Attribte Properties…. A dialog appears.\nUnder type, choose boundary.\nDelete the bad map and make a new one. Your boundaries should appear."
  },
  {
    "objectID": "04.70-adjustment-and-control.html",
    "href": "04.70-adjustment-and-control.html",
    "title": "21  citation-location: margin",
    "section": "",
    "text": "22 Adjustment and Control\nIn this chapter we discuss adjusting data for trends or baselines, and relate that to “controlling” for variables.\nThis is inspired by the problem of, for example, using BART data to assess how many people took BART to a game on Wednesday. The naïve student would sum the number of riders over a period of a few hours. That’s good, but you should subtract the number of people who would be taking BART at that time anyway. And how do you determine that? Some ideas:\nAdjustment will address ideas such as “adjusted for inflation” and “seasonally adjusted”—means of making fair comparisons when underlying trends change the data values.\nAs you work thoruhgh these examples, pay attention to what data moves are required. (I will call them out from time to time.)"
  },
  {
    "objectID": "04.80-plugin-logistics.html#but-first",
    "href": "04.80-plugin-logistics.html#but-first",
    "title": "22  Plugin Logistics",
    "section": "22.1 But first…",
    "text": "22.1 But first…\nSome basics.\nA plugin occupies a rectangular area in the CODAP window. In that way it’s like a table or a graph."
  },
  {
    "objectID": "04.80-plugin-logistics.html#method-1-drag-and-drop",
    "href": "04.80-plugin-logistics.html#method-1-drag-and-drop",
    "title": "22  Plugin Logistics",
    "section": "22.2 Method 1: Drag and Drop",
    "text": "22.2 Method 1: Drag and Drop"
  },
  {
    "objectID": "04.80-plugin-logistics.html#method-2-import",
    "href": "04.80-plugin-logistics.html#method-2-import",
    "title": "22  Plugin Logistics",
    "section": "22.3 Method 2: Import",
    "text": "22.3 Method 2: Import"
  },
  {
    "objectID": "04.80-plugin-logistics.html#method-3-use-the-url",
    "href": "04.80-plugin-logistics.html#method-3-use-the-url",
    "title": "22  Plugin Logistics",
    "section": "22.4 Method 3: Use the URL",
    "text": "22.4 Method 3: Use the URL"
  },
  {
    "objectID": "06-projects-part.html",
    "href": "06-projects-part.html",
    "title": "Small Projects",
    "section": "",
    "text": "In this section, we’ll see some larger, but still small, more open-ended assignments I have used with high-school students."
  },
  {
    "objectID": "06.50-world-bank-data.html#background",
    "href": "06.50-world-bank-data.html#background",
    "title": "27  World Bank Data",
    "section": "27.1 Background",
    "text": "27.1 Background\nOn their home page, you can read their mission: “The World Bank Group has two goals, to end extreme poverty and promote shared prosperity in a sustainable way.” In support of that, they set goals, promote policies, and collect data.\nOne of their products involves what they call “Sustainable Development Goals,” which are quite exciting and ambitious. To measure progress towards these goals, they collect data You (and students) can explore these goals"
  },
  {
    "objectID": "07.10-binomial.html#the-aunt-belinda-problem",
    "href": "07.10-binomial.html#the-aunt-belinda-problem",
    "title": "24  Probability and Binomial Simulator",
    "section": "24.1 The Aunt Belinda Problem",
    "text": "24.1 The Aunt Belinda Problem\nFor me, Aunt Belinda is one of those prototypical problems that you can refer back to time and again. As you will see, it’s really also an inference problem; it’s structured just like a significance test, but it’s designed to be understandable as a first example of the genre. Here we go!\nYour Aunt Belinda claims to have magical powers. She says she can make nickels come up heads when she flips them. You ask for a demo, and give her 20 nickels. She mumbles some magical-sounding words over the nickels and flings them into the air. You scurry around to pick them up and find that of the 20 nickels, 16 came up heads.\nIs it magic?\nWe’re not sure, but let’s make the question a little different:\nIf Aunt Belinda had no powers, how often would we get 16 heads in 20 flips?\nOf course, in the classroom, you do this with real coins and collect data from the students. You get maybe fifty trials and see that 16 heads is at least rare. But of course you want more data, so it becomes a job for a computer.\nIn the live illustration or separate link below, we have set up the simulator correctly for this problem. Notice:\n\nUnder Settings you have all the numbers for our investigation:\n\nThe probability of heads is 1/2. You can put fractions, decimals, or percentages in this box.\nThe number of trials, “20,” is listed under Settings as “flips per experiment.” That’s the number of coins Aunt Belina flips.\nThe number of experiments—200 in this case—is arbitrary but should be large.\n\nUnder Vocabulary is the cool stuff: the words we would use to decsribe the different parts of the context. In this case, they’re all pretty clear except for maybe “experiment,” which could just as easily be “trial” or “set” or “run.”\n\nTry this:\n\nClick Engage to do the 20-flip experiment 200 times. A table appears.\nGraph heads to see how many heads came up during each set of 20 tosses.\nObserve how many times out of 200 you got 16 (or more) heads. Not very many!\n\n\n\n\n\nThen, with the class, you have to ask the payoff question:\n\nWhat does this result tell us about Aunt Belinda?\n\nAccept all answers. Maybe she cheated. Maybe the coins were weighted. Maybe she really has magical powers. But guide the class to the payoff answer: either she was lucky, or something other than chance was working here.\nIf you want to go further,\n\nTry to find the probability that you get 16 or more heads in 20 tosses of a fair coin. Here, I’m not advocating going into combinatorics, but rather finding a better empirical probabilty. To do that, you need more cases; simply increase that 200 number and/or press Engage more times. This could lead to a summarizing data move, creating a formula in some column that gets moved to the left.\nHelp students clarify why we’re interested in the probabilty of 16 or more in this Aunt Belinda context, instead of just the probability of 16.\n\n\n24.1.1 Another way to make CODAP calculate for you\nDon’t forget that CODAP has all kinds of useful tools in the graph. If you aren’t ready to make a summary column and drag it left, just do this:\n\nGo to the eyeball palette and check Show measures for selection.\nIn the graph, go to the “ruler” palette and set it to report Count and Percent.\n\nThen, when you select the extreme points, you can see how many there are and what percent they represent."
  },
  {
    "objectID": "07.10-binomial.html#exercises",
    "href": "07.10-binomial.html#exercises",
    "title": "24  Probability and Binomial Simulator",
    "section": "24.2 Exercises!",
    "text": "24.2 Exercises!\n\n\nYou have about a one in five chance of making a great artwork in a day. After five days, what is the chance that you have produced at least one great work?\n(Hints: Start in the vocabulary. The basic event in this case is a day. The results are great art or not so great. Then it says, what do you call a set of 10 days? Well, in this problem, it’s 5, not 10, right? Call it a week and change the 10 to 5. Now the description text says you’ll be making 100 weeks. That’s fine; increase it if you want. This is a fantasy: if I were to do 100 weeks, how much great art would I produce during each of those weeks? That’s what you’ll simulate in one run. Try it. Don’t forget to set the probability!)\nYour plants grow 1 cm on sunny days and not at all on cloudy days. 5/6 of all days are sunny. How tall will the plants be after 30 days? What is a reasonable range for your prediction? (Again, the basic event is a day.)\nAloysius is taking a 10-item true-false test about which he knows nothing. What’s the chance that he will get a grade of 70%? (Or higher) (Basic event, a question.)\nHow much worse are his chances if it is a 20-item test?\n100 people each play 50 games of roulette, betting one chip on red every time. How many of them are winners? What is the most that a player has won? How much did the house win altogether from these people? (you might call 50 games a session)\nSuppose Grunt and Flaky, two Senate candidates, have equal numbers of supporters. You do a poll of 100 voters.\n\nWhat’s the chance that the poll shows one of them having 54% or more of the voters?\nRedo the problem with 1000 voters in each poll. Now what’s the chance of 54% or more?\n\nInvent an entirely new binomial problem and solve it!"
  },
  {
    "objectID": "07.10-binomial.html#teacher-notes-on-the-exercises",
    "href": "07.10-binomial.html#teacher-notes-on-the-exercises",
    "title": "24  Probability and Binomial Simulator",
    "section": "24.3 Teacher notes on the exercises",
    "text": "24.3 Teacher notes on the exercises\nThese problems should give you plenty of ideas for additional problems you—or your students—could write.\nThey also give you and your students ample opportunity to address other important issues that come up in stats and probability settings. Here are some:\nBinomial problems let you discuss variability. Although there may be a clear expected number of “successes,” every trial can be different. For example, we expect the plants to grow 25 cm.\nBut do they? Sometimes they do, sure, but they usually grow some other amount.\nThen there are issues around sample size. We’ve already mentioned how you need more trials to get a good value for the probability of 16 or more heads. But there’s another issue. Looking at Aloysius and his 20-item test, we see the chance of getting 70% or more goes down in comparison to his 10-item test. That’s because there’s less variability in the results (expressed as a percentage) from larger samples: his grade from random guessing will tend to be closer to 50% on 20 items than 10.\nThis exposes a point of confusion: in Belinda, we increased the number of experiments (the third Settings box) to get a more precise probability and increase the sample size. In Aloysius, we increased the number of questions in the test, and that number is in a different box.\nSo, we ask students, what is the difference between the numbers in those two boxes? Ultimately, the answer is (mostly) that the last box is all about precision, getting enough data points—and every “experiment” or “test” is a data point— to know the probability of some condition (heads ≥ 16, score ≥ 70%) more precisely.\nThe second box, on the other hand, is part of the context itself.\n\n\n\n\n\n\nEmpirical vs theoretical\n\n\n\nI like doing these problems empirically, at least at first. The theoretical approach, with combinations and permutations and factorials, needlessly snows the students. This approach is more humane and understandable. But it is not without cost, and you pay it right here: in the theoretical approach, you never have to decide how many times to try it.\n\n\nHaving said that, what about the Roulette problem? First, of course, someone has to learn about Roulette and figure out that (for standard US Roulette tables anyway) the probability of winning when you bet on red is 18/38, and that the payout is 1:1.\nBut then there’s the issue of what the “100” is doing in this problem. Here it naturally fits in the third Settings box, despite our saying that the number there is arbitrary, just good to be large.\nSure. Except the problem asks how many people out of 100 make money. So if you run the simulation once you get one answer. If you run it again, your answer may be different. So you could run it many times, and learn the distribution of the number of winners, the maximum winnings, and the house’s take.\nThat is, you can take the simulation another level up. It’s not just a set of 50 spins of the wheel, done 100 times, but rather {50 spins done 100 times} — done many times.\n\n24.3.1 Grunt and Flaky: the Bayesian thing\nNote that you have to describe the Grunt and Flaky problem carefully. The question we really want to ask is:\n\nSuppose, in our poll of 100 voters, Grunt gets 54%. What’s the probabilty that he is actually ahead?\n\nNotice that our question, “Suppose they’re tied, what’s the chance that Grunt appears to get 54%?” is a different question, and not really what we want to know.\nThe thing we really want to know is a Bayesian problem, not part of the traditional intro stats course."
  },
  {
    "objectID": "07.10-binomial.html#exploring-the-effect-of-sample-size",
    "href": "07.10-binomial.html#exploring-the-effect-of-sample-size",
    "title": "24  Probability and Binomial Simulator",
    "section": "24.4 Exploring the effect of sample size",
    "text": "24.4 Exploring the effect of sample size\nPerhaps after you’ve done other things for a while, return to this binomial simulator and revisit the Aloysius problems above.\n\n\n\n\n\n\nWhat’s next…\n\n\n\n\nMore to come, along the lines of the corresponding demo in Fifty Fathoms Show how the spread of the number of right answers increases with the number of questions in the test, but the spread of the precentage of right answers goes down.\n\nThen, see that it doubles (or halves) and the number of questions quadruples."
  },
  {
    "objectID": "07-stats-part.html#whats-in-stats-101",
    "href": "07-stats-part.html#whats-in-stats-101",
    "title": "Statistics, Data Science, and CODAP",
    "section": "What’s in Stats 101?",
    "text": "What’s in Stats 101?\nFor the purposes of this section, when I say “statistics” I will mean “the contents of the introductory statistics course.” That’s an important distinction because many Real Life Statisticians actually do data science in their everyday work: they use computational tools and deep thinking abut data to find the stories hidden in vast amounts of data.\nBut you are probably either a student in an introductory course—in data science or statistics—or a teacher in such a course. Or you may even be associated with a computer science course such as APCSP. In any of these cases, the contents of the course may be more relevant for this discussion.\nSo: what’s in Stats 101?\nStereotypically, it starts with “descriptive statistics” and then moves on to “inferential statistics.” It also usually includes some “modeling.” Let’s look at these elements.\nModeling can mean many different things in different contexts. A mathematical model is typically a simplification of reality. Reality is complex and messy, but if we make a good model, we can learn about the simplified situation and apply that understanding to reality, for example, by making predictions. These predictions won’t be perfect, but they can be pretty good.\nBut how do we make a good model? In a stats class, one task might be to look at points on a scatter plot and make a linear model, that is, find a line that approximates the data as well as possible.\nThere are various techniques for doing this; a famous one is least-squares regression, which gives you the (deceptively-named) “line of best fit.”\nDescriptive statistics includes making a variety of graphs that show one or two variables, and making summary calculations. These summary calculations tend to be measures of center such as the mean and the median, and measures of spread such as the standard deviation and the interquartile range. The summaries also include proportions such as the percent of light bulbs that are defective.1\nThese numbers lead to getting more comfortable with distributions of data. Students learn to identify and reason about the shape, center, and spread of distributions, and to identify and cope with outliers.\nThe overall idea is to be able to make graphs and calculations in order to draw conclusions about data. Students use graphs and summary values to address claims and answer questions about the situation that gave rise to the dataset.\nAnd if that sounds kind of like what we’ve been doing all along, you’re right. Data science has a lot in common with descriptive statistics, but with bigger datasets and, importantly, more variables. The sheer size of modern datasets leads to that “awash” sense and the need for more computation (and data moves) in order to cope.\nInferential Statistics is different. You can think of it in many ways; here are two:\n\nYou use it to draw conclusions about a population from a (sometimes small) sample.\nYou use it to decide if some difference you observed is “real,” that is, whether it might have arisen by chance.\n\nLet’s illustrate the population/sample situation with a prototypical task: polling.\nTwo candidates, Grunt and Flaky, are running for Senate. You do a poll of 100 people and 55 support Grunt. In fact, 5,000,000 people will be voting. Will Grunt win?\nThe obvious answer is, Grunt will get 55% of five million votes; we expect Grunt to win 55 to 45.\nBut will Grunt get exactly 55%? Of course not! The 100 people we asked were only a sample. Even the very best sample—a random one—will not precisely mirror the population (the five million voters). Randomness will throw it off. This sampling variation could result in our getting “too many” or “too few” Grunt voters in our sample.\nSo the question is, how confident are we that, having gotten 55 out of 100 randomly-chosen voters, Grunt will get more than 2,500,000 in the election?\nNow let’s consider another situation. The illustration shows the height distributions of boys and girls aged 11 and 17.\n\n\n\nGender differences in height. Left: age 11; right: age 17.\n\n\nIt’s really obvious that at age 17, the boys are taller. More precisely, it’s obvious that the mean height of the boys is greater—individual boys may be quite short, and individual girls can be taller than most boys.\nBut at age 11, it’s not as clear. For this set of data, the girls have a higher mean. But is that just because of the particular girls we have data for, or is this a “real” difference?\nIn all of the situations we’ve just looked at—the polling and the two height comparisons—the underlying question is really, could the difference we observe “plausibly” arise by chance?\nIn the case of the 17-year-olds, the answer is no. For the 11-year-olds, it seems plausible: maybe there’s really no (mean) height difference between the boys and the girls. For the Grunt/Flaky poll, it’s not obvious either way. It looks good for Grunt, but with a bigger poll, we might see Flaky pull ahead.\nMost of the second half of an introductory statistics course is all about questions like these. You learn a number of techniques for addressing these issues. And all of them are related to probability."
  },
  {
    "objectID": "07-stats-part.html#what-part-of-that-is-important-for-introductory-data-science",
    "href": "07-stats-part.html#what-part-of-that-is-important-for-introductory-data-science",
    "title": "Statistics, Data Science, and CODAP",
    "section": "What part of that is important for introductory data science?",
    "text": "What part of that is important for introductory data science?\nLet me re-issue this caveat: This is the opinion of a single author, and is subject to a lot of rethinking and revision.\nBut here are three “stats” things I think would be worth covering in an introductory data science course.\n\nSample size and stability\nSummary values (a.k.a measures, a.k.a. statistics) are more stable the larger the sample.\nIn the polling example, we might wonder, “Suppose I had taken a different sample of 100 voters. How different would the result have been?” We can make a simulation to answer that question. We can then change the size of the poll—the N—and see what that does.\n\n\n\n\n\n\nAn aside\n\n\n\nIf you were simulating Grunt and Flaky, what probability would you use for assigning a voter to Grunt? An obvious answer is 55%, until you stop and realize that 55% is the result of the poll, not the true, underlying probability of a voter voting for Grunt. That’s exactly what we don’t know! The sneaky trick we use is to simulate using 50%, pretending there is no difference between Grunt and Flaky, and seeing how often you get 55% in spite of that.\nLater, you might use a variety of “true” probabilites and decide over what range of probailities 55% is plausible. That’s called a confidence interval.\n\n\nI think it’s important that students build intuition about how poll results might vary. The most important realization is that the percent that will vote for Grunt will vary less the larger the poll is. That’s what I mean by stable.\nWhen you study statistics (or science) you’ll find out that the spread of the results goes down as the square root of N. But for now, a basic intuition is more important: If your data shows that the median income for Chilean-Americans is greater than the median income for whites in general, don’t take it too seriously if you have only two Chilean-Americans in your sample.\n\n\nPlausibility and chance\nFor the most part, looking at a graph (like the one above with heights) is enough to tell you whether some difference is important.\nBut once in a while, you need to check using inferential techniques.\nIn a stats course, you will learn various techniques that apply in different situations. But they all ask and answer the same question: If there were no difference, how likely is it that this difference would arise by chance? If it could happen reasonably often, we’d say it’s plausible. If it would be rare—depending how rare—we’d call it implausible; and as a shorthand, we might say the effect is “real.”\nIn introductory data science, it’s worth learning this basic idea and learning to apply it. But here you will not learn the many varied techniques. You’ll just learn two.\nThe first is to simulate “binomial” situations and use those simulations to reason about some underlying proportion or probability. Polling is such a context: there is an underlying proportion (the proportion of Grunt voters) and we pull a number of values at random (100 poll participants). By simulating many times, we learn how varied poll results can be, and how that spread depends on the sample size.\nA second technique is randomization. We ask, is there an association between gender and height? To decide, we artificially break any association by scrambling the values of Gender and computing the difference in means. By re-scambling many times and re-computing that difference, we learn whether the “true” unscrambled value would be unusual if there were no association.\nRandomization is for assessing the association between two variables. What about estimating some parameter of one variable, like the mean rainfall in a city? For that we can use a technique (and a CODAP plugin) called the bootstrap. This is the “randomization” equivalent of the confidence interval.\nMastering those two will set you up to understand the rest when the time comes.\n\n\nHow good is your model?\nModeling is a third area that a budding data scientist should address.\nModeling has many facets, but right here, I’m talking about using a function to approximate data. We use models in order to make predictions and to understand some underlying principles.\nImagine a scatter plot with a more-or-less linear swath of points. We want a good model, in this case, a line, that approximates the pattern we see. But what makes a model good?\nThe first part of that answer is that the line or curve you propose should have about the same shape as the data. It should have some of the same properties, too. For example, if the data are curved, you probably shouldn’t be satisfied with a line.\nAnother example: if the data are distances, they can never go below zero. So if your model is a function that goes below zero, it might not be a good model.\nSometimes it’s hard to decide if messy or zoomed-out data are curved or not, or, more generally, if the curve is the same shape as the data. In that case, it often helps to plot the residuals from the model in order to see if there is some underlying pattern you want to account for.\nThe second part of the answer is that you want to make your model pass close to the points.\nTo figure out how close the model is, create a measure of goodness of fit—a number, a statistic—that tells us how good the model passes to the data. At its crudest, the measure is the sum of the distances of all the points from the line. 2\nThen you change the line until that number is as small as possible. In that case, the line is as close as it can be to the points.\nBut what do you change? For a line, you can change the slope and the intercept, the m and b in y = mx + b. Those are two parameters that determine the line. So the trick is to figure out, of all possible values of slope and intercept, which ones give the smallest measure of goodness of fit.\nIf you use least squares, you can use calculus to figure that out. But for an introduction to data science, your eyeball can do a pretty good job. You’ll use sliders to vary the parameters and just look and see.\nThe key underlying idea is that of the measure itself."
  },
  {
    "objectID": "07.20-scrambling.html#driving-the-scrambler-plugin",
    "href": "07.20-scrambling.html#driving-the-scrambler-plugin",
    "title": "29  The Scrambler: Randomization Tests",
    "section": "29.6 Driving the Scrambler Plugin",
    "text": "29.6 Driving the Scrambler Plugin\nHere’s what to do:\n\nChoose scrambler from the Plugins menu in CODAP.\nPrepare your dataset for scrambling:\nMake a measure (a new attribute with a formula) that describes the effect you’re studying.\nDrag it left in the table (or up in the case card) so that it’s at a higher level in the hierarchy.\nIf there is anything wrong with the way you have prepared the table, you’ll get a message with help.\n\nTo specify the dataset and the attribute you want to scramble, drag the attribute into the scrambler plugin. // (We’re set up to scramble Gender in the heights dataset).\n\nAdjust the number and then click the buttons to create as many “scrambles” as you wish, up to 1000. (Usually, 400 is plenty.)\nA table of measures from the scrambling appears.\nAnalyze these to see if your original data seem unusual compared to the scrambled data.\n\n\n29.6.1 Example\n\nYou can try all this yourself!\nLet’s look at data about the difference in heights between 17-year-old males and females. We looked at this dataset [xxx where], and decided, without any calculation, that the males were clearly taller in general.\nxxx more description of how this works\n\n(Want a task? That document is set up to compare 13-year-olds. Make it compare 10-year-olds!)\n\n\n\nIn our example, we used the difference of means and called it dMeanHeights — and dragged it leftwards in the table. The CODAP formula looks like this:\n\n mean(Height, Gender=“Male”) - mean(Height, Gender=“Female”) \n\nWe see how much taller boys are in the actual data (in our case, 5.87 cm in the mean). Then we will see how much taller they are when the data have been all scrambled. Because the data are randomly assigned, sometimes the difference will be positive, sometimes negative (the “girls” will be taller).\n\n\nBut is it plausible that 5.87 could appear by chance?\n\n\nRepeat this process a few hundred times and see. In this case, no: even though it’s possible that the data could be that extreme (after all, the real data could come up when you scramble), it doesn’t happen very often."
  },
  {
    "objectID": "07.20-scrambling.html#analyzing-your-results",
    "href": "07.20-scrambling.html#analyzing-your-results",
    "title": "29  The Scrambler: Randomization Tests",
    "section": "29.7 Analyzing your results",
    "text": "29.7 Analyzing your results\n\nMake a graph of the measure from the “measures” table. You’ll see the sampling distribution. The picture shows the results from 200 scrambles.\n\n\n\nYou want to know what proportion of those measures are more extreme than your “test statistic” (which in our case is 5.87, the difference in mean heights).\n\n\nHere’s the trick:\n\n\n\nIn the graph, go to the “ruler” palette and press the Movable Value button.\n\nClick Add. A movable line appears on the plot.\n\nin the “ruler” palette again, click percent.\n\n\nNow you can see what percentage are on each side of the line.\n\n\nSet the line to 5.87 (you might need to rescale) to see how unusual it is! (Chances are, very few of your measures, positive or negative, are that large.)\n\n\n29.7.1 The P-value\n## When Things Go Wrong\n\n\nIf CODAP and the scrambler get confused, pressing the “refresh” arrow can help (it’s a circular, recyle-y arrow). That will basically restart the scrambler."
  },
  {
    "objectID": "07.20-scrambling.html#how-this-is-connected-to-traditional-stats",
    "href": "07.20-scrambling.html#how-this-is-connected-to-traditional-stats",
    "title": "29  The Scrambler: Randomization Tests",
    "section": "29.8 How this is connected to traditional stats",
    "text": "29.8 How this is connected to traditional stats"
  },
  {
    "objectID": "07.20-scrambling.html#the-problem-setup",
    "href": "07.20-scrambling.html#the-problem-setup",
    "title": "26  The Scrambler: Randomization Tests",
    "section": "26.1 The problem setup",
    "text": "26.1 The problem setup\nLet us return to the heights data we looked at in one of our first lessons. The dataset had measurements (including height) for 800 children and teens, ages 5–19.\nWe have set aside all but the 13-year-olds, and plotted the heights of the boys and the girls. On average, the boys are taller. Not all the boys are taller than all the girls—there is an overlap—but the overall trend is clear.\n\n\n\n\n\nHeight for 59 13-year-olds.\n\n\nAnd yet we wonder: this sample we have, of only 59 kids, does this difference reflect a real difference between all 13-year-old boys and girls? Or could it be that we got a difference this large just by chance alone?\nPut another way, with slightly different language:\nWe believe there is an association between Height and Gender: boys are taller. We are concerned that the height difference we see might be due to chance alone, when in fact, there is no association."
  },
  {
    "objectID": "07.20-scrambling.html#the-scrambling-strategy",
    "href": "07.20-scrambling.html#the-scrambling-strategy",
    "title": "26  The Scrambler: Randomization Tests",
    "section": "26.2 The scrambling strategy",
    "text": "26.2 The scrambling strategy\nHere’s the plan:\nWe will create fantasy universe where is definitely no association, and see how big a difference we get. We’ll do this by scrambling the values in one of the columns (it doesn’t matter which). In our example, we will mix up all the Genders so that the Male and Female values are all randomly assigned. Then we’ll see how big the difference is between these “boys” and “girls,” knowing that this difference is definitiely by chance.\nWe will then re-scramble many times, recording the differences, and see how often we get a difference as large as we saw in the original data.\nWhat difference, precisely, are we talking about? We get to decide. For this lesson, we will choose the difference in mean heights, which is 5.87 cm.\nWhen we do this scramble, and randomly assign Gender to all of the Heights, we expect the difference to be 0.00, right? Not exactly zero, but close.\nLet’s see what happens using CODAP’s scrambler plugin."
  },
  {
    "objectID": "07.20-scrambling.html#your-first-scramble",
    "href": "07.20-scrambling.html#your-first-scramble",
    "title": "29  The Scrambler: Randomization Tests",
    "section": "29.3 Your first scramble",
    "text": "29.3 Your first scramble\nWe will do this slowly.\n\nor use the live demo below. It will be cramped, but it’s doable! \nYou should already see:\n\nA table with 59 cases, with Gender and Height (just after Weight to the right).\nA quantity to the left, dMeanHeights, with a value of 5.87. This is the (true) difference in mean heights between the males and females in the sample.\nA box, the scrambler, with a number of controls. Importantl, it says that it’s OK to scramble Gender, that is, everything is set up correctly.\nA blue information button in the scrambler itself. It opens a help page; you can use it later in your life when you get confused and these pages are not right in front of you.\n\nDo these steps:\n\nMake a graph of Height (vertical axis) against Gender. Put means on the graph, and verify that the difference in means is about 6 cm. Make the graph narrow. you’ll need a lot of space!\nIn the scrambler, press the 1x button. This does one scramble.\n\nA new table, measures_heights appears with one value under dMeanHeight. This value is the difference in mean heighhts in the “scrambled” dataset. But you can’t see the scrambled dataset right now! Let’s fix that:\n\nClick the show scrambled button in the scrambler. The scrambled table appears. The value for dMeanHeight should match the one in the measures table.\n\n\n\n\n\nScroll to the top of that table and verify that the Gender values are in fact scrambled!\nMake a graph of Height against Gender for that table. Put the means on that graph. You will need to scrunch up some things to make them all fit!\n\nYou should see that the means are closer together.\n\nPress 1x a few more times and see what happens. Notice that when you do, the measures_heights table records the new value of dMeanHeight.\nGraph that dMeanHeight from measures_heights.\nPress 10x a few times to get more points in that graph.\n\nI did this for a total of 100 measures. My measures graph appears below. Yours will be different, of course, but should look more or less the same:\n\n\n\nResults from 100 scrambles. The “true” value of dMeanHeight is 5.87 cm\n\n\nThis is the payoff: In the graph, you can see that at most three points are as extreme as the real data, the 5.87.\nThat is, if there were no difference bewteen boys’ and girls’ heights, we might see differences this large in 3% of our samples.\nThat small value (called the \\(P\\) value) makes us confident that the mean height for all 13-year-old boys really is greater than the mean height for the girls.\n\n\n\n\n\n\nDo you need to see the scrambled dataset?\n\n\n\nNo. It helps when you’re first learning so you can see what’s going on, but once you get the hang of this, you only need to see the measures.\nThis will save a lot of screen space!"
  },
  {
    "objectID": "07.20-scrambling.html#deconstructing-p-what-it-means-and-what-it-doesnt.",
    "href": "07.20-scrambling.html#deconstructing-p-what-it-means-and-what-it-doesnt.",
    "title": "29  The Scrambler: Randomization Tests",
    "section": "29.6 Deconstructing \\(P\\): What it means and what it doesn’t.",
    "text": "29.6 Deconstructing \\(P\\): What it means and what it doesn’t.\nThe so-called \\(P\\)-value is the proportion of scrambled samples that are at least as extreme as the test statistic. Looking back at the heights of 13-year-olds, the test statistic is the value of the difference taken from the real data, which is 5.87 (cm).\nSo to calculate \\(P\\), take the number of samples with differences (dMeanHeight) above \\(5.87\\) or below \\(–5.87\\), and divide that by the number of samples. We saw three large values in 100 samples, so we get \\(P = 0.03\\).\nWith the rainfall, we got 46 out of 200 more extreme, or \\(P = 0.23\\). A much higher value.\n\n29.6.1 Traditional use of \\(P\\)\nFrom the scrambling experience, we see that if there are few scrambled samples as extreme as our data, that the difference in the data is more likely to be “real” and not due to chance alone.\nThat is, a small \\(P\\)-value suggests that the effect is real. For many people, a \\(P\\) that’s less than 0.05 satisfies them. That “0.05 limit” has come under a lot of fire in the 21st Century, so avoid it if you can.\nBut you can say that the smaller the \\(P\\)-value, the more “confidence” you have in the result.\n\n\n29.6.2 Caution, nuance, and depth\nThe true meaning of \\(P\\) is exactly what we said:\n\nIf there were no difference bewteen boys’ and girls’ heights, we might see differences this large in 3% of our samples.\n\nThe big mistake many students make is to think that it means this:\n\nThere is a 3% chance that there is no difference between the boys and the girls. (this is wrong)\n\nThe problem with the right meaning is that it’s kind of twisted and subjunctive, all that “if there were no difference…” stuff. It doesn’t tell us what we really want to know, which is more like,\n\nIf I said that the boys are taller than the girls, what’s the chance that I’m wrong?\n\nThat question is the province of Bayesian statistics, which is not usually a part of the introductory course.\nThe point of scrambling is to create a sampling distribution of some measure. For example, suppose that in your dataset it appears that 13-year-old boys are taller than 13-year-old girls. You want to assess whether it’s plausible that the difference in means that you see could happen by chance.\nTo do that, you will make the “null hypothesis” real: you will break any association between Gender and Height by scrambling the values for one of those attributes. Then you look to see how different the boys and girls seem to be when the difference is just chance.\nBut one trial is not enough. Furthermore, you have to decide what, specifically, to look at to say that the boys are taller. In this situation, that means coming up with a number that represents how much taller the boys are.\nThis is very important, and bears highlighting:\n\nYou must create a measure of the effect you’re seeing. It’s not enough to say that boys are taller than girls; you have to say how much taller."
  },
  {
    "objectID": "07.20-scrambling.html#another-example",
    "href": "07.20-scrambling.html#another-example",
    "title": "29  The Scrambler: Randomization Tests",
    "section": "29.4 Another example",
    "text": "29.4 Another example\nIn the previous example, the scrambler plugin was all set up for you. This time, you’ll do it yourself.\nYour data will be from the NOAA plugin, and consists of daily observations from two airports, Portland, Oregon (PDX) and Seattle-Tacoma (SEA), in 2021. The graph shows the daily precipitation, precip, in inches. My stereotype (and maybe yours) says that Seattle will be rainier.\nWhat a weird-looking graph…but it makes sense. Many days have zero rainfall or only a little. How will we make sense of this? If we compare the mean, it looks as if Seattle has a higher mean rainfall, but not by a lot. Could that be due to chance?\nWe should really compute the total yearly rainfall for the two cities. Then we could subtract those numbers to get a measure, maybe called dRain, that tells us how big the difference really is. How will we make that formula?\nWe did something very similar before, for dMeanHeight. Let’s look at its formula:\n\nYou could do something analogous for this problem. But the formula is long and complicated and it’s easy to mess up. We’ll do this instead:\n\nMake one attribute for the total Portland rainfall (PDXrain) and a separate one for SEArain.\nThe formula for PDXrain is sum(precip, where=\"PDX\"). Don’t forget the quotes!\nMake a third new attribute, dRain, whose formula is simply SEArain - PDXrain.\n\nThe left side of your table should look like this:\n\n\n\nTim’s table showing rainfall in Portland and Seattle—and the difference dRain\n\n\nYour value for dRain should turn out to be 7.76, the amount that Seattle’s rainfall was bigger than Portland’s.\n\nChoose scrambler from the Plugins tool. It will complain that you need a measure dragged to the left.\nSo drag the three new attributes to the left.\nNow it wants to know what attribute to scramble. We’ll scramble where. Drag that into the scrambler and drop it.\n\n\n\n\nNow your scrambler is ready to work! It should look like the illustration in the margin.\n\n\n\n\n\nScrambler ready to scramble.\n\n\nProceed with your investigation.\n\nStart with one scramble to be sure it works. Look at the scrambled dataset if you want—but be sure to put it away before you scramble a lot; it will slow the process down.\nGraph your scrambled value of dRain.\nKeep scrambling until you get 200 measures. Make sure the graph is updating so you see the distribution.\nLet’s be smart about counting the extreme points this time. Do this:\n\nIn the ruler palette of the graph, press the Movable value button twice.\nMove one of them to –7.76 and the other to +7.76. You will see the range highlighted.\nIn the ruler, up at the top of the palette, click the boxes for Count and Percent.\n\n\nMy graph looks like this:\n\n\n\nScrambled SD_difference values. 23% were more extreme. It’s not clear that the rainfall difference could not have been by chance.\n\n\nInteresting! My preconception—that Seattle is rainier—is right in that it got almost 8 inches more rain, but this analysis suggests that it might not be such a big deal.\nMore precisely, it suggests that if there were no association between the city and the rain, we would get a difference of 7.76 or more roughly a quarter of the time.\nAnd things with a probability of about 1/4 happen all the time.\nDoes that mean that Seattle is in fact not rainier? No.  But it does say that these data, on their own, are not enough to rule out chance."
  },
  {
    "objectID": "07.20-scrambling.html#another-example-rain-in-seattle-and-portland",
    "href": "07.20-scrambling.html#another-example-rain-in-seattle-and-portland",
    "title": "26  The Scrambler: Randomization Tests",
    "section": "26.4 Another example: rain in Seattle and Portland",
    "text": "26.4 Another example: rain in Seattle and Portland\nIn the previous example, the scrambler plugin was all set up for you. This time, you’ll do it yourself.\nYour data will be from the NOAA plugin, and consists of daily observations from two airports, Portland, Oregon (PDX) and Seattle-Tacoma (SEA), in 2021. The graph shows the daily precipitation, precip, in inches. My stereotype (and maybe yours) says that Seattle will be rainier.\n\n\n\nDaily rainfall in Seattle and Portland\n\n\nWhat a weird-looking graph…but it makes sense. Many days have zero rainfall or only a little. How will we make sense of this? If we compare the mean, it looks as if Seattle has a higher mean rainfall, but not by a lot. Could that be due to chance?\nWe need a measure. Let’s compute the total yearly rainfall for the two cities. Then we could subtract those numbers to get a measure, maybe called dRain, that tells us how big the difference really is. How will we make that formula?\n\n26.4.1 Making a formula for the total difference\nWe did something very similar before, for dMeanHeight. Let’s look at its formula:\n\n\n\n\n\nYou could do something analogous for this problem. But the formula is long and complicated and it’s easy to mess up. We’ll do this instead:\n\nMake one attribute for the total Portland rainfall (PDXrain) and a separate one for SEArain. You should do this without dragging anything to the left.\nThe formula for PDXrain is sum(precip, where=\"PDX\"). Don’t forget the quotes! You might remember this syntax from the summarizing chapter.\nMake a third new attribute, dRain, whose formula is simply\n\nSEArain - PDXrain\nThe right side of your table should look something like this:\n\n\n\nTim’s table showing rainfall in Portland and Seattle—and the difference dRain\n\n\nYour value for dRain should turn out to be 7.76, the amount that Seattle’s rainfall was bigger than Portland’s.\n\n\n26.4.2 Ready to scramble!\n\nMake the three new attributes as we described above.\nChoose scrambler from the Plugins tool. It will complain that you need a measure dragged to the left.\nDrag all three new attributes to the left.\nNow it wants to know what attribute to scramble. We’ll scramble where. Drag where into the scrambler and drop it.\n\n\n\n\nNow your scrambler is ready! It should look like the illustration in the margin. Proceed with your investigation.\n\n\n\n\n\nScrambler ready to scramble.\n\n\n\nStart with one scramble to be sure it works. Look at the scrambled dataset if you want—but be sure to put it away before you scramble a lot; having it visible will slow the process down.\nGraph your scrambled value of dRain.\nKeep scrambling until you get 200 measures. Make sure the graph is updating so you see the distribution.\nLet’s be smart about counting the extreme points this time. Do this:\n\nIn the ruler palette of the graph, press the Movable value button twice and Add it.\nMove one of the movable values to –7.76 and the other to +7.76. You will see the range highlighted.\nIn the ruler, up at the top of the palette, click the boxes for Count and Percent.\n\n\nMy graph looks like this:\n\n\n\nScrambled dRain values. 23% were more extreme. Notice that if a point were at ±7.7, it would not look particularly unusual. It would plausibly be part of this dataset.\n\n\nInteresting! My preconception—that Seattle is rainier—is right in that it got almost 8 inches more rain, but this analysis suggests that it might not be such a big deal.\nMore precisely, it suggests that if there were no association between the city and the rain, we would get a difference of 7.76 or more roughly a quarter of the time…\nAnd things with a probability of 1/4 happen a lot.\nDoes that mean that Seattle is in fact not rainier? No. But it does mean that these data, on their own, are not enough to rule out chance.\n\n\n\n\n\n\nReflections\n\n\n\n\nWe could have made just one measures formula, with the subtraction, just as we did with the heights. But making two extra columns doesn’t cost you anything, and makes the formulas easier to understand.\nYou might wonder, why not drag where to the left and compute totalRainfall there? Then you’ll have one row for Portland and one for Seattle, each with its total rainfall.\nTrue. And that’s exactly what we would do if we weren’t scrambling. But to scramble, we have to compute the difference. We can do that, but it’s a little more complicated. By all means, explore that possibility on your own!"
  },
  {
    "objectID": "03.40-Summarizing.html#summary-without-groups",
    "href": "03.40-Summarizing.html#summary-without-groups",
    "title": "15  Summarizing",
    "section": "15.6 Making summaries without groups",
    "text": "15.6 Making summaries without groups\nWe have been making new, summary attributes “on the left,” next to the grouping attribute. But what if you have no grouping attribute? What if you need to know a mean for the whole dataset?\nNo problem. Suppose we want to know the mean height of the people in a dataset. Try this in the live demo below:\n\nMake a new column in the main dataset. It will appear on the right.\nGive it a good name such as meanHeight.\nGive it a formula, mean(Height).\n\nYou will see that the column fills with values, and they are all the same: 169.08.\n\n\n\nMake sure you understand why this makes sense: in every row, for every individual, we are calculating the mean of the whole dataset. So of course that number is the same for everybody.\nNow, if you want, you can drag that attribute to the left, and then you will see only one value, the one that corresponds to the whole dataset:\n\n\n\n\n\n\nMultiple identical values of meanHeight\n\n\n\n\n\n\n\nSingle, whole-group value after dragging to the left\n\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nBecause the formula is built from a summarizing function (mean()), the values of meanHeight all apply to the whole dataset…so they are all the same.\nThen, because that column has so many duplicate values, it’s a candidate for dragging to the left.\nThat’s a grouping move, right? But here there is only one group. That’s OK. But it reminds us that, often, when you see lots of duplicate values in a column (for example, single in marital-status) that means that the column might make sense on the left, creating groups."
  },
  {
    "objectID": "07.20-scrambling.html#teaching-and-learning-scrambling",
    "href": "07.20-scrambling.html#teaching-and-learning-scrambling",
    "title": "29  The Scrambler: Randomization Tests",
    "section": "29.5 Teaching and learning scrambling",
    "text": "29.5 Teaching and learning scrambling\nScrambling is a form of simulation-based inference, which is a thing. You can google it. The tests we’ve been constructing are called randomization tests.\nThese have several elements:\n\nThe problem has to involve some kind of difference between groups.\n\nThe student needs to create a measure, typically an attribute with a formula, that expresses the difference they’re investigating. The value of that measure is called the test statistic.\nThe student needs to choose an attriute to scramble; typically that’s the attribute that defines the groups.\nThe scrambler creates scrambled datasets and collects values of the measure. That set of values is called the sampling distribution.\n\nI love the fact that we get to make our own measures out of something that makes sense in the context, such as the difference in total rainfall. This is in contrast to the more traditional, parametric tests such as \\(t\\), instead of trying to memorize the formulas for \\(t\\) and where the \\((n-1)\\) goes. Instead, we get to make a measure"
  },
  {
    "objectID": "07.20-scrambling.html#commentary",
    "href": "07.20-scrambling.html#commentary",
    "title": "26  The Scrambler: Randomization Tests",
    "section": "26.5 Commentary",
    "text": "26.5 Commentary\nThere is a lot to think about with scrambling; here are several related topics.\n\n26.5.1 Teaching and learning scrambling\nLearning scrambling is partly about what commands to issue when. Mor important, however, is helping students understand what the software is accomplishing. That comes with time, but it’s worth it here to list what some of the understandings.\nA scrambling situation and its solution have several elements:\n\nThe problem involves some kind of difference between groups—or a difference between your data and some fixed value.\nYou need to create a measure, typically an attribute with a formula, that expresses the difference you’re investigating.\nYou need to choose an attribute to scramble; typically that’s the attribute that defines the groups.\nThe scrambler creates scrambled datasets and collects values of the measure. You have to decide how many times to scramble, that is, how many values will be in your distribution. This becomes the denominator in your \\(P\\)-value; but the exact number doesn’t matter. It should be at least 100, and usually 400 is plenty.\nTo decide whether the difference is real, you must compare the true value of the measure (the test statistic) with the distribution you just collected. If the test statistic would be a typical value in that distribution, then the data are consistent with there being no difference.\n\nMaybe most important is a central concept we have not explicitly named: the null hypothesis. This is the fictional, alternate-universe situation in which there is no association between the groups and the measure: in this fantasyland, gender is not related to height; rainfall is not affected by whether you’re in Seattle or Portland. Scrambling makes the null hypothesis true, but in a stochastic way: there will be a difference between the groups, but that difference is due to chance alone. So it will be different every time—but not too different.\nUnderstanding this can help students realize an important truth: in real life, the null hypothesis is never true. There is always some gender difference; the rainfall is never the same. We are only asking whether the true situation is consistent with the fantasy situation of no difference.\n\n\n\n\n\n\nModeling connection\n\n\n\nThis is connected with modeling in that we make probability models all the time, in which, for example, a coin has a 50% chance of coming up heads.\nBut a coin never has a probability of exactly one half. It’s just close enough that we make predictions as if it were.\n\n\nThe measure The measure is where you get to be creative. The only requirement is that it apply to the entire dataset. That means it’s probably built out of “aggregate” functions such as mean(), sum(), or count(). Learning to write the formula for your measure is often the hardest part of this for many students.\nI love the fact that we get to make our own measures out of something that makes sense in the context, such as the difference in total rainfall.\nThis is in contrast to more traditional, parametric tests such as \\(t\\). If you scramble, you don’t need to memorize formulas for \\(t\\) or where the \\((n-1)\\) goes. With scrambling, you also don’t worry about whether the distributions are Normal enough, because (unlike with \\(t\\)) you are not relying on the Normal distribution to compute the \\(P\\)-value.\nAnother wonder of this system is that you can scramble any measure. You can study a difference of means or a difference in sum, as we have done; but you could also look at a difference in medians using exactly the same logic; or compare two proportions, or even test for slope.\n\n\n26.5.2 Orthodox vocabulary\nIf you study stats or teach an introductory course, including AP Statistics, outside this book and away from CODAP, you will use some specific terms for what we have been doing. We have gently steered away from some of the official vocabulary, but once you know what’s going on and want to learn more, it helps to know some of the words and phrases, if only so that you know what to look up.\nScrambling is a form of simulation-based inference, which is a thing. You can google it.\nThe tests we’ve been constructing are also called randomization tests.\nThe value of the measure, using the real data, is called the test statistic.\nIn a scrambled dataset, the null hypothesis is true.\nThe set of values of that measure when you scramble is called the sampling distribution.\nThe \\(P\\)-value is the proportion of cases in the sampling distribution that are at least as extreme as the test statistic.\n\n\n26.5.3 Deconstructing \\(P\\): What it means and what it doesn’t.\nThe so-called \\(P\\)-value is the proportion of scrambled samples that are at least as extreme as the test statistic. Looking back at the heights of 13-year-olds, the test statistic is the value of the difference taken from the real data, which is 5.87 (cm).\nSo to calculate \\(P\\), take the number of samples with differences (dMeanHeight) above \\(5.87\\) or below \\(–5.87\\), and divide that by the number of samples. We saw three large values in 100 samples, so we get \\(P = 0.03\\).\nWith the rainfall, we got 46 out of 200 more extreme, or \\(P = 0.23\\). A much higher value.\n\nTraditional use of \\(P\\)\nFrom the scrambling experience, we see that if there are few scrambled samples as extreme as our data, that the difference in the data is more likely to be “real” and not due to chance alone.\nThat is, a small \\(P\\)-value suggests that the effect is real. For many people, a \\(P\\) that’s less than 0.05 satisfies them. That “0.05 limit” has come under a lot of fire in the 21st Century, so avoid it if you can.\nBut you can say that the smaller the \\(P\\)-value, the more “confidence” you have in the result.\n\n\n\\(P\\): Caution, nuance, and depth\nThe true meaning of \\(P\\) is exactly what we said:\n\nIf there were no difference bewteen boys’ and girls’ heights, we might see differences this large in 3% of our samples.\n\nThe big mistake many students make is to think that it means this:\n\nThere is a 3% chance that there is no difference between the boys and the girls. (this is wrong)\n\nThe problem with the right meaning is that it’s kind of twisted and subjunctive, all that “if there were no difference…” stuff. It doesn’t tell us what we really want to know, which is more like,\n\nIf I said that the boys are taller than the girls, what’s the chance that I’m wrong?\n\nThat question is the province of Bayesian statistics, which is not usually a part of the introductory course."
  },
  {
    "objectID": "07.30-bootstrap.html#a-bootstrapping-example",
    "href": "07.30-bootstrap.html#a-bootstrapping-example",
    "title": "30  Randomization and Estimation: the Bootstrap",
    "section": "30.1 A bootstrapping example",
    "text": "30.1 A bootstrapping example\nSuppose you measure the weights of 10 capybaras and you find that their mean weight is 47 kilograms. If these capybaras are a suitably random sample of all capybaras, what can you say about the mean weight of all capybaras?\nClearly our best guess is 47 kilos as well. But will it be exactly 47 kilograms? Of course not. Capybaras vary in weight, and there’s no way we accidentally, randomly, got 10 capybaras that were perfectly representative of the whole species.\nThe big question is, how far off of the population mean is the mean of our single sample likely to be?\nTo find out, we do something that kinda sorta smells like what we did with randomization tests and scrambling.\nHere’s the plan:\nWe write the weights of all ten capybaras on chips and put them into a bag. Then we draw out ten chips, record the numbers, and find the mean. However, we put each chip back after we draw it. That is, we are sampling with replacement. This is called a “bootstrap sample.”\nBecause we put them back every time, we’ll probably get some duplicates. Similarly, some of our capybaras won’t be represented in a particular bootstrap sample. Therefore the means we get won’t be exactly the same as the mean of our “real” sample. Some means will be larger, some smaller, depending on which chips we pulled.\nWe will do this many times, and then look at the distribution of means in order to assess how far off our single-sample mean is likely to be.\n\n30.1.1 Your first bootstrap: April in Paris\nWe don’t have measurements of capybaras, but we do have one bag of 18 Trader Joe’s tangerines, measured by students at Lick-Wilmerding High School in San Francisco in 2017.\nThese data appear in a live example below, with a graph of the weights, also showing the mean, which is a little under 53 grams.\n\nMake a new column called meanWeight and compute the mean of weight. It should be 52.94 g.\nNow we want to get the bootstrap plugin. As of May 2023, it is not in the Plugins menu, so we will do the following:\n\nChoose Import… from the hamburger menu.\nClick URL and enter https://codap.xyz/plugins/bootstrap/ into the box.\nClick IMPORT. The bootstrap plugin appears.\n\nbootstrap will complain that it needs a measure. You have one: meanWeight! Drag it to the left.\n\nYou are ready to strap on your boots. Your workspace should look something like this:\n\n\n\n\n\nPress the 1x button to do a single bootstrap resample. A new dataset appears, called measures_basic tangerines with one value of the mean under the meanWeight column (on the right). It will probably not be the same value as the original, which makes sense.\n\n\n\n\n\n\nOne measure. My first bootstrap mean was 52.98.\n\n\n\nGraph meanWeight. Collect more samples. They should center roughly around the true mean, 52.94.\nKeep collecting so you have a total of 200.\n\nThis distribution should be pretty symmetrical, and centered on the original mean.\nThink about what each point represents: a mean of 18 values drawn from a distribution equal to that of the real data. That is, if all groups of TJ tangerines are statistically the same as the 18 in our bag, each one of these values is a possible mean.\n\n\n\n\n\n\nYou could look at the samples…\n\n\n\nTo see the last bootstrap sample,\n\nGo to the Tables menu.\nChoose bootstrapSample_basic tangerines\nGraph weight and compare it to your original data. Can you see the holes? The duplicates?\n\n(We did this when we scrambled the heights of 13-year-olds)\n\n\nAll of the means in our graph are possible, some values are more likely. We have 200 values; let’s find the “middle” 90%. We will exclude 10 cases on either end of the distribution.\n\nIn the ruler menu/palette, press Movable Value and Add twice. You now see a range.\nAlso in the ruler, check Count and Percent.\nMove the values to get as close as you can to 10 (or 5%) in each tail.\n\nYou should have a graph more or less like mine:\n\n\n\n\n\nWith this technique, we have created what I call a “plausibility interval.” My best guess for the mean weight of all Trader Joe’s tangerines is 52.94 grams. But I would not be at all surprised if it were anywhere in the range from 52.11 to 53.84. If somebody gave me a bag of 18 tangerines, and their mean weight was 55, I would think that maybe they were from another store, or that maybe TJ had changed supplier (since 2017, almost certain!), or something else that made these new tangerines systematically different from the ones in the original bag.\nNow notice, looking back at the original data, that a single tangerine of 55 grams is not surprising at all! Here are the two graphs together, scaled the same:\n\n\n\nBelow, in blue: the original data. Above, in tangerine: mean weights from 200 bootstrap samples.\n\n\nThis makes some kind of sense. Looking at the original data graph, we see that if we pulled a 55-gram tangerine out of a TJ bag, chances are good that most of the other 17 will be lighter.\nWe could spend a great deal of time right here on why the distribution of means is so much narrower than the distribution of the original data. That is the province of a more formal stats class.\nBut it does speak to our goal of developing a taste for stability. The tangerines have a range of about 10 grams, but a bag of 18’s mean is within about ±1 gram. The mean of a large sample is stable in a way that the individual data are not."
  },
  {
    "objectID": "08-arbor/08.10-arbor-index.html",
    "href": "08-arbor/08.10-arbor-index.html",
    "title": "31  Classification trees in CODAP using Arbor",
    "section": "",
    "text": "Arbor is a CODAP plugin that helps you make classification trees. If you are in a workshop or a class, your teacher will probably tell you a lot of what’s here. You can use this site as a reference, to remember what you’ve forgotten, or as a stand-alone self-paced course, in which case you can just work through the headings you see at left.\n\n31.0.1 Overview\n\nAnatomy of a tree\n\nLearn how to read a tree, using data from the Titanic disaster.\n\nFirst driving lesson\n\nThe basics of how to make a tree using Arbor.\n\nTrees and graphs\n\nTrees and graphs are both ways of looking at the data. And they are intimately connected. Learn how to use graphs to help you make th ebest trees.\n\nFbola example\n\nAnother example dataset, this time in a medical context. Use the tree to decide who probably has the virus.\n\nTree quality\n\nWhat makes a tree a good tree? Arbor can help you calculate tree quality.\n\nBreast cancer example\n\nWith a numerical predictor, we create our own measure of tree quality, and use it with graphs to make the best tree.\n\nMachine learning connection\n\nClassification is an important topic in machine learning. This explains the connection and how work with Arbor might fit in.\n\nThe configuration box\n\nWhen you “configure” a node in a tree, there is a dialog box. This explains all of its controls.\n\n\nTo learn why you would want to use this plugin, see xxx."
  },
  {
    "objectID": "08-arbor/08-arbor-part.html",
    "href": "08-arbor/08-arbor-part.html",
    "title": "Classification trees in CODAP using Arbor",
    "section": "",
    "text": "Arbor is a CODAP plugin that helps you make classification trees. If you are in a workshop or a class, your teacher will probably tell you a lot of what’s here. You can use this site as a reference, to remember what you’ve forgotten, or as a stand-alone self-paced course, in which case you can just work through the headings you see at left.\n\nOverview\n\nAnatomy of a tree\n\nLearn how to read a tree, using data from the Titanic disaster.\n\nFirst driving lesson\n\nThe basics of how to make a tree using Arbor.\n\nTrees and graphs\n\nTrees and graphs are both ways of looking at the data. And they are intimately connected. Learn how to use graphs to help you make th ebest trees.\n\nFbola example\n\nAnother example dataset, this time in a medical context. Use the tree to decide who probably has the virus.\n\nTree quality\n\nWhat makes a tree a good tree? Arbor can help you calculate tree quality.\n\nBreast cancer example\n\nWith a numerical predictor, we create our own measure of tree quality, and use it with graphs to make the best tree.\n\nMachine learning connection\n\nClassification is an important topic in machine learning. This explains the connection and how work with Arbor might fit in.\n\nThe configuration box\n\nWhen you “configure” a node in a tree, there is a dialog box. This explains all of its controls.\n\n\nTo learn why you would want to use this plugin, see xxx."
  },
  {
    "objectID": "08-arbor/08.10-arbor-anatomy.html",
    "href": "08-arbor/08.10-arbor-anatomy.html",
    "title": "31  Anatomy of a tree",
    "section": "",
    "text": "What does an Arbor tree look like? How do you read it? Let’s look at an example. This tree shows data from a famous dataset about the 1309 passengers aboard the Titanic. We will explore what seems to affect whether the passengers survived.\n\n\n\nLater, you will make this tree!\n\n\nFirst of all, this tree—like most trees in data analysis—is upside-down. The root and trunk are at the top, and the leaves are at the bottom.\n\n31.0.1 The root node\nSo: way up at the top, the “root” block, the root node, looks like this:\n\n\n\n\n\nThe first horizontal “stripe” tells us what attribute we are trying to predict. In this dataset, it’s called fate. (The value of fate is either survived or died.)\nThe second stripe tells us that we will consider survived to be the “positive” result. That seems obvious, but in many medical contexts, a “positive” test means that you have the disease.\n\n\n31.0.2 The trunk node\nJust below the root is the trunk. That block, that node, also has two stripes.\nThe top one tells you that 500 of the 1309 people—38.2% of them—survived. You know it’s survived because of what it says in the root. But if you forget, or get confused, you can always hover over that node to see details:\n\n\n\n\n\nThe bottom stripe sets up a branching. In this case, we ask about gender, and branch one way if the person is female and the other way if they are male.\n\n\n31.0.3 Other nodes\nEvery other node is structured more or less the same as the trunk node: the first stripe shows how many people are positive, that is, how many survived, and the second shows the branching if there is one.\nThe key thing about the number stripe is that it considers only the people who get to that point in the tree. So although 38% of all people survived, the tree shows us that there is a big gender difference: 73% of the females survived, but only 19% of the males.\nAnd then, among the males, the young males—under 15 years old—had a 50% survival rate compared to about 18% for males 15 or older.\n\n\n\n\n\n\n\n31.0.4 Terminal nodes: the leaves\nIf a node has no branches coming out of it, it is a terminal node. Every terminal node has a “leaf” below it that is rounded instead of rectangular. The leaf shows a prediction for the fate of the people in that category.\nSo for the females, our best guess is that they will survive. For older males, our best guess is that they will die.\nFor the young males, under 15, with a 50% survival rate, we’re predicting survival, although you can make a case that at 50% we shouldn’t predict. The other side of that argument is that they have a better chance than people in general (38%).\n\n\n31.0.5 The links\nSlanted white stripes, called links, connect nodes to their “children.” Every link has a label such as female or >= 15 so you can tell who that link applies to.\nSo: that’s how you read a tree! To learn how to make a tree, see the page about learning to drive."
  },
  {
    "objectID": "08-arbor/08.20-arbor-begin.html",
    "href": "08-arbor/08.20-arbor-begin.html",
    "title": "32  Your first tree",
    "section": "",
    "text": "Let’s learn to drive! We will begin by exploring a dataset about the passengers aboard the Titanic. This is the goal:\n\n\n\nYou will make this tree showing data about passengers on the Titanic. Notice the huge gender difference in survival rates.\n\n\n\n32.0.1 The target attribute\nTo make a tree, you first have to identify your target attribute. The target attribute might also be called\n\nthe dependent variable\nthe outcome variable\nthe effect\n\nIt’s the thing you are trying to predict. In the case of the Titanic data, it’s fate: we want to know what makes it more likely to have survived the disaster.\nIn the live example below, start your tree by dragging fate from the table at the right and dropping it into the middle of the blank, gray tree panel. You should see that 500 of the 1309 people survived.\n\n\n\n\n\n32.0.2 Making a branch: drag and drop\nNext, drop sex onto the white box with “500 of 1309” in it.\nThe tree will branch. You can see how the survival rate was different for males and females.\nNotice that the tree is composed of boxes (called nodes) and lines (links).\n\n\n\n\n\n\nLost in the Node?\n\n\n\nIt’s easy to lose track of what’s going on in a node. When that happens, just point at the node. Don’t click, just hover for a moment, and text will pop up describing that node in more detail.\n\n\n\n\n\n\n\nThe pop-up information for the “male” node\n\n\n\n\n32.0.3 Adding a numeric attribute\nNow drop age onto the “male” node. It will split, but probably using age 30 as a cutpoint. Click the gear on the age stripe and change that 30 to 15.\n\n\n\n\n\nConfiguring age: setting the cutpoint to 15.\n\n\nWhen you make a tree with Arbor, every node has a maximum of two branches. An attribute like age has so many values, you generally have to tell Arbor where that cutpoint is using that configuration box.\nYou can configure any attribute, but it’s more common for numerical ones.\n\n\n32.0.4 Assigning results to terminal nodes\n\n\n\n\n\nYour tree should look like this after you have assigned its leaf nodes to be survived or died.\n\n\nNow your tree should look like the one up at the top of the page… except that you still have to assign the terminal nodes — the leaves of the tree — to an outcome. Do that by clicking on the leaf nodes repeatedly until you see what you want."
  },
  {
    "objectID": "08-arbor/08.60-arbor-breast-cancer.html",
    "href": "08-arbor/08.60-arbor-breast-cancer.html",
    "title": "36  More about tree quality: an example with breast cancer data",
    "section": "",
    "text": "In this section, we will further our understanding of the quality of trees, remind ourselves about what happens with numerical data, and learn a brutal truth about false negatives and false positives.\nLet us begin by saying that the two ways you can be wrong—FN and FP—are usually not equally bad. The misclassification rate (MCR) makes no distinction: for the MCR, wrong is wrong.\nBut let us look at some breast cancer data.\nWe have a number of numerical measurements of tumors that were calculated by image processing software, using digital images. We will focus here on radius, which is in millimeters. There are other measurements too such perimeter, which is obvious, and concavity, which would demand some clear explanation. And as before, we have a version of the Truth: the result from a biopsy, which in this case is either benign or malignant. The goal is to predict malignancy without the expense and wait-time of a biopsy: just take an image and let the computer figure it out.\nNow, which would you rather have as your mistake—a false negative or a false positive?\nMost of us would say that a false negative is worse. (Never mind for a moment that you can make the opposite case.)\n\n36.0.1 Looking at the data\nIn the live illustration below, you already have a tree set to predict biopsy.\n\nLook at the tree: among these 569 tumors, 37.3% were malignant.\nLook at the graph: apparently (and unsurprisingly) benign tumors are generally smaller.\nAlso in the graph: we have a “movable value” currently set to 16.6 millimeters. Almost all of the tumors bigger than 16.5 mm are malignant.\n\nDo the following:\n\nDrop radius into the trunk of the tree to make a branch.1\n\nArbor will pick a cut point that determines which values of radius go to the larger branch and which go to the smaller. When I did this, that cut point was 26 mm. You need to change that to be 16.5.\n\nHover over the radius node and click the gear that appears. The configuration box appears.\n\n\n\n\n\n\nLearn more about the configuration box.\n\n\n\nChange the value in the box from 26 to 16.5 and press Done.\nSpecify diagnoses for your leaves: make the “large” tumors Malignant.\n\n\n\n\nIf your setup is like mine, that will give you 125 amlignant diagnoses, of which 122 were correct. So that leaves three false positives. On the otehr hand, there are 90 false negatives. Thats’ a lot!\nLet’s alter the graph to see this more clearly.\n\nDrop biopsy onto the vertical axis of the graph.\n\nThe graph splits to make two parallel dot plots, each with its own movable value. There are now four counts (and percentages); one for TP, one for FP, etc. See if you can clearly identify which is which, and why. Your graph should look like this:\n\n\n\nIf everything is working correctly, your graph should look like this.\n\n\nWe want to figure out what a good cut point would be. You can explore changing the movable values from 16.5 (you have to change both of them) and see how that affects the numbers of FP (upper right) and FN (lower left). And you should notice two sad truths:\n\nBecause the two distributions overlap, there is no way to get both FP = 0 and FN = 0.\nFalse negatives are the worst. We want to lower that number. But if you lower the cutpoint value to decrease FN, FP has to increase.\n\nSuppose we wanted to eliminate FN’s altogether. We would set the cutpoint to some small number (like zero)…but then all tumors would be diagnosed as malignant. No false negatives, because there would be no negatives at all!\nThis means that every cutpoint is a balancing act. We can use a measure of goodness of a tree to help us achieve that balance.\nWe could make a measure of our tree’s effectiveness just like we did in the previous section. Suppose we decide that a false negative is five times as bad as a false positive. Then our formula might be\nFN * 5 + FP\nand we would want to minimize that value.\nRemember that to get Arbor to calculate the values,\n\nOpen up the in order to export section below the tree.\nPress emit data.\n\nThat will give you a new table with values for FP etc. Then,\n\nMake a new attribute and give it that formula, FN * 5 + FP. The value you’ve calculated appears in the table.\nMake another new attribute and enter the value for the cut point (otherwise you might forget).\n\n\n\n36.0.2 Some tasks\nHere are some tasks you might do to extend your understanding.\n\n36.0.2.1 Find the minimum\nTry various cutpoints and see which one gives the lowest value for the measure.\n\nChange the value of the cutpoint using the configuration box\nRepeat the instructions above: emit data and enter the value of the cutpoint. The FN * 5 + FP will be calculated automatically.\nMake a graph of that value as a function of the cutpoint. Is there a minimum? (Yes!)\n\nNote: the movable values in the graph are not connected to the cut point!\n\n\n36.0.2.2 Try other attributes\nSee how you can do putting other attributes besides radius in the tree.\nDon’t forget to help yourself out by making graphs!\n\n\n\n\n\n\n\nThe table—collapsed into “case card view”—may be hiding behind the tree. Make the tree narrower.↩︎"
  },
  {
    "objectID": "index.html#smelling-like-data-science",
    "href": "index.html#smelling-like-data-science",
    "title": "Awash in Data",
    "section": "Smelling Like Data Science",
    "text": "Smelling Like Data Science\nData Science is becoming a big deal in our society. It’s a hot profession with lots of well-paid jobs. Even if you are not a data scientist, data science is in your life. Every time you do a web search, get directions on your phone, or see a recommendation for a movie, a song, or a brand of ketchup, somebody did some data science to bring you that information. When you hear about the latest unemployment figures or the trends in income inequality, that comes from data science too. And when you hear people worry that some technological convenience will lead to greater surveillance and loss of privacy, once again, data science would make it possible.\nIt sounds like we had better learn about data science—but it must be super complicated, right? Data science involves huge data sets, and sophisticated computing techniques like machine learning and artificial intelligence. Doesn’t it take years of study—and buckets of talent—to learn that stuff?\nAs with anything, it does take years to be an expert. And experts may disagree about talent. But there are underlying ideas and ways of thinking that you can experience right now—and that’s what we hope this book will give you. We’ll use medium-sized data sets—at most a few thousand cases at a time, along with a few common-sense techniques, and a drag-and-drop data platform, to help you get an idea of what data science, for lack of a better term, smells like. When you’re done, you’ll be able to use that “sniff test” to recognize a data science problem; you’ll have a better idea what went into the data that you see and use, making you a more critical and competent citizen; and you’ll be better able to study data science in earnest, if you so desire.\nHow should we start?\nAnd even before that, what is data science, really?\nAs of the Spring of 2020, the COVID Spring, no one really knows. Everyone pretty much agrees that it lives somewhere in the borderlands between Statistics and Computer Science.\nWe can use our emerging sniff test to recognize it. We’ll use two main ideas:\n\nA data science problem often begins with a feeling of being awash in data.\nData science uses data moves to manipulate data.\n\nAwash in Data. This is a very touchy-feely thing to put in something about data science, but imagine: there is an ocean of data and you’re in a small boat, metaphorially trying to make your way. But you’re in a data storm; the sails are flapping madly and the waves are high. Data are coming in over the gunwales, threatening to swamp you. Emotionally, you might be excited by the challenge, or you might be terrified, wishing you were somewhere, anywhere else.\nIn a data science situation—especially as a beginner— you often don’t know what to do. You have way too much data, and the data you have is confusing. Even though you’re not literally in a boat, you are awash in data. This book, then, is about coping with “awash-ness”: developing skills and perspectives appropriate for doing data science. We will explore techniques for finding the patterns and stories in the ocean of data—for calming the seas and filling our sails.\nData Moves. These are techniques for coping with data in a data-science context. One example is filtering, that is, looking at a subset of your data. Looking at a subset lets you focus on one thing; it reduces the scope of the problem. It’s often a good way to take a step when you don’t know what to do. By reducing the amount of data and giving you an action to take, filtering reduces that awashness.\n\nA move like filtering is broader than the specific command you give a computer or what menu item to choose. It’s an underlying thing-to-do that is about manipulating the data. Throughout this book, we’ll use highlighted paragraphs like this one to draw attention to data moves when you do them; we hope that you will begin to recognize them for yourself, and develop the habit of asking, when things get confusing, whether filtering (or grouping, or summarizing or…) will move your investigation forward.\n\nBut there’s more! Data science is more than being awash and using data moves. Communication is a huge part of data science, and nothing is more emblematic of good data communication than a great visualization. Professional data scientists make amazing and dynamic graphics that communicate the stories behind complicated, huge datasets.\nWe will not do that here. We’re beginners, and there is plenty for us to do that uses graphs with the normal axes and points that students are aleady familiar with.\nWe have included a chapter about visualizations that describes CODAP’s graphing environment, shows some student work, and reflects on how to use data moves to make your graphics better."
  },
  {
    "objectID": "index.html#computers-coding-and-codap",
    "href": "index.html#computers-coding-and-codap",
    "title": "Awash in Data",
    "section": "Computers, Coding, and CODAP",
    "text": "Computers, Coding, and CODAP\nIt’s utterly impractical to do data science by hand. Data science deals with large amounts of data, so it’s no surprise that you will use computer technology to do this work. And although you can learn many things by reading, you will not really understand about being awash—and therefore you will not really understand data science—without using a computer to wrestle with the data yourself.\nThe vast majority of online introductions to data science begin by using a programming language, usually R or Python. These are languages used by professionals in the field, so it makes sense to use them when you are learning. If you are already proficient in coding, you could certainly start there.\nBut in this book, you will use CODAP.\nOne reason for that is that the activities here were designed for a four- or five-session introduction to data science at the high-school level. They assume no programming experience at all. Because CODAP is not a programmming language, there is no strange syntax to learn, nothing to configure, no libraries to download and install.\nIt’s a tradeoff, of course: there are many things you cannot do in CODAP that are possible when you code. Good coding also makes your work reproducible and reusable, recording every step of your analysis.\nStill, CODAP is a good place to start. It will let you experience data analysis and data moves with a minimum of overhead—and you will still do a great deal of computational thinking. Later, when you move into coding, you can focus on that, undistracted by the high winds and salt spray on that ocean of data."
  },
  {
    "objectID": "index.html#using-this-book",
    "href": "index.html#using-this-book",
    "title": "Awash in Data",
    "section": "Using This Book",
    "text": "Using This Book\nThis book is divided into parts, as you can see in the table of contents that probably appears in the sidebar on the left side of your screen. (To make the sidebar appear or disappear, click the icon with the four gray horizontal bars—perhaps a Big Mac icon?)\nIf you are here to get an introduction to data science, or you’re about to teach it, you can work sequentially through the Lessons part, referring as necessary to other parts of the book. The Data Portals part describes some of the widgets we have developed to let you get your hands on some interesting data. We put instructions about those portals in their own chapters in order to shorten the text in the lessons.\nThe next part, Data Moves, is more substantial, and contains some of our thinking about what makes data science data science. If you are designing data science curriculum, you might want to read this part.\nFinally, CODAP Structure and Features explains some of CODAP’s underlying model, as well as some more esoteric aspects of the platform and its relationship to data science.\n\nUsing CODAP for lessons and examples\nTo do work on a problem in the book, or to try out an example, there are two ways to open up CODAP and start working:\n\nLook for the CODAP icon. Then use the link to open up a new tab or window in your browser. You will need to switch back and forth between the instructions in the book and the CODAP window.\n\n\n\n:::\n\n\nCan I just use my phone?\nNo. It will work, but the screen really is just too small."
  },
  {
    "objectID": "08-arbor/08.30-arbor-with-graphs.html",
    "href": "08-arbor/08.30-arbor-with-graphs.html",
    "title": "32  Trees and graphs",
    "section": "",
    "text": "How should you decide how to build a tree? What attributes should you drag in? How do you pick a cut point?\nYou could just try things, but it often helps to make graphs: graphs that show the relationship between the attribute you’re wondering about and your target variable.\nFor example, here are two graphs of the same data showing the height of 28 professional athletes and what their sport is. In this case, all of these athletes play either rugby or basketball. That attribute, sport, is our target variable. That is, we are trying to use height to predict what sport they play.\n\n\n\n\n\n\nusing the vertical axis\n\n\n\n\n\n\n\nusing a legend\n\n\n\n\n\n\n\nTwo ways to show sport and height. As we might expect, basketball players are generally taller.\nWhich display is best? You will have to decide what works for you. Be sure you remember that you have a choice!\nWe could use either graph to decide on a cut point. It looks as if it’s something like 175 cm.\nIn the live example make a tree…\n\n…that’s predicting sport\n…that branches on height\n…at a number near 175 cm\n…and that predicts that the taller athletes play basketball, and the rest rugby."
  },
  {
    "objectID": "08-arbor/08.40-arbor-fbola.html",
    "href": "08-arbor/08.40-arbor-fbola.html",
    "title": "30  Fbola example",
    "section": "",
    "text": "A medical context can be good for learning about trees because (a) this kind of thinking is actually used in medical contexts; and (b) the thinking and terminology of trees make sense here.\nOur goal is going to be to make a diagnosis. Traditionally, “positive” means that you have the disease, and “negative” means that you do not.1\nThe tree represents a procedure for making a diagnosis. Every node represents a question or a test, and every branch an answer. When you get to the end of the tree, when you arrive at a terminal node, that node must have a diagnosis assigned to it, either positive or negative.\nClearly it’s time for an example.\nSuppose we’re at a school and there is an epidemic of the Fbola2 virus. The sympoms include a face rash and a fever. We want to evaluate students as they arrive and send them home if they are sick. Here is a tree we might use:\n\n\n\nA tree we might use to diagnose students coming to school.\n\n\nThe tree makes what you’re supposed to do totally clear.\n\n30.0.1 The same tree, with data\nThat tree was made from intuition. We can also make it with data, using Arbor. Suppose that we take 100 students and give them an expensive, time-consuming test for Fbola; those test results appear in the Fbola column. We also record whether they have a rash or a fever.\nUse the live example below to make the tree. Don’t hesitate to make graphs to see relationships between the attributes.\nYou can even consider making a different tree to accomplish the same task—by asking about the fever first.\nDon’t forget to include diagnoses at the ends of all your branches!\n\n\n\n\n\n30.0.2 Tree as model\nNotice this very very important fact: our diagnosis might be wrong. A positive diagnosis using the tree might be a true positive (TP), that is, it’s correct and you have the disease; or it might be a false positive (FP), that is, we send you home even though you are well.\nSimilarly, you can get false negatives (FN) and true negatives (TN).\nYou can see all four possibilities by looking at the table tab in Arbor. (Do that!) A typical tree, with its corresponding table, look like this:\n\n\n\n\n\n\n\n\n\nThe 14 false positives combine seven from the leftmost leaf and seven from the middle one.\n\n\n\n\n\nThis leads us to an issue we have to emphasize: how do we know whether our diagnosis is right or wrong? The answer to that is, sometimes we don’t. In this case, though, we assume that the time-consuming and expensive test is perfectly accurate. In our practical situation at school, however, we hope our rash-and-fever tree does a good enough job.\nIn general, when we have a classification problem like this, there is some underlying Truth that we cannot see. We can only see the shadow3 of this Truth, in the form of data. We see the symptoms, not the actual disease. We use the data to make our best guess about the Truth.\nAnd in fact, look at at the tree in Arbor. We are trying to predict Fbola—the results of the expensive test—using the data about rash and fever.\nTaken together, this all means that a tree is a model. It’s an approximation of the truth that we will make as useful as possible. But it’s not the Truth; it’s a human construct.\nAlso, math nerds, notice that in this model, the tree’s procedure is a function. Its inputs are the data (fever and rash) and the inevitable output is either positive or negative. Notice how this is parallel to the situation when you use a line as an approximation to data in a scatter plot. The line is a function, and it’s not completely correct even though it can be useful.\nIn that situation, we can even try to find the best line using a criterion such as least squares. And that’s whats coming next with our lessons on trees.\n\n\n\n\n\n\nYes, this is backwards from what makes sense logically. But it’s a tradition!↩︎\nLike Ebola, but not as serious.↩︎\nSo Platonic!↩︎"
  },
  {
    "objectID": "08-arbor/08.50-arbor-measures.html",
    "href": "08-arbor/08.50-arbor-measures.html",
    "title": "35  How good is your tree?",
    "section": "",
    "text": "Now we come to the issue of the quality of a tree. If you’ve made a tree, how good is it? If you have two versions of a tree, which is better?\nOf course, if a tree is perfect—it gives a correct diagnosis every time—there is no problem deciding if it’s good enough. It is. But if you’ve stayed with us this far, you have seen that the prediction a tree gives is not always correct.\nSo: to compare two trees, we need a measure of tree quality. You will pick the tree with the best value for that measure.\nThe problem is, what measure should you pick? With Arbor, you can construct various measures and then ask yourself whether the measure you choose (or design) faithfully represents what you think is important in your context.\n\n35.0.1 Constructing a measure\nLet’s look at one common measure, called the misclassification rate, which I will abbreviate MCR. That’s equal to\n\\[\\rm{MCR} = \\frac{\\textrm{number of wrong diagnoses}}{\\textrm{number of diagnoses}}\\]\nArbor can supply you with various numbers you can combine to make your measure. These include the number of true positives (\\(\\rm{TP}\\)), false negatives (\\(\\rm{FN}\\)), and so forth. With those numbers, you could calculate the MCR like this:\n\\[\\rm{MCR} = \\frac{FP+FN}{TP+TN+FP+FN}\\]\nThe plan, then, would be to make a formula for MCR—or whatever measure you want—in CODAP. But where would you make that formula? And how would you get the attributes like TP to put in it?\n\n\n35.0.2 Getting FP and TN and all that\nAs you have seen, these numbers appear at the bottom of your tree. You could copy them and type them in, but you don’t have to.\nLet’s revisit the Fbola example. The live illustration below shows a tree that uses only the rash attribute as a predictor. In the context of the example, that means we send everybody home who has a rash, and do not take their temperature.\nAs you can see from the values below the tree, among the 100 people, we have 7 false positives (FP=7) and 12 false negatives. That means that the misclassification rate, MCR, is (7 + 12)/100, or 0.19.\n\n\n\nWe don’t want to do that calculation by hand every time, so do this:\n\nClick the disclosure triangle just left of in order to export. Some controls appear. Ignore most of them!\nPress the emit data button.\n\nAha! A new table appears called Classification Tree Records. You can see that it has already calculated MCR and (scroll right…) reports values for TP, FN, and the rest as well as N (the total number) and potentially useful quantities such as the number of nodes altogether and the depth of the tree.\nLet’s change the tree so we can compare!\n\nDrag fever in and drop it on the left-hand node.\nGive the two “vacant” leaves appropriate values.\nClick emit data again.\n\nYour new table should now look like this:\n\nAccording to the MCRs—where we want a small value—the new tree is better. The sensitivity1 (sens) in the table is also better (we want it to be large).\nNow. Arbor comes with MCR and sens pre-defined. But they are only CODAP columns with formulas. This means that you can make new columns inth is table and define any possible measure of quality for your trees.\n\n\n35.0.3 Which tree made this?\nThe best for last: As you might imagine, after you’ve made a few trees and emitted the data, you might not reemmber exactly what tree created which line in the table.\nDon’t worry: Arbor has your back.\nSimply click on the row you want to know about in the Records table, and Arbor will restore that tree.\nWe will continue with this topic in a bit more depth in the next example, using breast-cancer data.\n\n\n\n\n\n\nWe will discuss sensitivity in a bit↩︎"
  },
  {
    "objectID": "08-arbor/08.80-arbor-and-ml.html",
    "href": "08-arbor/08.80-arbor-and-ml.html",
    "title": "37  Trees and machine learning",
    "section": "",
    "text": "One reason to learn about trees is that they are a topic in Machine Learning (ML), and that topic is a hot one right now. So thinking deeply about trees helps you understand what ML is all about.\nThis is not the place for a treatise on machine learning. But I will natter on for a bit describing some of the connections I see. If ML is new to you, it should give you an idea of at least one part of ML and where this fits.\nSo: what’s the connection between classification trees and machine learning?\nIf you have worked through all of the pages before this one, you’ve seen that, although you can make pretty good trees just using your intuition, there are ways to assign numbers to the quality of the trees you make, and informally optimize a tree. That is, you can make a plausible tree, and then alter it—add another branch, change a break point—and gradually find the best one.\nIf you do that enough, it probably will occur to you that this is tedious, and wonder if we could have a computer do it.\nThis business of looking at a set of trees, then finding the best one, and maybe then using that to make another, generally better set of trees, and repeating that process, well, that’s an example of machine learning. It’s creating an algorithm to find the best—or at least a pretty good—version of something.\n\n37.0.1 Imagining automating tree-building\nIt’s one thing to say, we’ll have the computer do the boring part. It’s another to specify exactly what to have the computer do. We will not do that here completely, but we will talk about what’s required for that task.\n\n37.0.1.1 The branch-everything strategy\nFirst, let’s imagine a brute-force procedure: make the biggest tree possible. To do that, take the first “predictor” attribute that you see, and branch the tree using that attribute. Now you have two terminal nodes. Then take the next attribute, and branch both terminal nodes; now you have four. Continue until you’re out of attributes. (If you start with \\(n\\) attributes, you will wind up with \\(2^n\\) terminal nodes…which can be a lot!)\nTo do that, you still need three things:\n\nAt the beginning, you need a target attribute and you need to know what value(s) of that attribute are “positive.”\nYou need to know, for each attribute, if it has more than two values, how to split it: You need a splitting rule—perhaps a cut point.\nAt the end, you need to know, for each terminal node, what diagnosis you will assign.\n\nYour algorithm will have to address those issues. Then, if you’re about to make the last branch, you will notice situations where it makes no sense to branch the tree, for example:\n\nwhen the split makes almost no difference in the percentage of cases that are positive.\nwhen the number of cases in a branch is small\n\nSo to make a “trimmer” tree, your algorithm will need to specify what “almost no difference” or “small” mean.\nYou might also look at your completed tree and decide to “prune” it—to eliminate additional branches that yield little difference or have too few cases.\n\n\n37.0.1.2 A more prudent and frugal strategy\nAn alternative to branching everything is to be more careful about what branches you make. Imagine these steps:\n\nTake all the attributes and make a tree for each one, with just that first split.\nFor each tree, calculate how good the tree is using some measure such as the MCR (misclassification rate).\nTake the best tree as measured by that MCR, returning to step 1 using only the remaining attributes.\nIf you ever get a split that’s too little difference or too small, stop branching that part of the tree.\n\nYou still need to define many things, but this will generally yield smaller, more wieldy trees.\n\n\n\n37.0.2 Linear regression metaphor\nLet’s assume we all understand about least-squares linear regression, a process by which, given a set of data points, we find a line that minimizes the sum of the squares of the residuals. We compute the parameters of that line—the slope and intercept, \\(m\\) and \\(b\\)—using some formulas. And those formulas get derived using calculus.1\nNow let’s imagine that we don’t have calculus.\nWe can still solve the problem using an iterative process. Maybe we begin using the function \\(m=0\\) and \\(b=0\\), that is, \\(y=0x+0\\). We can compute the sum of squares of the residuals. Then, at every step, we look at new values of \\(m\\) and \\(b\\), offset a little from their current values, and calculate the sum of squares from new lines defined by the new values. We pick the line with the lowest sum of squares and then do it all over again. Gradually, when we get close, we can reduce the offsets, and thereby find an optimum line to as great a precision as you like.\nYou can even imagine this as walking on a surface. The \\(m\\) and \\(b\\) axes define a horizontal plane, and the height (\\(z\\)) of the surface is the sum of squares for each location \\((m, b)\\). Our task is to find the lowest point on the surface.\nCalculus does it immediately, but we have the computing power to do the “walk” very efficiently and quickly. And of course, this procedure—which is called gradient descent in ML-speak—works even when calculus does not.\ncost: entropy, gini instead of SSR\ntraining and test sets\noverfitting\n\n\n\n\n\n\nThis is not surprising, because this is an optimization problem with quadratic functions (sum of squares) so it involves setting a derivative equal to zero…and so forth.↩︎"
  },
  {
    "objectID": "08-arbor/08.90-arbor-configuration.html",
    "href": "08-arbor/08.90-arbor-configuration.html",
    "title": "38  Configuring a node: the details",
    "section": "",
    "text": "When you press the “gear” that appears when you hover over a node, the configuration box appears. That’s where you can set the cutpoint for a numeric attribute.\n\n\n\n\n\nThe gear appears when you hover over a node.\n\n\nYou can do other useful things as well.\nYou will be able to figure this box out pretty well on your own, but just in case, this page explains all the gory details.\n\n38.0.1 Left and right\nThe double-headed arrow diamond near the lower right is a button that reverses the node. With that control, you can (for example) set up every branching so that the more “positive” result flows to the left. A tree is less confusing if the results are less mixed up.\n\n\n38.0.2 Cutpoints\n\n\n\nSample configuration box for age\n\n\nIf the attribute is numeric, you have to decide what value separates the positive from the negative values.\nEnter the value you want for the cutpoint and use the menu to choose the operator that governs which value(s) go on the left.\nNotice that you can use equality. This is useful when you want to isolate a single value for some reason.\n\n\n38.0.3 More than two categorical values\nSuppose you have four values in the columns, such as Freshman, Sophomore, Junior, and Senior. By default, Arbor picks one value for the left side, and puts the rest on the right:\n\n\n\n“Before.” Frosh are alone on the left.\n\n\nNow suppose you want to split a node by whether the cases are upper or lower class.\nJust click on a value to move it to the other side. For example, clicking on the Soph button will move it to the left side. It will look like this, although you have to edit the labels as we did:\n\n\n\n“After.” Frosh and sophomores are together on the left."
  },
  {
    "objectID": "00-intro.html#smelling-like-data-science",
    "href": "00-intro.html#smelling-like-data-science",
    "title": "Introduction",
    "section": "Smelling Like Data Science",
    "text": "Smelling Like Data Science\nData Science is becoming a big deal in our society. It’s a hot profession with lots of well-paid jobs. Even if you are not a data scientist, data science is in your life. Every time you do a web search, get directions on your phone, or see a recommendation for a movie, a song, or a brand of ketchup, somebody did some data science to bring you that information. When you hear about the latest unemployment figures or the trends in income inequality, that comes from data science too. And when you hear people worry that some technological convenience will lead to greater surveillance and loss of privacy, once again, data science would make it possible.\nIt sounds like we had better learn about data science—but it must be super complicated, right? Data science involves huge data sets, and sophisticated computing techniques like machine learning and artificial intelligence. Doesn’t it take years of study—and buckets of talent—to learn that stuff?\nAs with anything, it does take years to be an expert. And experts may disagree about talent. But there are underlying ideas and ways of thinking that you can experience right now—and that’s what we hope this book will give you. We’ll use medium-sized data sets—at most a few thousand cases at a time, along with a few common-sense techniques, and a drag-and-drop data platform, to help you get an idea of what data science, for lack of a better term, smells like. When you’re done, you’ll be able to use that “sniff test” to recognize a data science problem; you’ll have a better idea what went into the data that you see and use, making you a more critical and competent citizen; and you’ll be better able to study data science in earnest, if you so desire.\nHow should we start?\nAnd even before that, what is data science, really?\nAs of the Spring of 2020, the COVID Spring, no one really knows. Everyone pretty much agrees that it lives somewhere in the borderlands between Statistics and Computer Science.\nWe can use our emerging sniff test to recognize it. We’ll use two main ideas:\n\nA data science problem often begins with a feeling of being awash in data.\nData science uses data moves to manipulate data.\n\nAwash in Data. This is a very touchy-feely thing to put in something about data science, but imagine: there is an ocean of data and you’re in a small boat, metaphorially trying to make your way. But you’re in a data storm; the sails are flapping madly and the waves are high. Data are coming in over the gunwales, threatening to swamp you. Emotionally, you might be excited by the challenge, or you might be terrified, wishing you were somewhere, anywhere else.\nIn a data science situation—especially as a beginner— you often don’t know what to do. You have way too much data, and the data you have is confusing. Even though you’re not literally in a boat, you are awash in data. This book, then, is about coping with “awash-ness”: developing skills and perspectives appropriate for doing data science. We will explore techniques for finding the patterns and stories in the ocean of data—for calming the seas and filling our sails.\nData Moves. These are techniques for coping with data in a data-science context. One example is filtering, that is, looking at a subset of your data. Looking at a subset lets you focus on one thing; it reduces the scope of the problem. It’s often a good way to take a step when you don’t know what to do. By reducing the amount of data and giving you an action to take, filtering reduces that awashness.\n\nA move like filtering is broader than the specific command you give a computer or what menu item to choose. It’s an underlying thing-to-do that is about manipulating the data. Throughout this book, we’ll use highlighted paragraphs like this one to draw attention to data moves when you do them; we hope that you will begin to recognize them for yourself, and develop the habit of asking, when things get confusing, whether filtering (or grouping, or summarizing or…) will move your investigation forward.\n\nBut there’s more! Data science is more than being awash and using data moves. Communication is a huge part of data science, and nothing is more emblematic of good data communication than a great visualization. Professional data scientists make amazing and dynamic graphics that communicate the stories behind complicated, huge datasets.\nWe will not do that here. We’re beginners, and there is plenty for us to do that uses graphs with the normal axes and points that students are aleady familiar with.\nWe have included a chapter about visualizations that describes CODAP’s graphing environment, shows some student work, and reflects on how to use data moves to make your graphics better."
  },
  {
    "objectID": "00-intro.html#computers-coding-and-codap",
    "href": "00-intro.html#computers-coding-and-codap",
    "title": "Introduction",
    "section": "Computers, Coding, and CODAP",
    "text": "Computers, Coding, and CODAP\nIt’s utterly impractical to do data science by hand. Data science deals with large amounts of data, so it’s no surprise that you will use computer technology to do this work. And although you can learn many things by reading, you will not really understand about being awash—and therefore you will not really understand data science—without using a computer to wrestle with the data yourself.\nThe vast majority of online introductions to data science begin by using a programming language, usually R or Python. These are languages used by professionals in the field, so it makes sense to use them when you are learning. If you are already proficient in coding, you could certainly start there.\nBut in this book, you will use CODAP.\nOne reason for that is that the activities here were designed for a four- or five-session introduction to data science at the high-school level. They assume no programming experience at all. Because CODAP is not a programmming language, there is no strange syntax to learn, nothing to configure, no libraries to download and install.\nIt’s a tradeoff, of course: there are many things you cannot do in CODAP that are possible when you code. Good coding also makes your work reproducible and reusable, recording every step of your analysis.\nStill, CODAP is a good place to start. It will let you experience data analysis and data moves with a minimum of overhead—and you will still do a great deal of computational thinking. Later, when you move into coding, you can focus on that, undistracted by the high winds and salt spray on that ocean of data."
  },
  {
    "objectID": "00-intro.html#using-this-book",
    "href": "00-intro.html#using-this-book",
    "title": "Introduction",
    "section": "Using This Book",
    "text": "Using This Book\nThis book is divided into parts, as you can see in the table of contents that probably appears in the sidebar on the left side of your screen. (To make the sidebar appear or disappear, click the icon with the four gray horizontal bars—perhaps a Big Mac icon?)\nIf you are here to get an introduction to data science, or you’re about to teach it, you can work sequentially through the Lessons part, referring as necessary to other parts of the book. The Data Portals part describes some of the widgets we have developed to let you get your hands on some interesting data. We put instructions about those portals in their own chapters in order to shorten the text in the lessons.\nThe next part, Data Moves, is more substantial, and contains some of our thinking about what makes data science data science. If you are designing data science curriculum, you might want to read this part.\nFinally, CODAP Structure and Features explains some of CODAP’s underlying model, as well as some more esoteric aspects of the platform and its relationship to data science.\n\nUsing CODAP for lessons and examples\nTo do work on a problem in the book, or to try out an example, there are two ways to open up CODAP and start working:\n\nLook for the CODAP icon. Then use the link to open up a new tab or window in your browser. You will need to switch back and forth between the instructions in the book and the CODAP window.\n\n\n\n:::\n\n\nCan I just use my phone?\nNo. It will work, but the screen really is just too small."
  },
  {
    "objectID": "07.20-scrambling.html#scramble-heights-13",
    "href": "07.20-scrambling.html#scramble-heights-13",
    "title": "26  The Scrambler: Randomization Tests",
    "section": "26.3 Your first scramble",
    "text": "26.3 Your first scramble\nWe will do this slowly.\n\nor use the live demo below. It will be cramped, but it’s doable! \nYou should already see:\n\nA table with 59 cases, with Gender and Height (just after Weight to the right).\nA quantity to the left, dMeanHeights, with a value of 5.87. This is the (true) difference in mean heights between the males and females in the sample.\nA box, the scrambler, with a number of controls. Importantl, it says that it’s OK to scramble Gender, that is, everything is set up correctly.\nA blue information button in the scrambler itself. It opens a help page; you can use it later in your life when you get confused and these pages are not right in front of you.\n\nDo these steps:\n\nMake a graph of Height (vertical axis) against Gender. Put means on the graph, and verify that the difference in means is about 6 cm. Make the graph narrow. you’ll need a lot of space!\nIn the scrambler, press the 1x button. This does one scramble.\n\nA new table, measures_heights appears with one value under dMeanHeight. This value is the difference in mean heighhts in the “scrambled” dataset. But you can’t see the scrambled dataset right now! Let’s fix that:\n\nClick the show scrambled button in the scrambler. The scrambled table appears. The value for dMeanHeight should match the one in the measures table.\n\n\n\n\n\nScroll to the top of that table and verify that the Gender values are in fact scrambled!\nMake a graph of Height against Gender for that table. Put the means on that graph. You will need to scrunch up some things to make them all fit!\n\nYou should see that the means are closer together.\n\nPress 1x a few more times and see what happens. Notice that when you do, the measures_heights table records the new value of dMeanHeight.\nGraph that dMeanHeight from measures_heights.\nPress 10x a few times to get more points in that graph.\n\nI did this for a total of 100 measures. My measures graph appears below. Yours will be different, of course, but should look more or less the same:\n\n\n\nResults from 100 scrambles. The “true” value of dMeanHeight is 5.87 cm\n\n\nThis is the payoff: In the graph, you can see that at most three points are as extreme as the real data, the 5.87.\nThat is, if there were no difference bewteen boys’ and girls’ heights, we might see differences this large in 3% of our samples.\nThat small value (called the \\(P\\) value) makes us confident that the mean height for all 13-year-old boys really is greater than the mean height for the girls.\n\n\n\n\n\n\nDo you need to see the scrambled dataset?\n\n\n\nNo. It helps when you’re first learning so you can see what’s going on, but once you get the hang of this, you only need to see the measures.\nThis will save a lot of screen space!"
  },
  {
    "objectID": "07.30-bootstrap.html#thinking-through-bootstrapping",
    "href": "07.30-bootstrap.html#thinking-through-bootstrapping",
    "title": "27  Randomization and Estimation: the Bootstrap",
    "section": "27.1 Thinking through bootstrapping",
    "text": "27.1 Thinking through bootstrapping\nSuppose you measure the weights of 10 capybaras and you find that their mean weight is 47 kilograms. If these capybaras are a suitably random sample of all capybaras, what can you say about the mean weight of all capybaras?\nClearly our best guess is 47 kilos as well. But will it be exactly 47 kilograms? Of course not. Capybaras vary in weight, and there’s no way we accidentally, randomly, got 10 capybaras that were perfectly representative of the whole species.\nThe big question is, how far off of the population mean is the mean of our single sample likely to be?\nTo find out, we do something that kinda sorta smells like what we did with randomization tests and scrambling.\nHere’s the plan:\nWe write the weights of all ten capybaras on chips and put them into a bag. Then we draw out ten chips, record the numbers, and find the mean. However, we put each chip back after we draw it. That is, we are sampling with replacement. This is called a “bootstrap sample.”\nBecause we put them back every time, we’ll probably get some duplicates. Similarly, some of our capybaras won’t be represented in a particular bootstrap sample. Therefore the means we get won’t be exactly the same as the mean of our “real” sample. Some means will be larger, some smaller, depending on which chips we pulled.\nWe will do this many times, and then look at the distribution of means in order to assess how far off our single-sample mean is likely to be."
  },
  {
    "objectID": "07.30-bootstrap.html#your-first-bootstrap-trader-joes-tangerines",
    "href": "07.30-bootstrap.html#your-first-bootstrap-trader-joes-tangerines",
    "title": "30  Randomization and Estimation: the Bootstrap",
    "section": "30.2 Your first bootstrap: Trader Joe’s tangerines",
    "text": "30.2 Your first bootstrap: Trader Joe’s tangerines\nWe don’t have measurements of capybaras, but we do have one bag of 18 Trader Joe’s tangerines, measured by students at Lick-Wilmerding High School in San Francisco in 2017.\nThese data appear in a live example below, with a graph of the weights, also showing the mean, which is a little under 53 grams.\n\nMake a new column called meanWeight and compute the mean of weight. It should be 52.94 g.\nNow we want to get the bootstrap plugin. As of May 2023, it is not in the Plugins menu, so we will do the following:\n\nChoose Import… from the hamburger menu.\nClick URL and enter https://codap.xyz/plugins/bootstrap/ into the box.\nClick IMPORT. The bootstrap plugin appears.\n\nbootstrap will complain that it needs a measure. You have one: meanWeight! Drag it to the left.\n\nYou are ready to strap on your boots. Your workspace should look something like this:\n\n\n\n\n\nPress the 1x button to do a single bootstrap resample. A new dataset appears, called measures_basic tangerines with one value of the mean under the meanWeight column (on the right). It will probably not be the same value as the original, which makes sense.\n\n\n\n\n\n\nOne measure. My first bootstrap mean was 52.98.\n\n\n\nGraph meanWeight. Collect more samples. They should center roughly around the true mean, 52.94.\nKeep collecting so you have a total of 200.\n\nThis distribution should be pretty symmetrical, and centered on the original mean.\nThink about what each point represents: a mean of 18 values drawn from a distribution equal to that of the real data. That is, if all groups of TJ tangerines are statistically the same as the 18 in our bag, each one of these values is a possible mean.\n\n\n\n\n\n\nYou could look at the samples…\n\n\n\nTo see the last bootstrap sample,\n\nGo to the Tables menu.\nChoose bootstrapSample_basic tangerines\nGraph weight and compare it to your original data. Can you see the holes? The duplicates?\n\n(We did this when we scrambled the heights of 13-year-olds)\n\n\nAll of the means in our graph are possible, some values are more likely. We have 200 values; let’s find the “middle” 90%. We will exclude 10 cases on either end of the distribution.\n\nIn the ruler menu/palette, press Movable Value and Add twice. You now see a range.\nAlso in the ruler, check Count and Percent.\nMove the values to get as close as you can to 10 (or 5%) in each tail.\n\nYou should have a graph more or less like mine:\n\n\n\n\n\nWith this technique, we have created what I call a “plausibility interval.” My best guess for the mean weight of all Trader Joe’s tangerines is 52.94 grams. But I would not be at all surprised if it were anywhere in the range from 52.11 to 53.84. If somebody gave me a bag of 18 tangerines, and their mean weight was 55, I would think that maybe they were from another store, or that maybe TJ had changed supplier (since 2017, almost certain!), or something else that made these new tangerines systematically different from the ones in the original bag.\nNow notice, looking back at the original data, that a single tangerine of 55 grams is not surprising at all! Here are the two graphs together, scaled the same:\n\n\n\nBelow, in blue: the original data. Above, in tangerine: mean weights from 200 bootstrap samples.\n\n\nThis makes some kind of sense. Looking at the original data graph, we see that if we pulled a 55-gram tangerine out of a TJ bag, chances are good that most of the other 17 will be lighter.\nWe could spend a great deal of time right here on why the distribution of means is so much narrower than the distribution of the original data. But that is the province of a more formal stats class.\nIt does, however, speak to our goal of developing a taste for stability. The tangerines have a weight range of about 53 ± 4 grams, but a bag of 18’s mean is within about 53 ± 1 gram. The mean of a large sample is stable in a way that the individual data are not."
  },
  {
    "objectID": "07.30-bootstrap.html#bootstrap-tangerines",
    "href": "07.30-bootstrap.html#bootstrap-tangerines",
    "title": "27  Randomization and Estimation: the Bootstrap",
    "section": "27.2 Your first bootstrap: Trader Joe’s tangerines",
    "text": "27.2 Your first bootstrap: Trader Joe’s tangerines\nWe don’t have measurements of capybaras, but we do have one bag of 18 Trader Joe’s tangerines, measured by students at Lick-Wilmerding High School in San Francisco in 2017.\nThese data appear in a live example below, with a graph of the weights, also showing the mean, which is a little under 53 grams.\n\nMake a new column called meanWeight and compute the mean of weight. It should be 52.94 g.\nNow we want to get the bootstrap plugin. As of May 2023, it is not in the Plugins menu, so we will do the following:\n\nChoose Import… from the hamburger menu.\nClick URL and enter https://codap.xyz/plugins/bootstrap/ into the box.\nClick IMPORT. The bootstrap plugin appears.\n\nbootstrap will complain that it needs a measure. You have one: meanWeight! Drag it to the left.\n\nYou are ready to strap on your boots. Your workspace should look something like this:\n\n\n\n\n\nPress the 1x button to do a single bootstrap resample. A new dataset appears, called measures_basic tangerines with one value of the mean under the meanWeight column (on the right). It will probably not be the same value as the original, which makes sense.\n\n\n\n\n\n\nOne measure. My first bootstrap mean was 52.98.\n\n\n\nGraph meanWeight. Collect more samples. They should center roughly around the true mean, 52.94.\nKeep collecting so you have a total of 200.\n\nThis distribution should be pretty symmetrical, and centered on the original mean.\nThink about what each point represents: a mean of 18 values drawn from a distribution equal to that of the real data. That is, if all groups of TJ tangerines are statistically the same as the 18 in our bag, each one of these values is a possible mean.\n\n\n\n\n\n\nYou could look at the samples…\n\n\n\nTo see the last bootstrap sample,\n\nGo to the Tables menu.\nChoose bootstrapSample_basic tangerines\nGraph weight and compare it to your original data. Can you see the holes? The duplicates?\n\n(We did this when we scrambled the heights of 13-year-olds)\n\n\nAll of the means in our graph are possible, some values are more likely. We have 200 values; let’s find the “middle” 90%. We will exclude 10 cases on either end of the distribution.\n\nIn the ruler menu/palette, press Movable Value and Add twice. You now see a range.\nAlso in the ruler, check Count and Percent.\nMove the values to get as close as you can to 10 (or 5%) in each tail.\n\nYou should have a graph more or less like mine:\n\n\n\n\n\nWith this technique, we have created what I call a 90% “plausibility interval.” My best guess for the mean weight of all Trader Joe’s tangerines is 52.94 grams. But I would not be at all surprised if it were anywhere in the range from 52.11 to 53.84. If somebody gave me a bag of 18 tangerines, and their mean weight was 55, I would think that maybe they were from another store, or that maybe TJ had changed supplier (since 2017, almost certain!), or something else that made these new tangerines systematically different from the ones in the original bag.\nNow notice, looking back at the original data, that a single tangerine of 55 grams is not surprising at all! Here are the two graphs together, scaled the same:\n\n\n\nBelow, in blue: the original data. Above, in tangerine: mean weights from 200 bootstrap samples.\n\n\nThis makes some kind of sense. Looking at the original data graph, we see that if we pulled a 55-gram tangerine out of a TJ bag, chances are good that most of the other 17 will be lighter.\nWe could spend a great deal of time right here on why the distribution of means is so much narrower than the distribution of the original data. But that is the province of a more formal stats class.\nIt does, however, speak to our goal of developing a taste for stability. The tangerines have a weight range of about 53 ± 4 grams, but a bag of 18’s mean is within about 53 ± 1 gram. The mean of a large sample is stable in a way that the individual data are not.\n\n\n\n\n\n\nDanger! Do not overstep!\n\n\n\nAll of the caveats we made in the scrambling chapter apply here as well.\nFor example, it is wrong to say that there is a 90% chance that the true mean falls in the range we described above.\nAll we can say is that if our bag is perfectly representative of the distribution of all tangerines, then samples of 18 drawn from that same distribution will have means in that interval about 90% of the time.\nGot that? It’s easier just to say that it’s plausible that the true mean is in the interval. How plausible? You’re 90% confident.\nIf you need something more official, learn about confidence intervals, but even then, watch out: they don’t mean what people often think either!"
  },
  {
    "objectID": "07.30-bootstrap.html#commentary",
    "href": "07.30-bootstrap.html#commentary",
    "title": "27  Randomization and Estimation: the Bootstrap",
    "section": "27.3 Commentary",
    "text": "27.3 Commentary\nThe one most important thing?\nWith real data, the numbers you calculate or measure are uncertain. Therefore, always consider reporting a range rather than a single number.\nThe Big Dogs are getting better and better at this. Polling results, for example, almost always have a margin of error.1 But how do you display that in a graph? There are a number of choices, none of which are easy to do in CODAP. But you can read about then on this interesting site from PBS about using data in news reporting.\nThe next thing:\nThat uncertainty (generally) goes down as your sample size increases. More people in your sample, more tangerines in your bag, the more certain you are about averages or other calculated values such as sums, proportions, medians, etc.\nCorrespondingly, if you break a sample down into groups, each group is smaller than the whole, so the group uncertainty is inevitably larger (i.e., worse) than the whole sample’s uncertainty.\n\n27.3.1 How close is our plausibility interval to a real confidence interval (CI)?\nYou can calculate a CI and compare.\n\nBring up a CODAP document with a plausibility interval, such as the one you made for tangerines, earlier in this chapter.\nMake a 95% plausibility interval, that is, make each tail 5%. If you have 400 samples, that’s 10 cases in each tail.\nIn the original data file, you have already computer the mean. Make three new attributes; let’s call them CI (for the half-width of the interval), low for the lower bound (which is the mean minus CI), and high for the upper bound (mean plus CI).\nThe value for CI is 1.96 times the standard error, so the formula would be 1.96 * stdErr(weight) if you’re doing tangerines.\nCompare your movable values with low and high.\n\nMy graph and table look like this:\n\n\n\nBy the way: the movable values’ percentages are listed as 3%. But we know they’re each 2.5%. CODAP doesn’t do parts of percents in the percentage display…but that’s also why it’s important to show the count."
  },
  {
    "objectID": "21.10-census-acs.html#the-small-portal",
    "href": "21.10-census-acs.html#the-small-portal",
    "title": "35  The Census/ACS Data Portals",
    "section": "35.1 The “small” portal",
    "text": "35.1 The “small” portal"
  },
  {
    "objectID": "21.10-census-acs.html#the-big-portal",
    "href": "21.10-census-acs.html#the-big-portal",
    "title": "35  The Census/ACS Data Portals",
    "section": "35.2 The “big” portal",
    "text": "35.2 The “big” portal\n\n\nYou can also access this portal from any CODAP document by choosing Microdata Portal in the Plugins menu."
  },
  {
    "objectID": "21-portal-part.html",
    "href": "21-portal-part.html",
    "title": "Data Portals Overview",
    "section": "",
    "text": "The next few chapters describe some data portals you can use to get data for analysis. In a CODAP data portal, you can pick what attributes you get, which cases you will choose from, and/or haw many cases you will download.\nWe use these portals because some datasets are so huge that it’s impractical to work on all the data at once. The BART data portal, for example, has over 40 million cases. The portals also help you get clean, pre-organized data, so that you can get to the data analysis more quickly.1\n\nMaking these choices is also a data move in itself: it’s filtering in advance.\n\n\nCensus/ACS data portals\nThese two portals—used in the assignments and the project in the lessons—supply you with data about individuals from the United States Census and the American Community Survey (ACS, also part of the Census).\n\n\nBART data portal\nThis portal is a window into public transit in the San Francisco Bay Area. You get ridership numbers for every hour beween every pair of stations—for four years, from 2015 to 2018 as of this writing.\nCan you see the commute in this graph?\n\n\n\nFive days of data from Orinda to Embarcadero Station\n\n\n\n\nNOAA data portals\nThis chapter describes two related portals that get you data direct from the National Oceanic and Atmospheric Administration (NOAA).\nYou pick weather stations and retrieve, for example, temperatures or precipitation. You can get daily or monthly values.\n\n\n\nLos Angeles and San Francisco: average monthly temperatures, 2010–2019.\n\n\n\n\nNHANES data portal\nRemember the data where we got the heights of 800 5- to 19-year-olds? It came from this portal. With the portal, you can change how many people you get, and what information you retrieve.\nDoes it really include data about having sex? You bet:\n\n\n\nFor 300 people over 30, how old they say they were when they first had sex.\n\n\n\n\n\n\n\n\nIn real, professional data science, getting the data in shape for an analysis is often most of the work. There is still plenty for you to do here in organizing your data; making the portals is part of a reasonable balance for learners.↩︎"
  },
  {
    "objectID": "21.20-BART.html#the-basics",
    "href": "21.20-BART.html#the-basics",
    "title": "36  The BART Data Portal",
    "section": "36.1 The basics",
    "text": "36.1 The basics\nThe data portal—the thing you interact with—lets you specify the stations and the day and hour. You will press a button to get the data, and it will be sent directly into CODAP so you can analyze it\nTry this in the live illustration below:\n\nLook at the panel; notice what day we’re looking at and what stations (Orinda and Embarcadero) we have specified by default.\nPress the get data button. A table will appear with data.\nMake a graph. Put when on the horizontal axis and riders on the vertical.\nNow: Orinda is a suburb. Embarcadero is in downtown San Francisco. What’s going on in the graph?\n\n\n\n\n\nHere are more things to do. This will further orient you to the data and its possibilities:\n\nAs you probably figured out, the graph shows people going to work. Let’s see them coming home. Press the swap button to exchange Orinda and Embarcadero. You should see this:\n\n\n\n\nAfter swapping Orinda and Embarcadero.\n\n\n\nPress get data. The return trips appear in the table and in the graph.\nIt would be great to color-code the points. Plop startAt into the middle of the graph.\nLet’s get data from other days! Change the date to the next day (probably April 19, 2018) and change how much data? to 7 days.\nPress get data. A whole week of data appears.\nNow drop day into the middle of the graph.\nInterpret what you see!\n\nMini-commentary: Notice that your understanding of how the world works informs how you interpret this dataset and the graphs you make. Knowing about weekends, for example, explains why there’s such a big dropoff in ridership."
  },
  {
    "objectID": "21.20-BART.html#more",
    "href": "21.20-BART.html#more",
    "title": "36  The BART Data Portal",
    "section": "36.2 More!",
    "text": "36.2 More!\n\nLimits to downloads\nThe controls for getting data are fairly self-explanatory, but a few things bear noting:\n\nThere is a limit to how much data you are allowed to get in one request. As a consequence, you may have to be strategic in the data you get. For example, a request for data from Embarcadero to any station gets (of course) fifty times as much data as a request to a single station. So while you can get a whole day of that, you can’t get a week or a month in a single request. That’s fine; being prudent about data is part of data science!\nTo use the between any two stations option, you have to restrict the hours to get only a single hour. (Otherwise it’s too much data.)\nIf you make multiple requests, and they overlap, you will wind up with two copies of the common data.\n\n\n\nThe thing about time\nEvery case has several different attributes for time:\n\nwhen is a “date-time”, where the time is the beginning of the hour the data are from.\nday is the day of the week, which is categorical but will appear ordered correctly.\nhour is an integer, the hour, in a 24-hour system.\ndate is the day (as a date) without the time. It’s categorical.\n\nThese nearly-synonymous attributes help you make different kinds of comparisons. For example, if you want to explore weekly commute patterns, you might make a graph like this, using when for the time:\n\n\n\nOne week of data from Orinda to Embarcadero.\n\n\nBut if you want to overlay the days on top of one another, you should use hour. In this graph, we have selected Friday; you can see that not only do fewer people commute, they do so later:\n\n\n\nThe same week of data from Orinda to Embarcadero, but overlaid with Friday selected.\n\n\nWe made these attributes as a convenience for you as a data-science learner. When you bring in your own data, it will probably be in one format, ansod you will have to do all these transformations yourself. If you need to know more, there’s a whole chapter on the issue of dates and times."
  },
  {
    "objectID": "21.20-BART.html#bart-suggestions",
    "href": "21.20-BART.html#bart-suggestions",
    "title": "36  The BART Data Portal",
    "section": "36.3 Suggestions for investigations",
    "text": "36.3 Suggestions for investigations\nHere are some things you can investigate with the BART data:\n\nWhen people take BART from SFO, where do they tend to go?\nMr Erickson thinks that people tend to leave work early on Friday. True? Myth? Does it depend?\nFrom which station do the most people come downtown during the morning commute?\nWithout looking at a schedule, find a day game in 2015 when the Giants were playing at home at AT&T Park. (What station(s) are relevant? Embarcadero and Montgomery.) All you’re trying to do is find a date with a day game. Then check to see if you’re right at this site; get the box score, it has the starting time.\nA’s fans: do the same, check at this site. (Coliseum station.)\nTry to estimate the total number of people who went to Civic Center Station for Pride, June 28, 2015. Find the date for Pride 2018 without looking it up and do the same.\nWhat other events or phenomena can you investigate? Think of one, study it! (Could be a one-time event, a repeating event, or something that happens every day, or…)\nPlay the secret meeting game, described below."
  },
  {
    "objectID": "21.20-BART.html#the-secret-meeting-game",
    "href": "21.20-BART.html#the-secret-meeting-game",
    "title": "36  The BART Data Portal",
    "section": "36.4 The secret meeting game",
    "text": "36.4 The secret meeting game\nA secret meeting is being held weekly near a BART station. Find it!\n\nYou know the meeting is on Tuesdays, you know it’s at Hayward, and you know that 160 people attend. But you don’t know the time. Collect data and make a display (or displays) to support a convincing argument that you know the time. Convince your neighbor!\nLet’s play that again. Go to the options panel. If necessary, abort the game. Then set Thursday, 160, and 12 noon, but choose surprise me for the location. Then solve the problem. How did you do it? What data did you collect, and why? (Remember, the possibilities for location are Orinda, Hayward, San Bruno, and Pleasant Hill.)\nAgain, but this time,\n\nReduce the size of the meeting\nSet surprise me for two of the remaining parameters"
  },
  {
    "objectID": "21.20-BART.html#commentary",
    "href": "21.20-BART.html#commentary",
    "title": "36  The BART Data Portal",
    "section": "36.5 Commentary",
    "text": "36.5 Commentary\nBesides giving you a chance to interact with a truly huge dataset (albeit only a little at a time), the BART data is a great environment for learning how to think about data in order to get what you want.\nSuppose you decide you’re going to investigate the Giants home game against the LA Dodgers on Tuesday, April 21, 2015. You want to know how many people took BART to the game. How can you figure that out?\nYou’ll be looking at people going to Embarcadero and Montgomery stations. You expect to see a bump in the ridership. But at what times? You should probably look at the data to see when the bump is. But what data? To Embarcadero and Montgomery, but from where? Everywhere, right?\nBut if you do that, you’ll have to add up the data from different stations. Sounds like dragging left and making a new column. But what do you drag left? The station? No.\nTo decide, one strategy is to ask yourself, “What graph do I want to make? What will it look like?”\nThe one in my head has a bump, with riders on the vertical axis and hour on the horizontal. That suggests that we want hour on the left side of the table. Then when we add, we’ll get the sum of the riders—for each hour. Yes. That sounds right.\n\n\n\nSum of riders (called total) from anywhere to Embarcadero for Tuesday afternoon, April 21, 2015.\n\n\nYou can see that there are a slew of similar problems to solve. None of them are mathematically sophisticated, but they can all be challenging and confusing. Here are three:\n\nHow do you add the people from Embarcadero and Montgomery together?\nHow do you account for the people who would have been riding BART anyway whether there was a game or not?\nHow do you know if the bump is for a game or for something else?\n\n\nAs you think about these, notice that the solutions use data moves:\n\nfiltering to get the right data,\ngrouping to set up appropriate comparisons, and\nsummarizing (or aggregating) and calculating to add up the riders and subtract out the “background” traffic.\n\n\nThe very idea of the “background” is an interesting topic. It smells very sciency, is hardly ever talked about in ordinary classwork, and requires only common sense to understand and cope with. To tell how many people went to the game on this Tuesday, April 21, 2015, we need to compare the pattern of ridership to some day when there was no game. Does it matter which day we compare? It might. Maybe it would be best to choose a different Tuesday. But we can’t be sure; maybe we should compare a few different non-game days. And so forth.\n\n\n\nSum of riders from anywhere to Embarcadero for three Tuesdays in April..\n\n\nThis is also closely related to the idea of controlling variables, which we talked about in the commentary about gender differences in income (xxx)."
  },
  {
    "objectID": "21.30-noaa-weather.html#the-little-one-by-tim",
    "href": "21.30-noaa-weather.html#the-little-one-by-tim",
    "title": "37  The NOAA Weather Portals",
    "section": "37.1 The little one (by Tim)",
    "text": "37.1 The little one (by Tim)\n\nxxx replace this graphic with a newer one\nThis plugin will let you choose…\n\nfrom a small selection of diverse stations, with a leaning towards California (because it was created for a workshop in the LA area);\nwhether the data are daily or monthly; and\nfrom among a small selection of data, e.g., precipitation or average temperature.\n\nThe software lets you choose dates back to roughly 1900.\n\n\n\nThe ‘little’ NOAA data portal\n\n\nThen there’s this thing about a “spreader.”\nThis is because we have been exploring a concept in data organization called “tidy data.” I will not go into it here; suffice to say:\n\nIf you get only one type of data, you will not have a problem. Just put value on the vertical axis.\nIf you get two temperatures (e.g., tMin and tMax) do the same, and plop what into the middle of the graph.\nIf you mix the data, like you get precipitation and temperature, your graphs will be unruly unless you use the spreader. Experiment, write me an email, or just use the bigger, more modern NOAA data portal!"
  },
  {
    "objectID": "21.30-noaa-weather.html#the-big-one-by-concord",
    "href": "21.30-noaa-weather.html#the-big-one-by-concord",
    "title": "37  The NOAA Weather Portals",
    "section": "37.2 The big one (by Concord)",
    "text": "37.2 The big one (by Concord)\nWhen the nice people at Concord Consortium saw my NOAA Portal they realized they needed something like it for a project they were doing. So they took my code and expanded upon it.\n\n\nYou can also access this portal from any CODAP document by choosing NOAA Weather in the Plugins menu.\n\nThere are differences:\n\nVery cool: It has a map you can use to choose from among a zillion stations.\nYou can only get one station at a time.\nIf you want more than one measurement, they appear in different columns. This is usually what you want. Also, this means you do not have to worry about tidy datasets or spreading.\nThe data go back, apparently, only to 1970.\nI have had trouble entering dates sometimes.\n\n\n\n\n\nThe ‘big’ NOAA data portal. Mt Washington in New Hampshire is selected"
  },
  {
    "objectID": "21.40-nhanes-portal.html",
    "href": "21.40-nhanes-portal.html",
    "title": "38  The NHANES Data Portal",
    "section": "",
    "text": "This brief chapter describes a data portal that gives you access to several thousand cases from the 2003 National Health And Nutrition Examination Survey (NHANES). This is the dataset where we got the 800 children and teens we used in two of the lessons in this book.\n\n\n\n\nThe attributes panel in the NHANES portal.\n\n\nA box at the top of the portal lets you choose your sample size, and a button will get your data. When the data downloads, a CODAP table appears with the data in it.\nThe portal has three panels:\n\nsummary: This describes what you have chosen to download. It includes a list of the attributes you have chosen.\nattributes: (Shown in the illustration.) Here you can choose which attributes you want. You can pick them from several different broad categories. This is similar to the Census/ACS portal design you have already used.\ncases: This lets you limit the range of ages of the people you will get. You do not have to fill in both boxes."
  },
  {
    "objectID": "04.70-adjustment-and-control.html#control-through-filters-and-grouping",
    "href": "04.70-adjustment-and-control.html#control-through-filters-and-grouping",
    "title": "21  Adjustment and Control",
    "section": "21.2 Control through filters and grouping",
    "text": "21.2 Control through filters and grouping\nLet’s begin by slicing or filtering as a way to control variables. We’ll look back at income data using some of the same thinking we used when we looked at gender and income.\n\n21.2.1 The setup: How much is college worth? A first pass\nThis time, we’ll do something parallel. If you get a bunch of Census data (for example, from the Microdata Portal), You will find that people with more education have a higher income.\nHow much higher? We can calculate that in dollars—though, as we will see, that’s not the only way.\n\nI used the Microdata Portal to collect 1000 Americans from 2017. Since I am interested in income and education, I make sure to include those attributes.1\nThe vanilla sample has people of all ages, so I set aside everyone who was not bewteen 25 and 55. Notice that this is a filtering data move.\n\nGraphing education against income shows, unsurprisingly, that people with more education earn more. But it’s too many catgories to deal with right now, so…\n\nI then made a new attribute, College, to indicate whether the person had any college. Each person got the value some or none. This is a calculating data move, even though I did not write a formula.\nThen I made a graph of College by Income-total (implied grouping), did the drag-left dance (grouping), and showed the medians (summarizing) to get my result:\n\n\n\n\nIncomes in 2017 for people with any college (median $35000) and with no college (median $22150)\n\n\n\n\n\n\n\n\nYikes!\n\n\n\nIf you are shocked by how low those numbers are, yes, it’s surprisingly bad. There are mitigating factors, however:\n\nIf you’re living in a wealthier area, note that this is a national random sample.\nWe have not filtered out people who are not working. That makes a difference, but not as much as you might think.\n\nTrying to fix those problems is a job for the a more nuanced extension of this work. In class, this would be a “dig deeper.”\n\n\nIt seems, using this naïve approach, that going to college is worth $12850 a year. We will come back to this in a moment.\n\n\n21.2.2 Extension: let’s look at 1960 as well\nI wondered what we would find in 1960.\nUsing the Microdata Portal, I can get exactly the same data, but from that year. In the plugin, I change the year and check the box for Keep existing data.\nNow I need to do the age filter and set the values of College for the new data. This is straightforward, though it’s interesting that the education categories in 1960 are not the same; I can still tell who went to college, however.\nThe simple results show the problem:\n\n\n\nMedian incomes (shown as bars!) for people with some college and no college, in 1960 and 2017\n\n\nIn 1960, by the same measure, college was worth $1300 per year. But, but… the scales of the two graphs are so different that you can’t really see a fair comparison.\nWe could look up the consumer price index and calculate the four values in “2017 dollars.” But we will save the CPI for the next example.\nInstead, let’s calculate the number that fills in the blank:\n\nFor every dollar a person with college makes, a person without college makes only ___ cents.\n\nAnd that’s easy to do: just divide 22150/35000 (which is 0.633) and 2250/3550 (which is 0.634). So for both years, those who didn’t go to college made only 63¢ for every dollar people who did go to college made.\nThe takeaway: It was hard to compare the situation in 2017 to the one in 1960, so we had to make an adjustment to the data, or the way we presented it, to make a fair comparison.\nFurthermore, you can often use a ratio to adjust things instead of a straight difference.\n\n\n21.2.3 Extending further\nIt was easy to do those divisions usng a calculator (or just Google). But suppose we want to take our study further. Perhaps\n\nwe want to remove the people who are not in the labor force; and/or\nwe want to include 1980 and 2000, to see more of the trends; and\nwe want to include the 63¢ values in a graph.\n\nIf we do that, pretty soon we’re making too many hand calculations. We should get the computer to help.\nYou know how to do most of these things already (or you can look them up in this book!). But you might not immediately see why we need another layer in the hierarchy, and you might need some help with the formulas.\nSo we’ll give you a break and start you out with the dataset at this point, with 2017 and 1960.\n\nor use the live example below.\nFirst, the layer issue.\nThe file currently has a bottom layer, with the people, and an upper layer, where the people are grouped by College. In the Colleges layer, we have the median income for people with and without college. Think about how that makes sense: these medians “belong” to particular values of College, they’re the medians for the cases some and none.\nNow we want to have a number that applies to the year. We want the “number of cents” thing attached to 2017 and 1960. Furthermore, the medians for some and none are inside these years.\nThat means we need a new level for year, above the Colleges level, and we will make our new attribute comparing college and no-college there.\n\nDrag Year left to make a new level.\nMake a new attribute there, called cents.\nGive cents a formula to perform this calculation. Here is one possibility, wordy but effective:\n\n100 * median(`Income-total`,College='none') / \nmedian(`Income-total`,College='some')\nYou should see “63” for both Years. Hierarchy works perfectly for us here; the formula does not need to mention the year, because it only applies to the cases attached to that cell in the hierarchy, so that the 1960 value for cents ultimately includes all the people from 1960, and only those people.\n\n\n\nNow we can continue:\n\nCollect 100 people from 1980 and another thousand from 2000.\nSet aside all the people under 25 or over 55.\n\nNotice the numbers in cents. The formulas should work for all four values of Year.\n\nMake a graph of cents against Year. You may need to re-order the (categorical) Years.\nSet aside all people who are not Employed. (Another filter move)\n\nIf your experience is like mine, you’ll get values between about 55% and 73%, with no obvious trend. I wondered if I had made some mistake, so I did it over again from the start.\nIt’s much easier the second time! Especially because you can get all the data first, and do all the filtering and recoding once. And you will probably get different results, as I did. A similar range, but a different pattern.\n\n\n\n\n\nTwo different samples, #6 and #8, split by College. Notice how the medians for some and none are over $1000 different in the different samples.\n\n\nIn fact, my 1980 data had a value of cents 10¢ different the second time through. And this with a sample of over 50 people in each category! I had imagined that the median value would be mroe stable—that it would not change as much with a new sample.\nOne takeaway: once you have done a compicated CODAP analysis, it’s often pretty easy—and illuminating—to redo it completely.\n\n\n21.2.4 Looking more deeply\nAre you disappointed that non-college people always seem to make about 60% of what college people make (in the median)? I was. I had thought that we would see an increasing pay gap between the hoity-toity intellectuals (like myself) and the people who do the real work in this country.\nBut even a “negative result” is useful.\nFor one thing, you can use the same data to explore other aspects of social change. For example, what propoprtion of people in this age range have a college education, as a function of year? All you need is a new column and a new formula. (Use the function count() to get the number of cases in a dataset.)\nThis dataset also has Sex, which means that you can track gender differences. You should predict first:\n\nBetween 1960 and 2017, how did the proportion of women going to college change?\nBetween 1960 and 2017, how did the proportion of women in the workforce change?\nIs that last figure any different when you compare collegiate women to non-?\n\nAn investigation like this would be a fine student project. It would not take too long, and there is a lot you can do with it. Many opportunities to dig deeper.\nAnd notice how many data moves you have to use. It gives the students plenty of practice—and possible entries in their data moves portfolio!"
  },
  {
    "objectID": "04.70-adjustment-and-control.html#home-prices-and-cpi",
    "href": "04.70-adjustment-and-control.html#home-prices-and-cpi",
    "title": "21  Adjustment and Control",
    "section": "21.4 Home prices and CPI",
    "text": "21.4 Home prices and CPI\nOver the years, houses get more expensive (at least that has been the experience during most of our lifetimes). And at the same time, prices for everything go up. It’s inflation, right? So the question is,\n\nHow does the change in home prices compare with inflation?\n\nFor our first pass though these data, we will use median home price data from Wisconsin2 and the Consumer Price Index (CPI).\nWhat we will find—and this is typical—is that setting up the data takes a bunch of work and hard thinking, and then the actual math is a single formula that once we have it, we’re done.\n\n\n\n\n\n\nJust like calculus\n\n\n\nCalc students and teachers: the next time you do a typical word problem, keep track of how much of it is the setup—understanding the geometry, drawing the diagram, converting the situation into algebra, and all that—and how much is actually doing calculus.\n\n\n\n21.4.1 The setup\nThis time I will not give you step-by-step instructions for the setup, but here are the basic, big-picture steps—with hints.\n\nData\nYou need two files: one with CPI data and one with real estate prices. Here are download links, though they only go up to March 2023. You can go online and get newer material!\n\nDownload Wisconsin housing data\nDownload Monthly CPI data\n\nDrag these two files into CODAP, and you will get two tables, one for each .tsv. (That’s “tab-separated value”, by the way, and CODAP understands it just fine. It’s better than using commas here, because some date formats include commas.)\nThe CPI dataset has, for every month since 1900, the overall urban CPI, which is what news reporters usually use. The actual CPI site at the Bureau of Labor Statistics is an amazing repository of great information, including the ingredients of the CPI. All the way down to the monthly price of eggs.\nThe CPI number is arbitrary but consistent. It had a value of about 100 in January 1984.\nThe real estate data from Redfin has month and price, where “price” is the median sale price of a house in thousands of dollars.\nGraph these Price and CPI to see that, in fact, the CPI generally goes up, and so does the price of a house in Wisconsin, although there’s an annual fluctuation.\n\n\n\n\n\n\nThe date format problem\n\n\n\nI kinda implied that I was letting you solve all the data-munging problems. Too late, I realized that the .tsv files you download already have the date formats fixed.\nIf you go out and get your own files, you will have to do that. Remember how? Open the raw download in Google Sheets (or some other capable spreadsheet) and use their vast formatting power to reformat the dates.\nHere is the section in the dates chapter if you want to review.\n\n\n\n\nThe join and the wrinkle\nIf you remember the chapter on the joining data move, you might look at the two files and think, “fantastic! I get to do a join! All I have to do is drag Date from the CPI dataset and drop it onto the Month column in the real estate, and the real estate table will now have a column with the CPI!”\nAbsolutely, except that when you try it 3 nothing happens. It should work, honest! But for now, you need a workaround:\n\nIn each dataset, make a new column with a sensible name such as when. The two names so not have to be the same.\nUse the number() function to convert the data to a number. So in the CPI dataset, you will enter number(Date). Very large numbers appear. They are the number of seconds since January 1, 1970.\nNow drag the number-i-fied date (when) from the CPI table to the corresponding column in the real estate table. The CPI column appears in the real estate table.\n\n\n\n\n21.4.2 The analysis\nBefore you do any calculations, make a graph.\n\nPlot Price against Month. You should see data that’s generally going up from 2012 to 2023, but wiggling every year.\nPlot CPI (from the real estate table) on the same axis. That is, drag it to the vertical axis where Price is, and then go “up” and drop it on the plus-outline that has appeared.\n\n\n\n\nNow you see the two graphs; that they fit on the same plot is a fortunate coincidence. It also looks as if housing is rising faster than inflation, that is, faster than the CPI.\nWhat we want now is a new column for adjustedPrice. Clearly it is a calculation that involves Price and CPI. But what should the formula be? I will leave it to you this time, noble reader, but here are some considerations:\n\nThe result should be in dollars, so it’s like the price itself as opposed to some housing-price index…which is what I unwittingly got with my first formula, which was Price/CPI. (Try it and see!)\nThe dollars should probably be in either 2023 dollars or 2012 dollars. You can decide, but it will affect the formula.\nThink: should the adjusted price be going up or down? And if you think it should go up, what would have to be true in the data for it to go down?\n\n\n\n21.4.3 You want more?\nOne extension would be to analyze the wiggles as we did in an earlier section on CO2 data.\nOr, for a larger dataset that lets you compare regions, download Housing data for all states."
  },
  {
    "objectID": "04.70-adjustment-and-control.html#tall-for-your-age-and-gender",
    "href": "04.70-adjustment-and-control.html#tall-for-your-age-and-gender",
    "title": "21  Adjustment and Control",
    "section": "21.1 Tall for your age (and gender)",
    "text": "21.1 Tall for your age (and gender)\nLet’s begin with the familiar height data. Suppose we want to identify “tall” children, and we decide that “tall” means, “more than 10 cm taller than the mean height for that age and gender.” We could do a similar thing for “short.”\nTo do this, we need to calculate the mean height for every age. We did this already (and included Gender as well) in one of our first lessons. We’ll do the same thing now, and use it for more!\nIn the live example below, do the following:\n\nThat table is taking up way too much space. Make it shorter.\nDrag Age and Gender to the left to form groups (grouping move!)\nMake a new column called meanHeight and give it an appropriate formula (summarizing).\n\nNow we have calculated the mean heights for each group. Recall that the high-level values appply to everythiing beneath them in the hierarchy. That means we can use that meanHeight in arithmetic the way we want:\n\nMake a new column over on the right; call it relativeHeight.\nGive it the formula, Height – meanHeight. That is, it’s how many centimeters taller (or shorter) you are than the mean in your group. (calculating!)\n\n\n\n\n\nMake two graphs: Height against Age and then just relativeHeight.\nSplit the first graph by Gender by dragging Gender to the upper “axis.”\nSelect everybody in the second graph with relativeHeight greater than (about) 10.\n\n\n\n\nYour graphs should look somethiing like this\n\n\nNotice where the selected people are in the first graph. They’re tall for their age, just as we wanted!\nIf we went further, we might make another new attribute with values such as tall, short, and normal, that we could use to study, for example, if tall people’s pulse is any different from short. You could make that attribute with a formula, or by using the case-selection capabilities of choosy.\nThe real point, though, is that we have created an attribute, a relative height, that is adjusted by age. (And we needed a few data moves to do it.)"
  },
  {
    "objectID": "04.70-adjustment-and-control.html#how-much-is-college-worth",
    "href": "04.70-adjustment-and-control.html#how-much-is-college-worth",
    "title": "21  Adjustment and Control",
    "section": "21.3 How much is college worth?",
    "text": "21.3 How much is college worth?\nIn this section, we’ll look back at income data using some of the same thinking we used when we looked at gender and income.\nWe will again use data moves to do our summaries and then to make an adjusted value. To give you a break, I will talk you through the beginning instead of having you do it.\n\n21.3.1 The setup: A first pass\nIf you get a bunch of Census data (for example, from the Microdata Portal), You will find that people with more education have a higher income.\nHow much higher? We can calculate that in dollars—though, as we will see, that’s not the only way.\n\nI used the Microdata Portal to collect 1000 Americans from 2017. Since I am interested in income and education, I make sure to include those attributes.1\nThe vanilla sample has people of all ages, so I set aside everyone who was not bewteen 25 and 55. Notice that this is a filtering data move.\n\nGraphing education against income shows, unsurprisingly, that people with more education earn more. But it’s too many catgories to deal with right now, so…\n\nI then made a new attribute, College, to indicate whether the person had any college. Each person got the value some or none. This is a calculating data move, even though I did not write a formula.\nThen I made a graph of College by Income-total (implied grouping), did the drag-left dance (grouping), and showed the medians (summarizing) to get my result:\n\n\n\n\nIncomes in 2017 for people with any college (median $35000) and with no college (median $22150)\n\n\n\n\n\n\n\n\nYikes!\n\n\n\nIf you are shocked by how low those numbers are, yes, it’s surprisingly bad. There are mitigating factors, however:\n\nIf you’re living in a wealthier area, note that this is a national random sample.\nWe have not filtered out people who are not working. That makes a difference, but not as much as you might think.\n\nTrying to fix those problems is a job for the a more nuanced extension of this work. In class, this would be a “dig deeper.”\n\n\nIt seems, using this naïve approach, that going to college is worth $12850 a year. We will come back to this in a moment.\n\n\n21.3.2 Extension: let’s look at 1960 as well\nI wondered what we would find in 1960.\nUsing the Microdata Portal, I can get exactly the same data, but from a different year. In the plugin, I change the year and check the box for Keep existing data.\nAfter I add 1000 people from 1960, I need to filter the age again and set the values of College for the new data. This is straightforward, though it’s interesting: the education categories in 1960 are not the same as they were in 2017—but I can still tell who went to college.\nThe simplest display of my results shows the problem:\n\n\n\nMedian incomes (shown as bars!) for people with some college and no college, in 1960 and 2017. Also, notice how I split the graph by Year the same way I did Gender in the previous section.\n\n\nIn 1960, by the same measure, college was worth $1300 per year. But, but… the scales of the two graphs are so different that you can’t really see a fair comparison.\nWe could look up the consumer price index and calculate the four values in “2017 dollars.” But we will save the CPI for the next example.\nInstead, let’s calculate the number that fills in the blank:\n\nFor every dollar a person with college makes, a person without college makes only ___ cents.\n\nAnd that’s easy to do: just divide 22150/35000 (which is 0.633) and 2250/3550 (which is 0.634). So for both years, those who didn’t go to college made only 63¢ for every dollar people who did go to college made.\nThe takeaway: It was hard to compare the situation in 2017 to the one in 1960, so we changed how we presented the data in order to make a fair comparison. In this case, we used a ratio to express the comparison instead of a straight difference.\n\n\n21.3.3 Extending further\nIt was easy to do those divisions usng a calculator (or just Google). But suppose we want to take our study further. Perhaps\n\nwe want to remove the people who are not in the labor force; and/or\nwe want to include 1980 and 2000, to see more of the trends; and\nwe want to display the 63¢ values in a graph.\n\nIf we do that, pretty soon we’re making too many hand calculations. We should get the computer to help.\nYou know how to do most of these things already (or you can look them up in this book!). But you might not immediately see why we need another layer in the hierarchy, and you might need some help with the formulas.\nWe will save you from getting to this point and start you out with the dataset already set up with 2017 and 1960.\n\nor use the live example below.\nWe will also save the explanation for why we’re doing what we’re doing for afterwards. See if you can figure it out as we go!\n\nMinimize the Microdata Portal for now. You need the screen space.\nDrag Year left to make a new level.\nMake a new attribute there, called cents.\nGive cents a formula to calculate how many cents a non-college person earns for every dollar a college person earns. Here is one possibility, wordy but effective:\n\n100 * median(`Income-total`,College='none') / \nmedian(`Income-total`,College='some')\nYou should see “63” for both Years. Hierarchy works perfectly for us here; the formula does not need to mention the year, because it only applies to the cases attached to that cell in the hierarchy, so that the 1960 value for cents ultimately includes all the people from 1960, and only those people.\n\n\n\nNow we can continue:\n\nOpen up the Portal and collect 1000 people from 1980 and another thousand from 2000. Minimize the portal again.\n\nAlas, these new years have not been filtered and massaged the way we did for 1960 and 2017. You’ll have to do that for the new cases:\n\nOnce again, set aside all the people under 25 or over 55. (I use a graph of Age to help.)\nDrag Education-degree recode next to College (again) and give any blank cells under College appropriate values of some or none.\nDrag Education-degree recode back to the right.\n\nNotice the numbers have now appeared in cents. The formula should work for all four values of Year.\n\nMake a graph of cents against Year. You may need to re-order the (categorical) Years.\nSet aside all people who are not Employed. (Another filter move)\n\n\n\nWhy did we do all that?\nHere is the rationale that would have been impossible to understand if you had not been through this:\nIt’s all about the hierarchy.\nWhen we began, the file had a bottom layer (on the right), with the people, and an upper layer, where the people were grouped by College. In the Colleges layer, we had the median income for people with and without college. Think about how that made sense: these medians “belonged” to particular values of College and ‘Year’, they’re the medians for the cases some and none— and for all of the cases subordinate to those values.\nWe wanted to calculate the number of cents people with no college earn. That quantity would apply only to the Year (e.g., 2017), not to the some 2017 or none 2017 sub-groups. Furthermore, the medians for some and none would have to be inside the years, because we needed them to calculate cents.\nThat meant we needed a new level for Year, above the Colleges level, and we made our new attribute, cents there, comparing college income to no-college.\n\nOnward!\nIf your experience is like mine, you’ll get values between about 55% and 73%, with no obvious trend. I wondered if I had made some mistake, so I did it over again from the start.\nIt’s much easier the second time! Especially because you can get all the data first, and do all the filtering and recoding once. You will probably get different results, as I did. A similar range, but a different pattern.\n\n\n\n\n\nTwo different samples from 1980, #6 and #8, split by College. Notice how the medians for some and none are over $1000 different in the different samples.\n\n\nIn fact, my 1980 data had a value of cents 10¢ different the second time through. And this with a sample of over 50 people in each category! I had imagined that the median value would be more stable—that it would not change as much with a new sample.\nOne takeaway: once you have done a compicated CODAP analysis, it’s often pretty easy—and illuminating—to redo it completely.\n\n\n\n21.3.4 Looking more deeply\nAre you disappointed that non-college people always seem to make about 60% of what college people make (in the median)? I was. I had thought that we would see an increasing pay gap between the hoity-toity intellectuals (like myself) and the people who do the real work in this country.\nBut even a “negative result” is useful.\nFor one thing, you can use the same data to explore other aspects of social change. For example, what proportion of people in this age range have a college education, as a function of year? All you need is a new column and a new formula. (Use the function count() to get the number of cases in a dataset.)\nThis dataset also has Sex, which means that you can track gender differences. You should predict first:\n\nBetween 1960 and 2017, how did the proportion of women going to college change?\nBetween 1960 and 2017, how did the proportion of women in the workforce change?\nIs that last figure any different when you compare collegiate women to non-?\n\nAn investigation like this would be a fine student project. It would not take too long, and there is a lot you can do with it. Many opportunities to dig deeper.\nAnd notice how many data moves you have to use. It gives the students plenty of practice—and possible entries in their data moves portfolio!"
  },
  {
    "objectID": "04.70-adjustment-and-control.html#coping-with-the-wiggles",
    "href": "04.70-adjustment-and-control.html#coping-with-the-wiggles",
    "title": "21  Adjustment and Control",
    "section": "21.4 Coping with the wiggles",
    "text": "21.4 Coping with the wiggles\nWe could just continue with home-price data, but let’s change context and worry about climate change.\nOne of the most famous scientific graphs ever is called the Keeling curve, named for its originator, Charles David Keeling. In 1958, Keeling started the program to monitor CO2 on the summit of Mauna Loa in Hawaii. And it wiggles. There is an annual variation.\n\n\n\nMauna Loa CO2 data from the NOAA site."
  },
  {
    "objectID": "04.70-adjustment-and-control.html#exploring-wiggles",
    "href": "04.70-adjustment-and-control.html#exploring-wiggles",
    "title": "21  Adjustment and Control",
    "section": "21.4 Exploring wiggles",
    "text": "21.4 Exploring wiggles\nWe could just continue with home-price data, but let’s change context and worry about climate change.\nOne of the most famous scientific graphs ever is called the Keeling curve, named for its originator, Charles David Keeling. In 1958, Keeling started the program to monitor CO2 on the summit of Mauna Loa in Hawaii. And it wiggles. There is an annual variation.\n\n\n\nMauna Loa CO2 data from the NOAA site.\n\n\nThe main point of he graph is that it’s going up: atmospheric CO2 has increased dramatically over the last 60+ years. In this section, however, we are just going to look at the annual variation.\nHere is the plan:\n\nFor every year, we’ll compute the average. We’ll call this annualMean.\nThen for every month, we’ll compute how much it is over or under that average. We’ll call this the extra.\n\nAs you can see, this is very much like computing whether you are tall for your age. The challenge is mostly going to be coping with dates. You can look back at the chapter on dates for help.\nIf you want to do this all yourself, go to this page on the NOAA site and download the monthly mean data CSV. That file will have all the years; we’ll just use 2010–2020 in the live example below.\nDo the following:\n\nExplore the dataset. Make graphs! The CO2 “mole fraction” in parts per million is the attribute average.\nDrag year to the left (i.e., group by year) and in that area, create annualMean and give it a good formula.\nBack on the right side of the table, make the extra attribute, and give it the formula average - annualMean.\n\n\n\n\nNow for some additional exploration. What do these new attributes let us do?\n\nMake a new graph of extra against decimalDate\nMake another graph of extra against month.\nPlop year into that graph and then, in the menu for that legend attribute, have CODAP treat it as categorical. This way, you get more distinct colors.\nIn the ruler palette of this last graph, check Connecting Lines.\n\nNow it’s easier to answer questions such as,\n\nWhich month has the highest CO2?\nIs it always that month?\nWhat is the amplitude of the annual variation?\n\nYou can also work to understand, for example, why that first graph, the one against decimalDate, looks the way it does.\n\n\n\n\n\n\nNote\n\n\n\nIn making our extra attribute, we just subtracted out the annual mean. But really, the underlying quantity of CO2 increases throughout the year…so really we should subtract out something that’s also increasing.\nThat’s worth additional thinking! For some ideas about how to do that, read how they make seasonal adjustment on the NOAA data page where we found the initial graph."
  },
  {
    "objectID": "04.70-adjustment-and-control.html#CO2-wiggles-section",
    "href": "04.70-adjustment-and-control.html#CO2-wiggles-section",
    "title": "21  Adjustment and Control",
    "section": "21.2 Exploring wiggles in CO2",
    "text": "21.2 Exploring wiggles in CO2\nOne of the most famous scientific graphs ever is called the Keeling curve, named for its originator, Charles David Keeling. In 1958, Keeling started the program to monitor CO2 on the summit of Mauna Loa in Hawaii. And it wiggles. There is an annual variation.\n\n\n\nMauna Loa CO2 data from the NOAA site.\n\n\nThe main point of he graph is that it’s going up: atmospheric CO2 has increased dramatically over the last 60+ years. In this section, however, we are just going to look at the annual variation.\nHere is the plan:\n\nFor every year, we’ll compute the average. We’ll call this annualMean.\nThen for every month, we’ll compute how much it is over or under that average. We’ll call this the extra.\n\nAs you can see, this is very much like computing whether you are tall for your age. The challenge is mostly going to be coping with dates. You can look back at the chapter on dates for help.\nIf you want to do this all yourself, go to this page on the NOAA site and download the monthly mean data CSV. That file will have all the years; we’ll just use 2010–2020 in the live example below.\nDo the following:\n\nExplore the dataset. Make graphs! The CO2 “mole fraction” in parts per million is the attribute average.\nDrag year to the left (i.e., group by year) and in that area, create annualMean and give it a good formula.\nBack on the right side of the table, make the extra attribute, and give it the formula average - annualMean.\n\n\n\n\nNow for some additional exploration. What do these new attributes let us do?\n\nMake a new graph of extra against decimalDate\nMake another graph of extra against month.\nPlop year into that graph and then, in the menu for that legend attribute, have CODAP treat it as categorical. This way, you get more distinct colors.\nIn the ruler palette of this last graph, check Connecting Lines.\n\nNow it’s easier to answer questions such as,\n\nWhich month has the highest CO2?\nIs it always that month?\nWhat is the amplitude of the annual variation?\n\nYou can also work to understand, for example, why that first graph, the one against decimalDate, looks the way it does.\n\n\n\n\n\n\nNote\n\n\n\nIn making our extra attribute, we just subtracted out the annual mean. But really, the underlying quantity of CO2 increases throughout the year…so really we should subtract out something that’s also increasing.\nThat’s worth additional thinking! For some ideas about how to do that, read how they make seasonal adjustment on the NOAA data page where we found the initial graph.\n\n\nNotice, looking back on this, that we used data moves again, in a similar way:\n\nwe grouped by year,\nwe summarized the year to create annualMean,\nwe did a calculation to find the extra."
  },
  {
    "objectID": "03.70-visualization.html",
    "href": "03.70-visualization.html",
    "title": "14  citation-location: margin",
    "section": "",
    "text": "15 Visualizing and Communicating\nCODAP’s graphs are limited. I think that’s a good thing in this context, because entering into the world of visualizations would be a distraction. Really, it’s a whole nother course.\nIn our work with the students, we focus on communicating well with the graphs that CODAP makes available. A big part of that is helping them see just what’s possible. So we will do a bunch of that here, along with some introductory design philosophy."
  },
  {
    "objectID": "06.80-data-moves-portfolio.html",
    "href": "06.80-data-moves-portfolio.html",
    "title": "24  citation-location: margin",
    "section": "",
    "text": "reference-location: margin\n    \n\n25 Data Moves Portfolio"
  },
  {
    "objectID": "07.40-inference-intermezzo.html",
    "href": "07.40-inference-intermezzo.html",
    "title": "28  Inference Intermezzo",
    "section": "",
    "text": "When we do inference this way, using scrambling and bootstrapping, are we doing data science?\nI think so, mostly, but there are ways in which it doesn’t smell as much like data science as the stuff we were doing in the introductory lessons.\nThe problem for me is that sometimes it smells like a statistics exercise, not like data science. For example, consider the tangerines. Our data set consisted of…eighteen weights. We are hardly awash in data.\nOn the other hand, when we did the bootstrap, we calculated a mean and dragged it to the left, reorganizing the dataset. Then we sampled that dataset with replacement, and we did that repeatedly, several hundred times, making an entirely new dataset. That’s a lot of messing about with data, including actions that are clearly data moves—moves that are not even on our list yet!\nPut another way, data moves can be important tools when you’re trying to do inferential statistics. They can make the work smell more like data science.\nA different approach to the question is to think of inferential statistics as a tool that you sometimes need when you’re doing data science. So we might imagine exploring international trade, or the paths of wolves, or some other topic, and we end up wondering whether some difference we see is due to chance or what a reasonable range of values for some parameter is.\nI see another connection in the “dig deeper” genre of assignment. There we want students to be constructively skeptical, including by posing alternative hypotheses about whether a proposed relationship exists. Inference testing essentially asks us always to include chance as an alternative explanation for any claim. Therefore we need a way to evaluate that possibility.\nAny of these approaches seems reasonable to me, and convince me that stats and data science clearly are in the same family… though perhaps as cousins rather than siblings."
  },
  {
    "objectID": "01.40-teens2.html#making-summary-calculations-for-each-group-summary-calcs-by-group",
    "href": "01.40-teens2.html#making-summary-calculations-for-each-group-summary-calcs-by-group",
    "title": "5  800 Children and Teens, part two",
    "section": "5.2 Making Summary Calculations for Each Group (#summary-calcs-by-group)",
    "text": "5.2 Making Summary Calculations for Each Group (#summary-calcs-by-group)\nNow we want the mean height for each of our groups. To do that, we’ll make a new column in the “groups” table on the left, and write a formula for the column:\n\nBe sure the table is selected.\nOn the left-hand side of the table, up at the top on the right, there is a gray circle with a plus sign in it. It might be hidden by some text.\n\n\n\n\nClick the gray circle with the plus sign to create a new column.\n\n\n\nClick the gray plus thingy. A new column appears, with a name ready to be editied.\nGive it a good name such as MeanHeight. Press enter to finish editing. The column should be blank.\nLeft-click on the column (attribute) name; a menu appears. Choose Edit Formula. A formula box appears.\nEnter mean(Height). Press Apply.\n\n\n\n\nThe formula editor.\n\n\nHooray! You see the mean height for each age in the right row in that new column.\n\nDoes it bother you that the ages are not in order? Click on the colum heading for Age to get the menu, then choose Sort Ascending.\n\n\nThe mean height is a summary of each group. This action of summarizing (sometimes also called aggregating) is the third core data move. We now have three: filtering, grouping, and summarizing.\nA summary doesn’t have to be a mean. It might be a median, or a sum, or just the count (a.k.a. frequency) of the cases in the group. It could even be a percentage, like the percentage of people in the group who have a BMI under 30.\n\nCODAP has a number of functions that serve as summaries. Here are four of the most important:\n\n\n\n\n\n\n\nmean(foo)\nthe mean of foo\n\n\nmedian(foo)\nthe median of foo\n\n\nsum(foo)\nadd up the values of foo\n\n\ncount()\nhow many cases there are"
  },
  {
    "objectID": "01.50-assignment-2.html#gender-and-income",
    "href": "01.50-assignment-2.html#gender-and-income",
    "title": "6  A Second Assignment",
    "section": "6.2 Example: Gender and Income",
    "text": "6.2 Example: Gender and Income\nHere’s an example of the kind of thing we have in mind:\nSuppose we were interested in gender and income. The simple approach is to (duh) plot gender and income. The graph alone looks vaguely like the men get more, but if you put the median on the graph, and rescale it, it’s really obvious:\n\n\n\nTotalIncome split by Gender, with lines showing median income for each group.\n\n\nThis is what we probably expect: men earn more than women. If we look at median values, men earn $17,000, versus $9,200 for the women. But does that tell the whole story? How could we dig deeper? (…as required in the assignment)\nWe might ask:\n\nIs it possible that the incomes really are equal, but we’re looking at it wrong?\nCan we be more nuanced? For example, is there some other factor that affects income?\n\nLooking at the graph, see the large number of people who seem to earn zero—or close to it? That spike at zero is taller for women. Maybe that’s because more women work in the home, and are not paid.\nSo maybe the incomes for people with jobs are equal between men and women, but because more women do not get paid, their median income is lower overall. This reasoning is an example of exploring whether we’re looking at it wrong, and that there is another factor—employment—that affects income. That is, it’s not just gender.\nTo test this idea, let’s just look at people with jobs. It turns out that we can get data for an attribute (a column) called EmplStatus for “employment status.”\n\nThat means you could filter to focus your investigation on people with jobs. That way, you can explore whether men with jobs generally earn more than women with jobs.\n\nTry that in the live illustration below. The “employment status” attribute is at the far right of the table.\n\n\n\nYou should find that the men still earn more. If you hide everyone but those that are “Civilian employed,” the men earn $45,000 to the women’s $30,000. So the fact that more women do unpaid work does not completely explain the difference in income.\nNotice that this will only work if you have downloaded EmplStatus data. If you get partway through your investigation and realize that you wish you had downloaded something else, or something more, you can’t add additional columns to the cases you already have. But remember: starting over is free. Just go back and get fresh data with the attributes you want."
  },
  {
    "objectID": "04.50-dates-and-times.html#dates-improve-formats",
    "href": "04.50-dates-and-times.html#dates-improve-formats",
    "title": "19  Dates and Times",
    "section": "19.3 Using other tools to massage the data",
    "text": "19.3 Using other tools to massage the data\nThe previous example worked fine; we made an edit and everything was OK. But in our work we’ll often have hundreds or thousands of data points. The truth is that the Big Dogs (e.g., Google) have lots more programming power that Concord Consortium, who make CODAP. Sometimes you may need to turn to your trusty spreadsheet to wrangle your data.\nGoogle Sheets understands many more date formats that CODAP. If you put your data, temporarily, in a Google Sheet, you can update its format there and then re-import it into CODAP. Select the whole column of dates, then go to the “123” format menu, and choose Date. The column will re-format to be consistent.\n\n\n\n\n\n\nBefore (in Sheets)\n\n\n\n\n \n\n\n\n\n\nthe menu\n\n\n\n\n \n\n\n\n\n\nAfter (in Sheets)\n\n\n\n\n\nOnce it is correct in Sheets, you can simply copy the data, switch to CODAP, and use the Table menu to import the data."
  },
  {
    "objectID": "07.60-tangerines-redux.html",
    "href": "07.60-tangerines-redux.html",
    "title": "29  Tangerines Revisited",
    "section": "",
    "text": "We saw data on tangerines in the chapter on bootstrapping, but we didn’t see all of the data. The more complete dataset has 72 cases and three attributes:\n\nid\n\nThe initials of the student who “owned” the tangerine\n\nweight\n\nThe weight of the tangerine in grams\n\nday\n\nWhich day of the lesson this measurement was taken\n\n\nThat’s right, we have repeated measurements over the course of a week. What do you suppose happens to the weight of a tangerine as time passes?1\nNow, what to do? First, we will concentrate only on day 0 and day 2, so that’s 36 cases in two groups. If you plot the two distributions of weight, you see this:\n\n\n\nWeights and means for 18 tangerines on day zero and day 2. Notice that we have made day categorical in the graph.\n\n\nThis presents one clear task:\n\nDo a randomization test (i.e., scrambling) to see if it is plausible that the difference in mean weights could be due to chance alone. Find a \\(P\\)-value and draw a conclusion.\n\nBut then, there is a different task we can do to approach this problem from another direction\n\nEstimate (i.e., use a bootstrap) the mean difference in weight of the 18 tangerines between day 0 and day 2.\n\nThat is,\n\n\n\n\n\nDifferences for 18 tangerines\n\n\n\nCalculate, for each individual tangerine, how much weight it gained or lost. (You will have to do a grouping move. But what should you group by?)\nPlot those gains or losses to make sure they make sense. Author’s results at right.\nFind the mean of those values.\nDo a bootstrap to find a plausible range for that mean.\nDoes the range overlap with zero? (no!)\nExplain what that must mean.\n\nThen compare your two results.\nYou can\n\nor you can do your work in the live demo below.\n\n\n\nYou should have discovered that the “paired” version, where you looked at the 18 differences from individual tangerines rather than the overall difference in the distribution of 18 tangerines, was much more obviously not due to chance.\nSee if you can make sense of this. Of these two statements:\n\nIt is obvious that the distribution of differences does not overlap zero.\nIt is obvious that the means of the two distributions are different.\n\nWhy is (a) so much more obvious?\nResearchers often use this idea of a “paired” test when they can, especially in a situation like this (which is called “repeated measures.”)\nSuppose you have a drug that’s supposed to reduce cholesterol. If you have two ways of analyzing the data:\n\nFind the reduction (or gain) of each individual in the study before and after they take the drug, and find the average of the changes.\nFind the mean cholesterol for the group before and after they take the drug, and find the change of the averages.\n\nYou will usually find a stronger result with the first (paired) scheme. Again, see if you can figure out, conceptually, why.\nNow, as to where the data science is: think about the different ways you had to arrange the dataset in order to get what you were looking for. Some of this was about knowing CODAP and how to “drive” the program. But part is also your conceptual understanding of how a dataset works and the power of grouping.\n\n\n\n\n\nThis is a science question, really, which means that the best answer is often, “it depends.” To which we ask, “depends on what?”, which makes it a data question again.↩︎"
  },
  {
    "objectID": "08-arbor/08.10-arbor-anatomy.html#the-root-node",
    "href": "08-arbor/08.10-arbor-anatomy.html#the-root-node",
    "title": "30  Anatomy of a tree",
    "section": "30.1 The root node",
    "text": "30.1 The root node\nSo: way up at the top, the “root” block, the root node, looks like this:\n\n\n\n\n\nThe first horizontal “stripe” tells us what attribute we are trying to predict. In this dataset, it’s called fate. (The value of fate is either survived or died.)\nThe second stripe tells us that we will consider survived to be the “positive” result. That seems obvious, but in many medical contexts, a “positive” test means that you have the disease."
  },
  {
    "objectID": "08-arbor/08.10-arbor-anatomy.html#the-trunk-node",
    "href": "08-arbor/08.10-arbor-anatomy.html#the-trunk-node",
    "title": "30  Anatomy of a tree",
    "section": "30.2 The trunk node",
    "text": "30.2 The trunk node\nJust below the root is the trunk. That block, that node, also has two stripes.\nThe top one tells you that 500 of the 1309 people—38.2% of them—survived. You know it’s survived because of what it says in the root. But if you forget, or get confused, you can always hover over that node to see details:\n\n\n\n\n\nThe bottom stripe sets up a branching. In this case, we ask about gender, and branch one way if the person is female and the other way if they are male."
  },
  {
    "objectID": "08-arbor/08.10-arbor-anatomy.html#other-nodes",
    "href": "08-arbor/08.10-arbor-anatomy.html#other-nodes",
    "title": "30  Anatomy of a tree",
    "section": "30.3 Other nodes",
    "text": "30.3 Other nodes\nEvery other node is structured more or less the same as the trunk node: the first stripe shows how many people are positive, that is, how many survived, and the second shows the branching if there is one.\nThe key thing about the number stripe is that it considers only the people who get to that point in the tree. So although 38% of all people survived, the tree shows us that there is a big gender difference: 73% of the females survived, but only 19% of the males.\nAnd then, among the males, the young males—under 15 years old—had a 50% survival rate compared to about 18% for males 15 or older."
  },
  {
    "objectID": "08-arbor/08.10-arbor-anatomy.html#terminal-nodes-the-leaves",
    "href": "08-arbor/08.10-arbor-anatomy.html#terminal-nodes-the-leaves",
    "title": "30  Anatomy of a tree",
    "section": "30.4 Terminal nodes: the leaves",
    "text": "30.4 Terminal nodes: the leaves\nIf a node has no branches coming out of it, it is a terminal node. Every terminal node has a “leaf” below it that is rounded instead of rectangular. The leaf shows a prediction for the fate of the people in that category.\nSo for the females, our best guess is that they will survive. For older males, our best guess is that they will die.\nFor the young males, under 15, with a 50% survival rate, we’re predicting survival, although you can make a case that at 50% we shouldn’t predict. The other side of that argument is that they have a better chance than people in general (38%)."
  },
  {
    "objectID": "08-arbor/08.10-arbor-anatomy.html#the-links",
    "href": "08-arbor/08.10-arbor-anatomy.html#the-links",
    "title": "30  Anatomy of a tree",
    "section": "30.5 The links",
    "text": "30.5 The links\nSlanted white stripes, called links, connect nodes to their “children.” Every link has a label such as female or >= 15 so you can tell who that link applies to.\nSo: that’s how you read a tree! To learn how to make a tree, see the page about learning to drive."
  },
  {
    "objectID": "08-arbor/08-arbor-part.html#overview",
    "href": "08-arbor/08-arbor-part.html#overview",
    "title": "Classification trees in CODAP using Arbor",
    "section": "Overview",
    "text": "Overview\n\nAnatomy of a tree\n\nLearn how to read a tree, using data from the Titanic disaster.\n\nFirst driving lesson\n\nThe basics of how to make a tree using Arbor.\n\nTrees and graphs\n\nTrees and graphs are both ways of looking at the data. And they are intimately connected. Learn how to use graphs to help you make th ebest trees.\n\nFbola example\n\nAnother example dataset, this time in a medical context. Use the tree to decide who probably has the virus.\n\nTree quality\n\nWhat makes a tree a good tree? Arbor can help you calculate tree quality.\n\nBreast cancer example\n\nWith a numerical predictor, we create our own measure of tree quality, and use it with graphs to make the best tree.\n\nMachine learning connection\n\nClassification is an important topic in machine learning. This explains the connection and how work with Arbor might fit in.\n\nThe configuration box\n\nWhen you “configure” a node in a tree, there is a dialog box. This explains all of its controls.\n\n\nTo learn why you would want to use this plugin, see xxx."
  },
  {
    "objectID": "08-arbor/08.20-arbor-begin.html#the-target-attribute",
    "href": "08-arbor/08.20-arbor-begin.html#the-target-attribute",
    "title": "31  Your first tree",
    "section": "31.1 The target attribute",
    "text": "31.1 The target attribute\nTo make a tree, you first have to identify your target attribute. The target attribute might also be called\n\nthe dependent variable\nthe outcome variable\nthe effect\n\nIt’s the thing you are trying to predict. In the case of the Titanic data, it’s fate: we want to know what makes it more likely to have survived the disaster.\nIn the live example below, start your tree by dragging fate from the table at the right and dropping it into the middle of the blank, gray tree panel. You should see that 500 of the 1309 people survived."
  },
  {
    "objectID": "08-arbor/08.20-arbor-begin.html#making-a-branch-drag-and-drop",
    "href": "08-arbor/08.20-arbor-begin.html#making-a-branch-drag-and-drop",
    "title": "31  Your first tree",
    "section": "31.2 Making a branch: drag and drop",
    "text": "31.2 Making a branch: drag and drop\nNext, drop sex onto the white box with “500 of 1309” in it.\nThe tree will branch. You can see how the survival rate was different for males and females.\nNotice that the tree is composed of boxes (called nodes) and lines (links).\n\n\n\n\n\n\nLost in the Node?\n\n\n\nIt’s easy to lose track of what’s going on in a node. When that happens, just point at the node. Don’t click, just hover for a moment, and text will pop up describing that node in more detail.\n\n\n\n\n\n\n\nThe pop-up information for the “male” node\n\n\n\n31.2.1 Adding a numeric attribute\nNow drop age onto the “male” node. It will split, but probably using age 30 as a cutpoint. Click the gear on the age stripe and change that 30 to 15.\n\n\n\n\n\nConfiguring age: setting the cutpoint to 15.\n\n\nWhen you make a tree with Arbor, every node has a maximum of two branches. An attribute like age has so many values, you generally have to tell Arbor where that cutpoint is using that configuration box.\nYou can configure any attribute, but it’s more common for numerical ones.\n\n\n31.2.2 Assigning results to terminal nodes\n\n\n\n\n\nYour tree should look like this after you have assigned its leaf nodes to be survived or died.\n\n\nNow your tree should look like the one up at the top of the page… except that you still have to assign the terminal nodes — the leaves of the tree — to an outcome. Do that by clicking on the leaf nodes repeatedly until you see what you want."
  },
  {
    "objectID": "08-arbor/08.40-arbor-fbola.html#the-same-tree-with-data",
    "href": "08-arbor/08.40-arbor-fbola.html#the-same-tree-with-data",
    "title": "33  Fbola example",
    "section": "33.1 The same tree, with data",
    "text": "33.1 The same tree, with data\nThat tree was made from intuition. We can also make it with data, using Arbor. Suppose that we take 100 students and give them an expensive, time-consuming test for Fbola; those test results appear in the Fbola column. We also record whether they have a rash or a fever.\nUse the live example below to make the tree. Don’t hesitate to make graphs to see relationships between the attributes.\nYou can even consider making a different tree to accomplish the same task—by asking about the fever first.\nDon’t forget to include diagnoses at the ends of all your branches!"
  },
  {
    "objectID": "08-arbor/08.40-arbor-fbola.html#trees-as-models",
    "href": "08-arbor/08.40-arbor-fbola.html#trees-as-models",
    "title": "33  Fbola example",
    "section": "33.2 Trees as models",
    "text": "33.2 Trees as models\nNotice this very very important fact: our diagnosis might be wrong. A positive diagnosis using the tree might be a true positive (TP), that is, it’s correct and you have the disease; or it might be a false positive (FP), that is, we send you home even though you are well.\nSimilarly, you can get false negatives (FN) and true negatives (TN).\nYou can see all four possibilities by looking at the table tab in Arbor. (Do that!) A typical tree, with its corresponding table, looks like this:\n\n\n\n\n\n\n\n\n\nThe 14 false positives combine seven from the leftmost leaf and seven from the middle one.\n\n\n\n\n\nThis leads us to an issue we have to emphasize: how do we know whether our diagnosis is right or wrong? The answer to that is, sometimes we don’t. In this case, though, we assume that the time-consuming and expensive test is perfectly accurate. In our practical situation at school, however, we hope our rash-and-fever tree does a good enough job.\nIn general, when we have a classification problem like this, there is some underlying Truth that we cannot see. We can only see the shadow3 of this Truth, in the form of data. We see the symptoms, not the actual disease. We use the data to make our best guess about the Truth.\nAnd in fact, look at at the tree in Arbor. We are trying to predict Fbola—the results of the expensive test—using the data about rash and fever.\nTaken together, this all means that a tree is a model. It’s an approximation of the truth that we will make as useful as possible. But it’s not the Truth; it’s a human construct.\nAlso, math nerds, notice that in this model, the tree’s procedure is a function. Its inputs are the data (fever and rash) and the inevitable output is either positive or negative. Notice how this is parallel to the situation when you use a line as an approximation to data in a scatter plot. The line is a function, and it’s not completely correct even though it can be useful.\nIn that situation, we can even try to find the best line using a criterion such as least squares. And that’s whats coming next with our lessons on trees."
  },
  {
    "objectID": "08-arbor/08.50-arbor-measures.html#constructing-a-measure",
    "href": "08-arbor/08.50-arbor-measures.html#constructing-a-measure",
    "title": "34  How good is your tree?",
    "section": "34.1 Constructing a measure",
    "text": "34.1 Constructing a measure\nLet’s look at one common measure, called the misclassification rate, which I will abbreviate MCR. That’s equal to\n\\[\\rm{MCR} = \\frac{\\textrm{number of wrong diagnoses}}{\\textrm{number of diagnoses}}\\]\nArbor can supply you with various numbers you can combine to make your measure. These include the number of true positives (\\(\\rm{TP}\\)), false negatives (\\(\\rm{FN}\\)), and so forth. With those numbers, you could calculate the MCR like this:\n\\[\\rm{MCR} = \\frac{FP+FN}{TP+TN+FP+FN}\\]\nThe plan, then, would be to make a formula for MCR—or whatever measure you want—in CODAP. But where would you make that formula? And how would you get the attributes like TP to put in it?"
  },
  {
    "objectID": "08-arbor/08.50-arbor-measures.html#getting-fp-and-tn-and-all-that",
    "href": "08-arbor/08.50-arbor-measures.html#getting-fp-and-tn-and-all-that",
    "title": "34  How good is your tree?",
    "section": "34.2 Getting FP and TN and all that",
    "text": "34.2 Getting FP and TN and all that\nAs you have seen, these numbers appear at the bottom of your tree. You could copy them and type them in, but you don’t have to.\nLet’s revisit the Fbola example. The live illustration below shows a tree that uses only the rash attribute as a predictor. In the context of the example, that means we send everybody home who has a rash, and do not take their temperature.\nAs you can see from the values below the tree, among the 100 people, we have 7 false positives (FP=7) and 12 false negatives. That means that the misclassification rate, MCR, is (7 + 12)/100, or 0.19.\n\n\n\nWe don’t want to do that calculation by hand every time, so do this:\n\nClick the disclosure triangle just left of in order to export. Some controls appear. Ignore most of them!\nPress the emit data button.\n\nAha! A new table appears called Classification Tree Records. You can see that it has already calculated MCR and (scroll right…) reports values for TP, FN, and the rest as well as N (the total number) and potentially useful quantities such as the number of nodes altogether and the depth of the tree.\nLet’s change the tree so we can compare!\n\nDrag fever in and drop it on the left-hand node.\nGive the two “vacant” leaves appropriate values.\nClick emit data again.\n\nYour new table should now look like this:\n\nAccording to the MCRs—where we want a small value—the new tree is better. The sensitivity1 (sens) in the table is also better (we want it to be large).\nNow. Arbor comes with MCR and sens pre-defined. But they are only CODAP columns with formulas. This means that you can make new columns inth is table and define any possible measure of quality for your trees."
  },
  {
    "objectID": "08-arbor/08.50-arbor-measures.html#which-tree-made-this",
    "href": "08-arbor/08.50-arbor-measures.html#which-tree-made-this",
    "title": "34  How good is your tree?",
    "section": "34.3 Which tree made this?",
    "text": "34.3 Which tree made this?\nThe best for last: As you might imagine, after you’ve made a few trees and emitted the data, you might not reemmber exactly what tree created which line in the table.\nDon’t worry: Arbor has your back.\nSimply click on the row you want to know about in the Records table, and Arbor will restore that tree.\nWe will continue with this topic in a bit more depth in the next example, using breast-cancer data."
  },
  {
    "objectID": "08-arbor/08.60-arbor-breast-cancer.html#looking-at-the-data",
    "href": "08-arbor/08.60-arbor-breast-cancer.html#looking-at-the-data",
    "title": "35  More about tree quality: an example with breast cancer data",
    "section": "35.1 Looking at the data",
    "text": "35.1 Looking at the data\nIn the live illustration below, you already have a tree set to predict biopsy.\n\nLook at the tree: among these 569 tumors, 37.3% were malignant.\nLook at the graph: apparently (and unsurprisingly) benign tumors are generally smaller.\nAlso in the graph: we have a “movable value” currently set to 16.6 millimeters. Almost all of the tumors bigger than 16.5 mm are malignant.\n\nDo the following:\n\nDrop radius into the trunk of the tree to make a branch.1\n\nArbor will pick a cut point that determines which values of radius go to the larger branch and which go to the smaller. When I did this, that cut point was 26 mm. You need to change that to be 16.5.\n\nHover over the radius node and click the gear that appears. The configuration box appears.\n\n\n\n\n\n\nLearn more about the configuration box.\n\n\n\nChange the value in the box from 26 to 16.5 and press Done.\nSpecify diagnoses for your leaves: make the “large” tumors Malignant.\n\n\n\n\nIf your setup is like mine, that will give you 125 malignant diagnoses, of which 122 were correct. So that leaves three false positives. On the other hand, there are 90 false negatives. That’s a lot!\nLet’s alter the graph to see this more clearly.\n\nDrop biopsy onto the vertical axis of the graph.\n\nThe graph splits to make two parallel dot plots, each with its own movable value. There are now four counts (and percentages); one for TP, one for FP, etc. See if you can clearly identify which is which, and why. Your graph should look like this:\n\n\n\nIf everything is working correctly, your graph should look like this.\n\n\nWe want to figure out what a good cut point would be. You can explore changing the movable values from 16.5 (you have to change both of them) and see how that affects the numbers of FP (upper right) and FN (lower left). And you should notice two sad truths:\n\nBecause the two distributions overlap, there is no way to get both FP = 0 and FN = 0.\nFalse negatives are the worst. We want to lower that number. But if you lower the cutpoint value to decrease FN, FP has to increase.\n\nSuppose we wanted to eliminate FN’s altogether. We would set the cutpoint to some small number (like zero)…but then all tumors would be diagnosed as malignant. No false negatives, because there would be no negatives at all!\nThis means that every cutpoint is a balancing act. We can use a measure of goodness of a tree to help us achieve that balance.\nWe could make a measure of our tree’s effectiveness just like we did in the previous section. Suppose we decide that a false negative is five times as bad as a false positive. Then our formula might be\nFN * 5 + FP\nand we would want to minimize that value.\nRemember that to get Arbor to calculate the values,\n\nOpen up the in order to export section below the tree.\nPress emit data.\n\nThat will give you a new table with values for FP etc. Then,\n\nMake a new attribute and give it that formula, FN * 5 + FP. The value you’ve calculated appears in the table.\nMake another new attribute and enter the value for the cut point (otherwise you might forget)."
  },
  {
    "objectID": "08-arbor/08.60-arbor-breast-cancer.html#some-tasks",
    "href": "08-arbor/08.60-arbor-breast-cancer.html#some-tasks",
    "title": "35  More about tree quality: an example with breast cancer data",
    "section": "35.2 Some tasks",
    "text": "35.2 Some tasks\nHere are some tasks you might do to extend your understanding.\n\n35.2.1 Find the minimum\nTry various cutpoints and see which one gives the lowest value for the measure.\n\nChange the value of the cutpoint using the configuration box\nRepeat the instructions above: emit data and enter the value of the cutpoint. The FN * 5 + FP will be calculated automatically.\nMake a graph of that value as a function of the cutpoint. Is there a minimum? (Yes!)\n\nNote: the movable values in the graph are not connected to the cut point!\n\n\n35.2.2 Try other attributes\nSee how you can do putting other attributes besides radius in the tree.\nDon’t forget to help yourself out by making graphs!"
  },
  {
    "objectID": "08-arbor/08.90-arbor-configuration.html#left-and-right",
    "href": "08-arbor/08.90-arbor-configuration.html#left-and-right",
    "title": "37  Configuring a node: the details",
    "section": "37.1 Left and right",
    "text": "37.1 Left and right\nThe double-headed arrow diamond near the lower right is a button that reverses the node. With that control, you can (for example) set up every branching so that the more “positive” result flows to the left. A tree is less confusing if the results are less mixed up."
  },
  {
    "objectID": "08-arbor/08.90-arbor-configuration.html#cutpoints",
    "href": "08-arbor/08.90-arbor-configuration.html#cutpoints",
    "title": "37  Configuring a node: the details",
    "section": "37.2 Cutpoints",
    "text": "37.2 Cutpoints\n\n\n\nSample configuration box for age\n\n\nIf the attribute is numeric, you have to decide what number separates the positive from the negative values.\nEnter the number you want for the cutpoint and use the menu to choose the operator that governs which value(s) go on the left.\nNotice that you can use equality. This is useful when you want to isolate a single value for some reason."
  },
  {
    "objectID": "08-arbor/08.90-arbor-configuration.html#more-than-two-categorical-values",
    "href": "08-arbor/08.90-arbor-configuration.html#more-than-two-categorical-values",
    "title": "37  Configuring a node: the details",
    "section": "37.3 More than two categorical values",
    "text": "37.3 More than two categorical values\nSuppose you have four values in the columns, such as Freshman, Sophomore, Junior, and Senior. By default, Arbor picks one value for the left side, and puts the rest on the right:\n\n\n\n“Before.” Frosh are alone on the left.\n\n\nNow suppose you want to split a node by whether the cases are upper or lower class.\nJust click on a value to move it to the other side. For example, clicking on the Soph button will move it to the left side. It will look like this, although you have to edit the labels as we did:\n\n\n\n“After.” Frosh and sophomores are together on the left."
  },
  {
    "objectID": "08-arbor/08.80-arbor-and-ml.html#imagining-automating-tree-building",
    "href": "08-arbor/08.80-arbor-and-ml.html#imagining-automating-tree-building",
    "title": "36  Trees and machine learning",
    "section": "36.1 Imagining automating tree-building",
    "text": "36.1 Imagining automating tree-building\nIt’s one thing to say, we’ll have the computer do the boring part. It’s another to specify exactly what to have the computer do. We will not do that here completely, but we will talk about what’s required for that task.\n\n36.1.1 The branch-everything strategy\nFirst, let’s imagine a brute-force procedure: make the biggest tree possible. To do that, take the first “predictor” attribute that you see, and branch the tree using that attribute. Now you have two terminal nodes. Then take the next attribute, and branch both terminal nodes; now you have four. Continue until you’re out of attributes. (If you start with \\(n\\) attributes, you will wind up with \\(2^n\\) terminal nodes…which can be a lot!)\nTo do that, you still need three things:\n\nAt the beginning, you need a target attribute and you need to know what value(s) of that attribute are “positive.”\nYou need to know, for each attribute, if it has more than two values, how to split it: You need a splitting rule—perhaps a cut point.\nAt the end, you need to know, for each terminal node, what diagnosis you will assign.\n\nYour algorithm will have to address those issues. Then, if you’re about to make the last branch, you will notice situations where it makes no sense to branch the tree, for example:\n\nwhen the split makes almost no difference in the percentage of cases that are positive.\nwhen the number of cases in a branch is small\n\nSo to make a “trimmer” tree, your algorithm will need to specify what “almost no difference” or “small” mean.\nYou might also look at your completed tree and decide to “prune” it—to eliminate additional branches that yield little difference or have too few cases.\n\n\n36.1.2 A more prudent and frugal strategy\nAn alternative to branching everything is to be more careful about what branches you make. Imagine these steps:\n\nTake all the attributes and make a tree for each one, with just that first split.\nFor each tree, calculate how good the tree is using some measure such as the MCR (misclassification rate).\nTake the best tree as measured by that MCR, returning to step 1 using only the remaining attributes.\nIf you ever get a split that’s too little difference or too small, stop branching that part of the tree.\n\nYou still need to define many things, but this will generally yield smaller, more wieldy trees.\n\n\n36.1.3 Linear regression metaphor\nLet’s assume we all understand about least-squares linear regression, a process by which, given a set of data points, we find a line that minimizes the sum of the squares of the residuals. We compute the parameters of that line—the slope and intercept, \\(m\\) and \\(b\\)—using some formulas. And those formulas get derived using calculus.1\nNow let’s imagine that we don’t have calculus.\nWe can still solve the problem using an iterative process. Maybe we begin using the function \\(m=0\\) and \\(b=0\\), that is, \\(y=0x+0\\). We can compute the sum of squares of the residuals. Then, at every step, we look at new values of \\(m\\) and \\(b\\), offset a little from their current values, and calculate the sum of squares from new lines defined by the new values. We pick the line with the lowest sum of squares and then do it all over again. Gradually, when we get close, we can reduce the offsets, and thereby find an optimum line to as great a precision as you like.\nYou can even imagine this as walking on a surface. The \\(m\\) and \\(b\\) axes define a horizontal plane, and the height (\\(z\\)) of the surface is the sum of squares for each location \\((m, b)\\). Our task is to find the lowest point on the surface.\nCalculus does it immediately, but we have the computing power to do the “walk” very efficiently and quickly. And of course, this procedure—which is called gradient descent in ML-speak—works even when calculus does not.\ncost: entropy, gini instead of SSR\ntraining and test sets\noverfitting"
  },
  {
    "objectID": "08-arbor/08.90-arbor-configuration.html#labels",
    "href": "08-arbor/08.90-arbor-configuration.html#labels",
    "title": "37  Configuring a node: the details",
    "section": "37.4 Labels",
    "text": "37.4 Labels\nThe text in the Labels boxes appears on the tree, attached to the links to the “child” nodes.\nEnter whatever you want, in order to make the tree clear and correct.\nIf you have a numerical attribute and a cutpoint, Awash automatically fills in the label with an operator and a value."
  }
]